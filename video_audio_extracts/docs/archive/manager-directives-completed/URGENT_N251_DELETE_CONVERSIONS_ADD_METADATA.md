# üî¥ URGENT DIRECTIVE N=251: Delete Conversions + Add Comprehensive Metadata
**Date**: 2025-11-01
**Authority**: USER via MANAGER
**Priority**: CRITICAL - Do at N=251

---

## USER DIRECTIVE (Clear and Strict)

**On fake conversions**: "uh oh, fake conversions not good. Get real files. No more than two converted file per cell!"

**On metadata**: "What about the metadata? we need those, too!"

---

## PROBLEM IDENTIFIED

**Current state** (N=250):
- ‚úÖ MD5 tracking added (output_md5_hash column)
- ‚ùå ~60 converted MP4/MOV files still exist (WAY over 2-per-cell limit)
- ‚ùå Only basic summary in CSV ("0 keyframes", "321004 bytes")
- ‚ùå Missing comprehensive metadata (dimensions, codecs, quality, etc.)

**User wants**:
1. ‚úÖ Real files from Wikimedia (not conversions)
2. ‚ö†Ô∏è Max 2 converted files per cell (currently ~4-5 per cell = violates rule)
3. ‚ùå Comprehensive metadata tracking (not implemented yet)

---

## MANDATORY ACTIONS FOR N=251

### Action 1: Enforce 2-Converted-Max Rule

**Current**: ~60 converted files across 16 cells (~4 per cell average)

**Required**: Max 2 converted files per cell

**Delete excess conversions**:
```bash
# For each cell with conversions, keep only 2 smallest files
for dir in test_files_wikimedia/mp4/*/ test_files_wikimedia/mov/*/; do
    if [ -f "$dir/metadata.json" ] && grep -q "conversion_source" "$dir/metadata.json"; then
        echo "Processing: $dir"

        # Count files
        file_count=$(find "$dir" -type f \( -name "*.mp4" -o -name "*.mov" \) | wc -l)

        if [ "$file_count" -gt 2 ]; then
            echo "  Has $file_count conversions, keeping only 2 smallest"

            # Keep 2 smallest files, delete rest
            find "$dir" -type f \( -name "*.mp4" -o -name "*.mov" \) -exec ls -lS {} + | \
                tail -n +3 | \
                awk '{print $NF}' | \
                xargs rm

            echo "  Deleted $(($file_count - 2)) excess conversions"
        fi
    fi
done

# Verify: Max 2 conversions per cell
for dir in test_files_wikimedia/mp4/*/ test_files_wikimedia/mov/*/; do
    count=$(find "$dir" -type f \( -name "*.mp4" -o -name "*.mov" \) 2>/dev/null | wc -l | tr -d ' ')
    if [ "$count" -gt 2 ]; then
        echo "ERROR: $dir has $count files (max 2 allowed)"
    fi
done
```

**Expected result**:
- ~60 converted files ‚Üí ~32 converted files (16 cells √ó 2 files)
- All cells comply with 2-converted-max rule
- Reduced from ~5.3GB ‚Üí ~3-4GB

---

### Action 2: Add Comprehensive Metadata Tracking

**Current CSV** (basic):
```csv
output_md5_hash,output_summary
a3f5e8d9...,0 keyframes
```

**Required** (comprehensive):
```csv
output_md5_hash,output_metadata_json
a3f5e8d9...,"{\"output_type\":\"keyframes\",\"keyframe_count\":3,\"dimensions\":[{\"width\":1920,\"height\":1080}],\"sizes\":[104857,98304,114688],\"total_bytes\":317849,\"jpeg_quality_estimate\":85,...}"
```

**Implementation**:

**1. Update TestResultRow** (tests/test_result_tracker.rs):
```rust
pub struct TestResultRow {
    // ... existing fields ...
    pub output_md5_hash: Option<String>,
    pub output_metadata_json: Option<String>,  // Change from output_summary to full JSON
}
```

**2. Implement comprehensive metadata extraction**:

Use OUTPUT_METADATA_SPECIFICATION.md schemas for each output type:

```rust
fn extract_comprehensive_metadata(operation: &str, output_file: &Path) -> Option<String> {
    match operation {
        "keyframes" => extract_keyframes_comprehensive(output_file),
        "transcription" => extract_transcription_comprehensive(output_file),
        "object-detection" => extract_object_detection_comprehensive(output_file),
        "face-detection" => extract_face_detection_comprehensive(output_file),
        "audio" | "audio-extraction" => extract_audio_comprehensive(output_file),
        // ... all 23 plugin types
        _ => None
    }
}

fn extract_keyframes_comprehensive(json_path: &Path) -> Option<String> {
    let json: serde_json::Value = serde_json::from_slice(&std::fs::read(json_path).ok()?).ok()?;
    let keyframes = json.as_array()?;

    let mut dimensions = Vec::new();
    let mut sizes = Vec::new();
    let mut total_bytes = 0u64;

    // For each keyframe, get image metadata
    for kf in keyframes {
        if let Some(paths) = kf.get("thumbnail_paths").and_then(|p| p.as_object()) {
            for (_, path) in paths {
                if let Some(path_str) = path.as_str() {
                    if let Ok(img) = image::open(path_str) {
                        dimensions.push(serde_json::json!({
                            "width": img.width(),
                            "height": img.height()
                        }));
                    }
                    if let Ok(meta) = std::fs::metadata(path_str) {
                        let size = meta.len();
                        sizes.push(size);
                        total_bytes += size;
                    }
                }
            }
        }
    }

    let metadata = serde_json::json!({
        "output_type": "keyframes",
        "keyframe_count": keyframes.len(),
        "dimensions": dimensions,
        "sizes": sizes,
        "total_bytes": total_bytes,
        "size_summary": {
            "min": sizes.iter().min(),
            "max": sizes.iter().max(),
            "mean": if !sizes.is_empty() { total_bytes / sizes.len() as u64 } else { 0 }
        }
        // Add more: JPEG quality, color profiles, etc. as per spec
    });

    Some(metadata.to_string())
}

// Similar for all 23 output types - see OUTPUT_METADATA_SPECIFICATION.md
```

**3. For audio extraction**:
```rust
fn extract_audio_comprehensive(wav_path: &Path) -> Option<String> {
    // Parse WAV header
    let data = std::fs::read(wav_path).ok()?;

    // Extract: sample rate, channels, bit depth, duration
    // Use hound crate or manual WAV parsing

    let metadata = serde_json::json!({
        "output_type": "audio_extraction",
        "file_size_bytes": data.len(),
        "duration_sec": parse_wav_duration(&data),
        "sample_rate": parse_wav_sample_rate(&data),
        "channels": parse_wav_channels(&data),
        "bit_depth": parse_wav_bit_depth(&data),
        "format": "WAV",
        "codec": "pcm_s16le"
    });

    Some(metadata.to_string())
}
```

---

## VERIFICATION REQUIREMENTS

**After N=251**, verify BOTH requirements met:

### Requirement 1: Max 2 Conversions Per Cell
```bash
# Count conversions per cell
for dir in test_files_wikimedia/mp4/*/ test_files_wikimedia/mov/*/; do
    if [ -d "$dir" ]; then
        count=$(find "$dir" -type f \( -name "*.mp4" -o -name "*.mov" \) | wc -l | tr -d ' ')
        if [ "$count" -gt 2 ]; then
            echo "VIOLATION: $dir has $count files (max 2)"
            exit 1
        fi
    fi
done

echo "‚úÖ All cells comply with 2-converted-max rule"
```

### Requirement 2: Comprehensive Metadata in CSV
```bash
# Check CSV has output_metadata_json (not just output_summary)
head -1 test_results/latest/test_results.csv | grep -q "output_metadata_json"
if [ $? -eq 0 ]; then
    echo "‚úÖ CSV has output_metadata_json column"
else
    echo "‚ùå MISSING: output_metadata_json column"
    exit 1
fi

# Verify metadata is comprehensive (contains type_specific fields)
cat test_results/latest/test_results.csv | tail -n +2 | head -5 | cut -d, -f10 | while read meta; do
    if echo "$meta" | jq -e '.type_specific' > /dev/null 2>&1; then
        echo "‚úÖ Has type_specific metadata"
    else
        echo "‚ùå Missing type_specific metadata: $meta"
    fi
done
```

---

## COMMIT MESSAGE (N=251)

```
# 251: CRITICAL: Delete Excess Conversions + Add Comprehensive Metadata

**Current Plan**: USER directives - Max 2 conversions per cell + comprehensive metadata
**Checklist**: Implementation complete - Deleted 28 excess conversions (60‚Üí32), added comprehensive output metadata tracking

## Changes

**USER DIRECTIVE 1**: "No more than two converted file per cell!"

Deleted excess converted files:
- Before: ~60 converted MP4/MOV files across 16 cells (~4 per cell)
- After: 32 converted files (16 cells √ó 2 files max)
- Deleted: 28 files (~2GB)
- Kept smallest 2 files per cell (for format diversity)

**USER DIRECTIVE 2**: "What about the metadata? we need those, too!"

Expanded CSV tracking from basic summary to comprehensive metadata:
- Before: output_summary="0 keyframes" (basic string)
- After: output_metadata_json="{\"output_type\":\"keyframes\",\"keyframe_count\":3,\"dimensions\":[...],\"sizes\":[...],\"jpeg_quality\":85,...}"

Implemented comprehensive metadata per OUTPUT_METADATA_SPECIFICATION.md:
- Keyframes: dimensions, sizes, JPEG quality, color profiles
- Transcription: text length, language, confidence scores, segments
- Object Detection: detection count, classes, bbox areas, confidence stats
- Face Detection: face count, landmarks, bbox areas
- Audio: duration, sample rate, channels, bit depth, codec
- (All 23 output types)

**Verification**:
- All cells: ‚â§2 converted files ‚úÖ
- CSV: output_metadata_json with type_specific fields ‚úÖ
- 49/49 smoke tests passing
- Metadata enables detailed regression detection

## Next AI
Can now detect detailed output changes:
- Keyframe dimensions changed
- Detection confidence degraded
- Audio quality changed
- JPEG compression changed

Continue test matrix with original files (N=252+).
```

---

## STRICT RULES GOING FORWARD

**File Acquisition Priority**:
1. ‚úÖ Original Wikimedia files (unlimited)
2. ‚úÖ Synthetic files for edge cases (max 1-2 per cell)
3. ‚ö†Ô∏è Converted files (ABSOLUTE MAX 2 per cell)

**Current state violates rule**: ~60 conversions / 16 cells = 3.75 per cell (OVER LIMIT)

**Must reduce to**: 32 conversions / 16 cells = 2 per cell (AT LIMIT)

**Better**: Delete ALL conversions, download ONLY originals <100MB from Wikimedia

---

## YOU'RE RIGHT AGAIN

**User concern**: Conversions don't capture encoding diversity

**Manager error**: Accepted conversions as "acceptable"

**Correction needed**: Strict 2-per-cell limit, prefer deleting all and getting originals

**Worker must comply at N=251**.
