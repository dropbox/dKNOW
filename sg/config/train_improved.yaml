# Improved training config with all optimizations
# - Quality filtered data
# - Query augmentation
# - BM25 hard negatives
# - Language-aware batching
# - MRR validation with early stopping
# - Combined InfoNCE + margin loss

data:
  train: "data/training_improved.jsonl"
  validation: "data/training_improved.val.jsonl"
  # languages: ["rust", "python", "lean"]  # Optional filter

model:
  base: "google/xtr-base-en"
  output: "checkpoints/xtr-improved"

training:
  method: "lora"

  # LoRA config (targeting all attention + FFN)
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q"
    - "v"
    - "k"
    - "o"
    - "wi_0"
    - "wi_1"
    - "wo"

  # Training hyperparameters
  batch_size: 12
  gradient_accumulation_steps: 4  # Effective batch = 48
  epochs: 3
  learning_rate: 2.0e-5
  warmup_steps: 500
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Contrastive learning
  temperature: 0.07
  max_hard_negatives: 3  # Use pre-mined BM25 negatives

  # Margin loss (helps separation)
  use_margin_loss: true
  margin: 0.2
  margin_weight: 0.1

  # Sequence length
  max_length: 512

  # Key improvements
  language_aware_batching: true  # Stronger in-batch negatives
  gradient_checkpointing: true
  cache_tokenized: true

  # Validation and early stopping
  eval_every: 300           # Evaluate every 300 steps
  early_stopping_patience: 5  # Stop after 5 evals without improvement
  log_every: 10             # Log every 10 steps for faster feedback
  save_every: 1000

  seed: 42
