# Training config for CodeSearchNet 2M pairs
# License: MIT (safe for any use)
# Target: SOTA code search with Recall@K AUC optimization
#
# Expected results:
#   - Training time: ~4-6 hours on A100, ~12-18 hours on M2 Max
#   - Recall@100 AUC: 0.85+ (vs ~0.70 baseline)
#   - Retrieval latency: <20ms for top-100

data:
  train: "data/codesearchnet_all.jsonl"  # ~2M pairs after filtering
  # Languages: Python, Java, Go, PHP, JavaScript, Ruby

model:
  base: "google/xtr-base-en"
  output: "checkpoints/xtr-codesearchnet-v1"

training:
  method: "lora"

  # LoRA config - higher rank for 2M scale
  lora_r: 64          # Up from 32 for more capacity
  lora_alpha: 128
  lora_dropout: 0.05
  target_modules:
    - "q"
    - "v"
    - "k"
    - "o"
    - "wi_0"
    - "wi_1"
    - "wo"

  # Batch size - maximize for in-batch negatives
  # A100 (40GB): batch_size=64, accum=4 → effective=256
  # A100 (80GB): batch_size=128, accum=2 → effective=256
  # M2 Max (32GB): batch_size=16, accum=8 → effective=128
  batch_size: 64
  gradient_accumulation_steps: 4  # Effective batch = 256

  epochs: 2  # 2M pairs x 2 epochs = 4M training steps
  learning_rate: 5e-6  # Lower for larger batch
  warmup_steps: 2000
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Recall@K optimization
  temperature: 0.05    # Lower = harder discrimination
  margin: 0.1          # Margin for separation
  loss: "infonce_with_margin"
  hard_negatives: 63   # Use top-63 hard negatives (batch-1)

  max_length: 512

  # GPU optimizations
  gradient_checkpointing: true
  cache_tokenized: true
  use_amp: true        # Enable for CUDA
  use_compile: true    # PyTorch 2.0 compile
  num_workers: 8

  # Logging and checkpoints
  log_every: 100
  save_every: 5000
  eval_every: 2500
  eval_spec: "eval/code_queries.json"
  seed: 42

# For M2 Max (MPS), override:
# training:
#   batch_size: 16
#   gradient_accumulation_steps: 8
#   use_amp: false
#   use_compile: false
#   num_workers: 0
