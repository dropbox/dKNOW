# XTR Fine-Tuning on Personal Rust Code
# Phase 2: Fine-tune CodeSearchNet checkpoint on personal Rust repos
#
# Prerequisites:
#   1. Complete Phase 1 (train_codesearchnet.yaml)
#   2. Extract training data: python scripts/extract_rust_training_data.py ~/sg -o data/rust_training_data.jsonl
#
# Usage:
#   python scripts/train_xtr_code.py --config config/train_rust.yaml

model:
  base: "checkpoints/xtr-code-base"  # From Phase 1
  output: "checkpoints/xtr-rust"

data:
  train: "data/rust_training_data.jsonl"
  languages:
    - rust

training:
  method: "lora"
  lora_r: 8           # Smaller rank for fine-tuning (already code-trained)
  lora_alpha: 16
  lora_dropout: 0.1
  target_modules:
    - q
    - v

  epochs: 5           # More epochs for smaller dataset
  batch_size: 16
  learning_rate: 1.0e-5
  warmup_steps: 100
  max_length: 512

  loss: "infonce"
  temperature: 0.07
  hard_negatives: 3   # Fewer hard negatives for smaller dataset

# Compute requirements (M1 Mac):
#   Time: ~2-4 hours (MPS acceleration)
#   Storage: ~5GB
