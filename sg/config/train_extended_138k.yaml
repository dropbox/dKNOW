# Training config for extended 138K multi-language corpus
# Languages: Rust (96K), Python (25K), Lean (9K), Swift (3K), Java (3K), TS, C++, ObjC

data:
  train: "data/training_data_extended.jsonl"
  # Optional: filter to specific languages
  # languages: ["rust", "python", "lean"]

model:
  base: "google/xtr-base-en"
  output: "checkpoints/xtr-extended-138k"

training:
  method: "lora"

  # LoRA configuration
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
    - "q"
    - "v"
    - "k"
    - "o"
    - "wi_0"
    - "wi_1"
    - "wo"

  # Training hyperparameters
  batch_size: 8
  gradient_accumulation_steps: 8  # Effective batch = 64
  epochs: 2
  learning_rate: 2.0e-5
  warmup_steps: 1000
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Contrastive learning
  temperature: 0.07
  hard_negatives: 15  # Use top-15 hard negatives per sample

  # Sequence length
  max_length: 512

  # Optimizations
  gradient_checkpointing: true
  cache_tokenized: true
  use_amp: false  # Disabled for MPS stability
  use_compile: false  # Not well supported on MPS
  num_workers: 0  # MPS doesn't work well with multiprocessing

  # Logging and checkpoints
  log_every: 50
  save_every: 2000  # Save checkpoint every 2000 steps
  seed: 42
