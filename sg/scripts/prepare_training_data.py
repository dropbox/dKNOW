#!/usr/bin/env python3
"""
Combine, clean, and deduplicate training data from multiple sources.

Features:
- Combine multiple JSONL files
- Quality filtering (length, content)
- Near-duplicate removal using MinHash
- Language balancing
- Train/validation split

Usage:
    python scripts/prepare_training_data.py \
        --inputs data/codesearchnet_all.jsonl data/thestack_extracted.jsonl data/training_data_extended.jsonl \
        --output data/combined_training.jsonl \
        --dedupe
"""

import argparse
import hashlib
import json
import random
import re
from collections import defaultdict
from pathlib import Path
from typing import Iterator


def normalize_text(text: str) -> str:
    """Normalize text for deduplication comparison."""
    # Lowercase
    text = text.lower()
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    # Remove common noise
    text = re.sub(r'[^\w\s]', '', text)
    return text.strip()


def compute_hash(text: str) -> str:
    """Compute hash for exact deduplication."""
    normalized = normalize_text(text)
    return hashlib.md5(normalized.encode()).hexdigest()


def shingle(text: str, k: int = 5) -> set[str]:
    """Create k-shingles for MinHash."""
    normalized = normalize_text(text)
    words = normalized.split()
    if len(words) < k:
        return {normalized}
    return {' '.join(words[i:i+k]) for i in range(len(words) - k + 1)}


def jaccard_similarity(set1: set, set2: set) -> float:
    """Compute Jaccard similarity between two sets."""
    if not set1 or not set2:
        return 0.0
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    return intersection / union if union > 0 else 0.0


def is_quality_pair(pair: dict, min_query_len: int, min_code_len: int, max_code_len: int) -> bool:
    """Check if a pair meets quality criteria."""
    query = pair.get("query", "")
    code = pair.get("positive", "")

    # Length checks
    if len(query) < min_query_len:
        return False
    if len(code) < min_code_len:
        return False
    if len(code) > max_code_len:
        return False

    # Query quality
    query_lower = query.lower()

    # Skip trivial docstrings
    trivial_starts = [
        "todo", "fixme", "xxx", "hack", "note:",
        "see ", "deprecated", "internal", "private",
        "constructor", "destructor", "getter", "setter",
        "returns", "param", "@", "#",
    ]
    if any(query_lower.startswith(t) for t in trivial_starts):
        if len(query) < 50:  # Allow longer ones
            return False

    # Skip auto-generated
    if "auto-generated" in query_lower or "generated by" in query_lower:
        return False

    # Skip empty/placeholder docstrings
    if query_lower in ["none", "n/a", "na", "todo", "...", "pass"]:
        return False

    # Code quality
    code_lower = code.lower()

    # Skip test files (usually low quality for search)
    if "test" in pair.get("func_name", "").lower():
        # Allow some test utilities
        if "assert" not in code_lower and "expect" not in code_lower:
            return False

    # Skip trivial functions
    if code.count('\n') < 2:  # Single-line functions
        return False

    return True


def load_and_filter(
    input_files: list[Path],
    min_query_len: int,
    min_code_len: int,
    max_code_len: int,
) -> Iterator[dict]:
    """Load pairs from multiple files and apply quality filtering."""
    for input_file in input_files:
        if not input_file.exists():
            print(f"  Skipping {input_file} (not found)")
            continue

        print(f"  Loading {input_file}...")
        count = 0
        filtered = 0

        with input_file.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                try:
                    pair = json.loads(line)
                except json.JSONDecodeError:
                    continue

                if is_quality_pair(pair, min_query_len, min_code_len, max_code_len):
                    count += 1
                    yield pair
                else:
                    filtered += 1

        print(f"    {count:,} pairs loaded, {filtered:,} filtered")


def deduplicate(pairs: list[dict], similarity_threshold: float = 0.8) -> list[dict]:
    """Remove near-duplicates using MinHash approximation."""
    print(f"  Deduplicating {len(pairs):,} pairs...")

    # First pass: exact deduplication on code
    seen_hashes = set()
    unique_pairs = []

    for pair in pairs:
        code_hash = compute_hash(pair.get("positive", ""))
        if code_hash not in seen_hashes:
            seen_hashes.add(code_hash)
            unique_pairs.append(pair)

    print(f"    After exact dedup: {len(unique_pairs):,} pairs")

    if similarity_threshold >= 1.0:
        return unique_pairs

    # Second pass: near-duplicate removal (expensive, sample-based)
    if len(unique_pairs) > 100000:
        print(f"    Near-dedup on sample (too many pairs)...")
        # For large datasets, do bucket-based dedup
        return bucket_dedupe(unique_pairs, similarity_threshold)

    # Full pairwise comparison for smaller datasets
    shingles = [shingle(p.get("positive", "")) for p in unique_pairs]

    keep = [True] * len(unique_pairs)
    for i in range(len(unique_pairs)):
        if not keep[i]:
            continue
        for j in range(i + 1, len(unique_pairs)):
            if not keep[j]:
                continue
            if jaccard_similarity(shingles[i], shingles[j]) >= similarity_threshold:
                keep[j] = False

    result = [p for p, k in zip(unique_pairs, keep) if k]
    print(f"    After near-dedup: {len(result):,} pairs")

    return result


def bucket_dedupe(pairs: list[dict], similarity_threshold: float) -> list[dict]:
    """Fast bucket-based deduplication for large datasets."""
    # Group by (language, first 100 chars hash)
    buckets = defaultdict(list)

    for pair in pairs:
        lang = pair.get("language", "unknown")
        code = pair.get("positive", "")[:100]
        bucket_key = (lang, compute_hash(code)[:8])
        buckets[bucket_key].append(pair)

    result = []
    for bucket_pairs in buckets.values():
        if len(bucket_pairs) == 1:
            result.append(bucket_pairs[0])
        else:
            # Dedupe within bucket
            seen = set()
            for pair in bucket_pairs:
                h = compute_hash(pair.get("positive", ""))
                if h not in seen:
                    seen.add(h)
                    result.append(pair)

    return result


def balance_languages(pairs: list[dict], max_per_language: int | None = None) -> list[dict]:
    """Balance dataset across languages."""
    by_language = defaultdict(list)

    for pair in pairs:
        lang = pair.get("language", "unknown")
        by_language[lang].append(pair)

    print(f"  Language distribution:")
    for lang, lang_pairs in sorted(by_language.items(), key=lambda x: -len(x[1])):
        print(f"    {lang}: {len(lang_pairs):,}")

    if max_per_language is None:
        return pairs

    result = []
    for lang, lang_pairs in by_language.items():
        if len(lang_pairs) > max_per_language:
            random.shuffle(lang_pairs)
            lang_pairs = lang_pairs[:max_per_language]
        result.extend(lang_pairs)

    random.shuffle(result)
    return result


def create_train_val_split(pairs: list[dict], val_ratio: float = 0.02) -> tuple[list[dict], list[dict]]:
    """Split into training and validation sets."""
    random.shuffle(pairs)
    val_size = int(len(pairs) * val_ratio)

    return pairs[val_size:], pairs[:val_size]


def main():
    parser = argparse.ArgumentParser(description="Prepare training data")
    parser.add_argument("--inputs", "-i", type=Path, nargs="+", required=True)
    parser.add_argument("--output", "-o", type=Path, required=True)
    parser.add_argument("--dedupe", action="store_true", help="Remove duplicates")
    parser.add_argument("--similarity-threshold", type=float, default=0.8)
    parser.add_argument("--min-query-len", type=int, default=15)
    parser.add_argument("--min-code-len", type=int, default=50)
    parser.add_argument("--max-code-len", type=int, default=8000)
    parser.add_argument("--max-per-language", type=int, default=None)
    parser.add_argument("--val-ratio", type=float, default=0.02)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    random.seed(args.seed)

    print("="*60)
    print("TRAINING DATA PREPARATION")
    print("="*60)

    # Load and filter
    print("\n1. Loading and filtering...")
    pairs = list(load_and_filter(
        args.inputs,
        args.min_query_len,
        args.min_code_len,
        args.max_code_len,
    ))
    print(f"  Total after filtering: {len(pairs):,}")

    # Deduplicate
    if args.dedupe:
        print("\n2. Deduplicating...")
        pairs = deduplicate(pairs, args.similarity_threshold)

    # Balance
    print("\n3. Balancing languages...")
    pairs = balance_languages(pairs, args.max_per_language)
    print(f"  Total after balancing: {len(pairs):,}")

    # Split
    print("\n4. Creating train/val split...")
    train_pairs, val_pairs = create_train_val_split(pairs, args.val_ratio)
    print(f"  Training: {len(train_pairs):,}")
    print(f"  Validation: {len(val_pairs):,}")

    # Write output
    print("\n5. Writing output...")
    args.output.parent.mkdir(parents=True, exist_ok=True)

    with args.output.open("w", encoding="utf-8") as f:
        for pair in train_pairs:
            f.write(json.dumps(pair, ensure_ascii=False) + "\n")

    val_output = args.output.with_suffix(".val.jsonl")
    with val_output.open("w", encoding="utf-8") as f:
        for pair in val_pairs:
            f.write(json.dumps(pair, ensure_ascii=False) + "\n")

    print(f"  Training: {args.output}")
    print(f"  Validation: {val_output}")

    # Stats
    stats = {
        "total_train": len(train_pairs),
        "total_val": len(val_pairs),
        "inputs": [str(p) for p in args.inputs],
        "config": {
            "dedupe": args.dedupe,
            "similarity_threshold": args.similarity_threshold,
            "min_query_len": args.min_query_len,
            "min_code_len": args.min_code_len,
            "max_code_len": args.max_code_len,
        }
    }

    stats_file = args.output.with_suffix(".stats.json")
    with stats_file.open("w") as f:
        json.dump(stats, f, indent=2)

    print(f"\n{'='*60}")
    print(f"DONE: {len(train_pairs):,} training + {len(val_pairs):,} validation pairs")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
