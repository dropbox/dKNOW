# Test Infrastructure Design - Questions & Answers

**Date**: 2025-10-31
**Status**: UNDER REVIEW - Awaiting user feedback
**Purpose**: Capture design decisions for pytest test suite overhaul

---

## SCOPE CLARIFICATION

### Q1: How many PDFs should have tests?

**Current assumption**: 60 PDFs in master test suite
**User correction**: **ALL 452 PDFs** should have tests

**Impact**:
- 452 PDFs × 3 tests = **1,356 static test functions**
- Need organization strategy for 452 test files
- Need efficient expected output storage (452 PDFs worth of data)

**Follow-up questions**:
- Should 452 PDFs be organized into subdirectories by category?
  - `tests/pdfs/arxiv/test_arxiv_001.py`
  - `tests/pdfs/cc/test_cc_001.py`
  - `tests/pdfs/edge_cases/test_344775293.py`
[yes, use markers]

- Should there be a "standard_60_set" subset marker for critical tests? [yes, just a marker]
- What about the 60-PDF master list - is it obsolete now? [yes, just encode as a marker]

**USER RESPONSE:**
responses inline. use markers to create groups. Yes, create 1356 static test functions nicely divided into files and organized for llms to find them with metadata.

---

## TEST GENERATION STRATEGY

### Q2: Should tests be auto-generated or hand-written?

**Concern**: "I'm worried about having auto-generated tests, because then the LLM may not know about the tests and would regenerate its own tests or get confused."

**Options Evaluated**:

**Option A: Fully Static (Hand-Written)**
- 452 test files written explicitly
- Pros: LLM can see and understand all tests
- Cons: Manual creation, harder to update test logic

**Option B: Generated + Committed to Git**
- Script generates 452 files once
- Commit generated files to git
- Pros: LLM can see tests, no regeneration
- Cons: Git diff noise when logic changes, merge conflicts

**Option C: Auto-Generated at Collection Time**
- Tests generated during `pytest --collect`
- Pros: Easy to update logic
- Cons: LLM can't see tests, may regenerate

**Recommendation**: **Option B - Generated + Committed**

**Rationale**:
- LLM can read and reference all 452 test files
- One-time generation using `python lib/generate_test_files.py`
- If test logic changes, regenerate + commit
- Clear in git when test logic was updated (all 452 files change together)
- Include comment in each generated file: "Generated by lib/generate_test_files.py - DO NOT EDIT"

**USER RESPONSE:**
B and commited. It's ok that there is some git noise. The LLM not being confused and correctness is most important.

---

## EXPECTED OUTPUT STORAGE

### Q3: How to handle large expected outputs (452 PDFs)?

**User feedback**:
- "They take a while to generate, so having them precomputed is efficient"
- "One compromise may be to use pdfium's settings to save smaller images or save JPGs"
- "Another idea to save each extracted page of text and be able to extract each page individually"

**Storage Size Estimate** (for 452 PDFs):
- Text: ~50MB total (varies by PDF)
- JSONL: ~60MB total
- Images (PNG): **~20GB total** (452 PDFs × avg 150 pages × ~300KB/page)
- Images (JPEG 80%): **~2GB total** (10x reduction)

**Recommendation**: **Hybrid Storage Strategy**

```
expected_outputs/
  <pdf_stem>/
    manifest.json              # Always committed

    text/                      # Per-page text (committed if total < 1MB)
      page_0000.txt
      page_0001.txt
      ...
      full.txt                 # Concatenated (for quick comparison)

    jsonl/                     # Per-page JSONL (committed if total < 1MB)
      page_0000.jsonl
      page_0001.jsonl
      ...
      full.jsonl

    images/                    # NOT committed (too large)
      page_0000.png            # Generated on-demand or stored locally
      page_0000.png.md5        # MD5 hash committed
      ...
```

**Benefits**:
- ✅ Per-page text: Fast debugging (compare page N only)
- ✅ Small files: Can commit text/jsonl to git
- ✅ MD5 hashes: Verify images without storing them
- ✅ On-demand image generation: Generate images as needed for testing

**Trade-offs**:
- First-time setup: Must generate all images (~1-2 hours)
- Images not in git: Separate storage or regeneration needed
- Alternative: Store images in Git LFS (Large File Storage)

**USER RESPONSE:**
- per-page text: yes, much better, and will help with diff and edit distance
- small files: image MD5 and image size and dimensions and other metadata. we don't need the images and can regenerate them as needed, you're right.

---

## PER-PAGE TEXT EXTRACTION

### Q4: Should text extraction save per-page files?

**User suggestion**: "Save each extracted page of text and be able to extract each page individually. That will dramatically accelerate debugging text errors."

**Current**: Full document text in one file
```
text.txt      # All pages concatenated
```

**Proposed**: Per-page + full
```
text/
  page_0000.txt    # Page 0 only
  page_0001.txt    # Page 1 only
  ...
  full.txt         # All pages (for quick comparison)
```

**Benefits**:
- ✅ Debug failures faster: "diff page_0042.txt" instead of searching in 1000-page file
- ✅ Isolate page-specific issues
- ✅ Page-level test granularity possible later
- ✅ Matches PDFium's native output (pdfium_test creates per-page files)

**Implementation**:
```python
def generate_text_baseline_per_page(pdf_path, baseline_binary, output_dir):
    """Generate per-page text files"""
    # pdfium_test --txt creates: <pdf>.pdf.0.txt, <pdf>.pdf.1.txt, ...
    page_files = run_pdfium_test(baseline_binary, pdf_path, '--txt')

    text_dir = output_dir / 'text'
    text_dir.mkdir(exist_ok=True)

    all_text = []
    for i, page_file in enumerate(sorted(page_files)):
        page_text = page_file.read_bytes()

        # Save per-page
        (text_dir / f'page_{i:04d}.txt').write_bytes(page_text)

        # Accumulate for full file
        all_text.append(page_text)

    # Save full text
    (text_dir / 'full.txt').write_bytes(b''.join(all_text))
```

**USER RESPONSE:**
Yes! we want per-page files. Put them in a directory together per test source.

---

## IMAGE STORAGE STRATEGY

### Q5: How to store expected images (20GB for 452 PDFs)?

**User suggestion**: "Use pdfium's settings to save smaller images or save JPGs"

**Options**:

**A: PNG with compression**
```bash
pdfium_test --png --png-compression=9  # Smaller PNGs
# Size: ~10GB (vs 20GB uncompressed)
```

**B: JPEG with quality setting**
```bash
pdfium_test --jpg --jpg-quality=85  # JPEG
# Size: ~2GB (10x reduction)
# Trade-off: Lossy compression
```

**C: MD5 hashes only (no images in git)**
```
images/
  page_0000.png.md5     # Only hash committed
  page_0001.png.md5
  ...
```
Images stored:
- Locally in developer's machine
- On shared network drive
- Generated on-demand for testing

**D: Git LFS (Large File Storage)**
```bash
git lfs track "*.png"
# Images in git, but stored efficiently
```

**E: Thumbnail + Full (Hybrid)**
```
images/
  page_0000_thumb.jpg   # 100x100 thumbnail (committed)
  page_0000.png.md5     # Full image hash (committed)
  page_0000.png         # Full image (ignored, generated on-demand)
```

**Recommendation**: **Option C (MD5 only) + on-demand generation**

**Rationale**:
- Manifests with MD5s are small (~100KB total)
- First test run generates images if missing
- Tests fail fast if hash mismatch
- No 20GB in git

**USER RESPONSE:**
yes, all MD5s and image metadata. We don't need the thumbnail can generate on demand if needed. I also want to generate JPG by default: it's a more reasonable format that we're more likely to use in production. Is there a reason to not use jpg? Do both png and jpg but only save the MD5 and metadata. We don't need the thumbnails.


---

## TEST STRUCTURE ORGANIZATION

### Q6: How to organize 452 test files?

**Flat structure** (current plan):
```
tests/pdfs/
  test_arxiv_001.py
  test_arxiv_005.py
  ...
  test_cc_001.py
  ...
  (452 files in one directory)
```

**Hierarchical structure**:
```
tests/pdfs/
  arxiv/
    test_arxiv_001.py
    test_arxiv_005.py
    ...
  cc/
    test_cc_001.py
    ...
  edge_cases/
    test_344775293.py
    ...
  pages/
    test_0100pages_7FKQLKX273JBHXAAW5XDRT27JGMIZMCI.py
    ...
```

**Benefits of hierarchical**:
- ✅ Better organization
- ✅ Run all arxiv: `pytest tests/pdfs/arxiv/`
- ✅ Easier to navigate
- ✅ Matches PDF directory structure

**USER RESPONSE:**
organization, match the PDF directory structure

---

## PYTEST HOOKS VS FIXTURES

### Q7: Should telemetry use pytest hooks or fixtures?

**Current plan**: Telemetry in fixtures (side effects)

**Best practice**: Telemetry in hooks

```python
# conftest.py

def pytest_runtest_makereport(item, call):
    """Automatically log telemetry for every test"""
    if call.when == "call":
        telemetry.log({
            'test_nodeid': item.nodeid,
            'duration': call.duration,
            'outcome': 'passed' if call.excinfo is None else 'failed',
            'pdf_name': extract_pdf_from_nodeid(item.nodeid),
            'workers': item.config.getoption('--workers'),
            # ...
        })
```

**Benefits**:
- ✅ Zero test code changes needed
- ✅ Works for ALL tests automatically
- ✅ No side effects in fixtures
- ✅ Standard pytest pattern

**USER RESPONSE:**
ok, use in hooks. I don't fully understand this point so long as we have logging for every test

---

## TEST EXECUTION CONFIGURATION

### Q8: Should test execution be deterministic or configurable?

**Configurable** (current plan):
```bash
pytest --workers=8 --api=fast --debug
# Same test, different behavior based on flags
```

**Pros**:
- Flexible for different scenarios
- One test suite, multiple configurations

**Cons**:
- Non-deterministic (fails reproduce)
- Hard to track which config was used
- Against pytest best practices

**Deterministic** (alternative):
```python
# Correctness tests: Always 4 workers, bulk API
def test_text_correctness_arxiv_001(...):
    extract_text(workers=4, api='bulk')

# Separate test file for scaling
@pytest.mark.scaling
@pytest.mark.parametrize("workers", [1, 2, 4, 8])
def test_text_scaling_arxiv_001(workers, ...):
    extract_text(workers=workers, api='bulk')
```

**Recommendation**: **Hybrid Approach**

**Correctness tests**: Fixed config (4 workers, bulk API)
```python
# tests/pdfs/arxiv/test_arxiv_001.py
def test_text_correctness_arxiv_001(...):
    # Always runs with: workers=4, api='bulk', debug=False
    result = extract_text(PDF_PATH, workers=4, api='bulk')
```

**Configuration tests**: Separate suite
```python
# tests/configuration/test_worker_scaling.py
@pytest.mark.parametrize("workers", [1, 2, 4, 8, 16])
@pytest.mark.parametrize("pdf", REPRESENTATIVE_PDFS)  # Subset, not all 452
def test_worker_scaling(workers, pdf, ...):
    result = extract_text(pdf, workers=workers)
```

**USER RESPONSE:**
hybrid


---

## MANIFEST STRUCTURE

### Q9: What should the PDF manifest contain?

**Currently planned**:
```csv
pdf_name, pdf_path, pdf_md5, pdf_size, pdf_pages, pdf_category, pdf_size_class,
text_baseline_path, text_baseline_exists, text_baseline_md5, ...
```

**Should also include**:
- Per-page text file paths?
- Per-page byte/char counts?
- Image file paths (452 PDFs × avg 150 pages = ~68,000 images)?

**Recommendation**: **Two-level manifest**

**Level 1: PDF Manifest** (one row per PDF)
```csv
pdf_name, pdf_path, pdf_md5, pdf_pages, pdf_category, pdf_size_class,
expected_output_manifest_path
```

**Level 2: Per-PDF Expected Output Manifest** (one JSON per PDF)
```json
{
  "pdf": "arxiv_001.pdf",
  "pages": 10,
  "text": {
    "full": {"path": "text/full.txt", "md5": "...", "bytes": 12345, "chars": 12000},
    "pages": [
      {"page": 0, "path": "text/page_0000.txt", "md5": "...", "bytes": 1200, "chars": 1150},
      {"page": 1, "path": "text/page_0001.txt", "md5": "...", "bytes": 1180, "chars": 1140},
      ...
    ]
  },
  "jsonl": {
    "full": {"path": "jsonl/full.jsonl", "md5": "...", "lines": 100},
    "pages": [...]
  },
  "images": {
    "format": "png",
    "pages": [
      {"page": 0, "path": "images/page_0000.png", "md5": "...", "bytes": 45000},
      ...
    ]
  }
}
```

**USER RESPONSE:**
per-page

---

## EXPECTED OUTPUTS IN GIT

### Q10: What expected outputs should be committed to git?

**User feedback**:
- Expected outputs should be pre-generated (not on-demand)
- Concerned about size (~20GB images)
- Suggested: smaller images (JPEG) or per-page text

**Options**:

**A: Commit Everything**
- Text (per-page + full): ~50MB
- JSONL (per-page + full): ~60MB
- Images (PNG): ~20GB ❌ Too large
- Images (JPEG 85%): ~2GB ⚠️ Still large

**B: Commit Text/JSONL, MD5 Only for Images**
- Text: ~50MB ✅
- JSONL: ~60MB ✅
- Image MD5s: ~100KB ✅
- Images: Developer generates locally (~1-2 hours one-time)

**C: Git LFS for Images**
- All files in git
- Images stored in LFS (efficient)
- Requires Git LFS setup

**D: External Storage**
- Text/JSONL: In git
- Images: S3/shared drive
- Download script: `python lib/download_expected_images.py`

**Recommendation**: **Option B (MD5 + Local Generation)**

**Rationale**:
- Manifests with MD5s are tiny (~10MB total for 452 PDFs)
- Text/JSONL are manageable (~110MB total)
- Images generated on-demand (detected if missing)
- First test run: "Expected images not found, generating..." (1-2 hours)
- Subsequent runs: Fast (images cached locally)

**Alternative if per-page text is too large**: Commit only full.txt + MD5s for page files

**USER RESPONSE:**
text per page, md5s and meta

---

## PER-PAGE TEXT EXTRACTION

### Q11: Should per-page text files be generated?

**User feedback**: "That will dramatically accelerate debugging text errors."

**Proposal**:

**Generation**:
```python
def generate_text_per_page(pdf_path, baseline_binary):
    """Generate per-page text files"""
    # pdfium_test --txt creates: <pdf>.pdf.0.txt, <pdf>.pdf.1.txt

    for page_num in range(page_count):
        page_text = extract_page(pdf_path, page_num, baseline_binary)
        save_to(f'text/page_{page_num:04d}.txt', page_text)

    # Also save full text
    full_text = concatenate_pages(all_pages)
    save_to('text/full.txt', full_text)
```

**Test Usage**:
```python
def test_text_correctness_arxiv_001(...):
    # Extract all pages
    actual_pages = extract_text_per_page(pdf_path, test_binary)
    expected_pages = load_expected_pages("arxiv_001")

    # Compare page by page
    for page_num, (actual, expected) in enumerate(zip(actual_pages, expected_pages)):
        assert actual == expected, f"Page {page_num} mismatch"
```

**Debugging workflow**:
```bash
# Test fails on arxiv_001
pytest tests/pdfs/arxiv/test_arxiv_001.py::test_text_correctness_arxiv_001

# Output: "Page 42 mismatch"

# Debug just that page
diff expected_outputs/arxiv_001/text/page_0042.txt \
     test_output/arxiv_001/text/page_0042.txt

# Much faster than diffing 1000-page files
```

**USER RESPONSE:**
per page

---

## IMAGE COMPARISON STRATEGY

### Q12: How to compare images?

**User response**: "1 first, then 2 if no match" (MD5 first, perceptual diff fallback)

**Confirmed Implementation**:
```python
def compare_image(expected_img, actual_img, page_num):
    """Compare images: MD5 first, perceptual diff fallback"""

    # Strategy 1: MD5 hash (fast, strict)
    actual_md5 = compute_md5(actual_img)
    expected_md5 = expected_img.md5  # From manifest

    if actual_md5 == expected_md5:
        return True  # Perfect match

    # Strategy 2: Perceptual diff (slower, tolerant)
    expected_data = expected_img.load_bytes()
    diff_score = perceptual_diff(expected_data, actual_img)

    threshold = 0.01  # 1% difference allowed
    if diff_score < threshold:
        return True  # Close enough

    # Failure
    pytest.fail(
        f"Image mismatch on page {page_num}\n"
        f"  MD5: {actual_md5} != {expected_md5}\n"
        f"  Perceptual diff: {diff_score:.4f} > {threshold}"
    )
```

**Question**: What perceptual diff algorithm?
- Option 1: SSIM (Structural Similarity Index)
- Option 2: MSE (Mean Squared Error)
- Option 3: ImageHash (phash/dhash)

**USER RESPONSE:**
SSIM

---

## BATCH BULK INPUT

### Q13: How should batch bulk tests pass PDFs?

**User response**: "Explicit file list"

**Confirmed Implementation**:
```python
# tests/test_batch_bulk.py

BATCH_BULK_PDFS = [
    "arxiv_001.pdf",
    "arxiv_005.pdf",
    # ... all 452 explicitly listed
]

def test_batch_bulk_text(test_binary, benchmark_pdfs):
    """Batch text extraction for all PDFs"""

    # Build explicit file list
    pdf_paths = [str(benchmark_pdfs / pdf) for pdf in BATCH_BULK_PDFS]

    cmd = [
        str(test_binary),
        '--txt',
        '--batch',
        *pdf_paths  # Pass all 452 as arguments
    ]

    result = subprocess.run(cmd, capture_output=True, timeout=7200)
    assert result.returncode == 0
```

**Confirmed**: ✅ Explicit file list as arguments
also accept a directory as a parameter with a flag

---

## DETERMINISM TESTS

### Q14: Should there be separate determinism tests?

**User response**: "No, determinism is implied by correctness tests"

**Confirmed**: ✅ No separate determinism tests (180 tests total, not 240)

---

## TEST SUITE SIZE

### Q15: How many total tests?

**Calculation** (assuming all 452 PDFs):

**Per-PDF Tests**:
- Text extraction: 452 tests
- JSONL extraction: 452 tests
- Image rendering: 452 tests
- **Subtotal: 1,356 tests**

**Infrastructure Tests**:
- test_000_1_pdf_manifest: 1 test
- test_000_2_expected_outputs_integrity: ~452 parametrized (one per PDF)
- test_000_3_baseline_generation_reproducible: 1 test
- **Subtotal: ~454 tests**

**Batch Bulk Tests**:
- test_batch_bulk_text: 1 test
- test_batch_bulk_jsonl: 1 test
- test_batch_bulk_images: 1 test
- test_batch_bulk_all: 1 test
- **Subtotal: 4 tests**

**TOTAL: ~1,814 tests**

**Questions**:
- Is 1,814 tests too many for one suite?
- Should edge_cases PDFs have all 3 tests, or just smoke tests?
- Should there be a "critical_subset" marker for CI (e.g., 60 PDFs)?

**USER RESPONSE:**
yes, we need a subset for a smoke test. something that can run in 1 minute and be a git commit trigger. 

---

## JSONL FORMAT SPECIFICATION

### Q16: What should JSONL output contain?

**User feedback**: "Yes, all of these. There is a standard object returned by pdfium that has this information in text mode."

**PDFium Text Object** (from `FPDFText_*` APIs):
```c
// Per character
FPDFText_GetTextUnicode()      // Character value
FPDFText_GetCharBox()          // Bounding box
FPDFText_GetFontSize()         // Font size
FPDFText_GetFontInfo()         // Font name/flags
FPDFText_GetTextRenderMode()   // Render mode

// Per page
FPDFText_CountChars()          // Character count
FPDFText_CountRects()          // Text rectangles
```

**Proposed JSONL Schema** (one line per character or text run):
```jsonl
{"page":0,"char_idx":0,"char":"H","unicode":72,"x":100.5,"y":200.3,"width":8.2,"height":12.0,"font":"Arial","font_size":12,"render_mode":0}
{"page":0,"char_idx":1,"char":"e","unicode":101,"x":108.7,"y":200.3,"width":7.5,"height":12.0,"font":"Arial","font_size":12,"render_mode":0}
...
```

**Or grouped by text run**:
```jsonl
{"page":0,"text":"Hello World","chars":11,"bbox":{"x0":100.5,"y0":200.3,"x1":180.2,"y1":212.3},"font":"Arial","size":12}
{"page":0,"text":"Next line","chars":9,"bbox":{"x0":100.5,"y0":215.0,"x1":165.0,"y1":227.0},"font":"Arial","size":12}
...
```

**Question**: Character-level or text-run level granularity?

**USER RESPONSE:**
We want character level, but that's probably too much information. Every character will take almost 200x the space! Let's treat the JSONL like images (get metadata and md5) and let's only save the first page of each pdf.

---

## BASELINE BINARY VERIFICATION

### Q17: How to ensure correct baseline binary?

**User feedback**: "Baseline needs to be hardcoded."

**Confirmed Implementation**:
```python
# conftest.py

# Hardcoded baseline binary (upstream PDFium, unmodified)
BASELINE_BINARY_PATH = Path(__file__).parent.parent / 'out' / 'Optimized-Shared' / 'pdfium_test'
BASELINE_BINARY_MD5 = '00cd20f999bf60b1f779249dbec8ceaa'

@pytest.fixture(scope="session", autouse=True)
def verify_baseline_binary():
    """Verify baseline binary before any tests run"""
    assert BASELINE_BINARY_PATH.exists(), f"Baseline not found: {BASELINE_BINARY_PATH}"

    actual_md5 = compute_md5(BASELINE_BINARY_PATH)
    assert actual_md5 == BASELINE_BINARY_MD5, (
        f"Baseline binary MD5 mismatch\n"
        f"  Expected: {BASELINE_BINARY_MD5}\n"
        f"  Actual: {actual_md5}\n"
        f"  The baseline binary has changed. Update BASELINE_BINARY_MD5 if intentional."
    )
```

**Confirmed**: ✅ Hardcoded path + MD5 verification

---

## PERFORMANCE TEST ASSERTIONS

### Q18: Should performance tests assert thresholds or just log?

**User response**: "Just log. We will compute runtimes and analytics as log analysis"

**Confirmed Implementation**:
```python
@pytest.mark.performance
def test_performance_arxiv_001(...):
    """Performance benchmark for arxiv_001.pdf"""

    # Run with different worker counts
    times = {}
    for workers in [1, 4, 8]:
        start = time.time()
        extract_text(pdf_path, workers=workers)
        times[workers] = time.time() - start

    # Calculate speedup
    speedup_4w = times[1] / times[4]
    speedup_8w = times[1] / times[8]

    # Log only (no assertions)
    print(f"Performance results:")
    print(f"  1w: {times[1]:.2f}s")
    print(f"  4w: {times[4]:.2f}s (speedup: {speedup_4w:.2f}x)")
    print(f"  8w: {times[8]:.2f}s (speedup: {speedup_8w:.2f}x)")

    # Telemetry logs this automatically via hooks
    # No assertion - analytics done separately
```

**Confirmed**: ✅ Log only, no assertions

---

## OUTSTANDING QUESTIONS SUMMARY

**Must Answer**:
1. ✅ **Q1**: All 452 PDFs? Organization structure?
2. ✅ **Q2**: Generated + committed to git?
3. ✅ **Q5**: Image storage strategy? MD5 only? Git LFS?
4. ✅ **Q6**: Flat or hierarchical test file organization?
5. ✅ **Q9**: Two-level manifest structure?
6. ✅ **Q10**: What to commit? Text per-page OK?
7. ⚠️ **Q11**: Per-page text comparison in tests?
8. ⚠️ **Q12**: Which perceptual diff algorithm?
9. ⚠️ **Q15**: Is 1,814 tests too many? Need subset?
10. ⚠️ **Q16**: JSONL schema - character or text-run level?

**Already Confirmed**:
- ✅ **Q4**: Per-page text structure
- ✅ **Q7**: Use pytest hooks for telemetry
- ✅ **Q8**: Deterministic correctness tests
- ✅ **Q13**: Batch bulk uses explicit file list
- ✅ **Q14**: No separate determinism tests
- ✅ **Q17**: Hardcoded baseline binary + MD5
- ✅ **Q18**: Performance tests log only

---

## NEXT STEPS

1. **User**: Answer outstanding questions in this document
2. **Manager**: Generate code path trace documents (text extraction, image rendering)
3. **Manager**: Revise plan based on answers
4. **Manager**: Implement final design
5. **Worker**: Execute tests

**STATUS**: Awaiting user response on questions marked ✅
