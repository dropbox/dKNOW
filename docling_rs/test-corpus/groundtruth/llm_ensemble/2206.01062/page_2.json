{
  "page_number": 2,
  "elements": [
    {
      "label": "page_header",
      "text": "KDD '22, August 14–18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar",
      "bbox": {
        "l": 0.0,
        "t": 0.0,
        "r": 100.0,
        "b": 5.0
      },
      "confidence": 0.94
    },
    {
      "label": "section_header",
      "text": "1 INTRODUCTION",
      "bbox": {
        "l": 0.0,
        "t": 6.0,
        "r": 100.0,
        "b": 10.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Despite the substantial improvements achieved with machine-learning (ML) approaches and deep neural networks in recent years, document conversion remains a challenging problem, as demonstrated by the numerous public competitions held on this topic [1–4]. The challenge originates from the huge variability in PDF documents regarding layout, language and formats (scanned, programmatic or a combination of both). Engineering a single ML model that can be applied on all types of documents and provides high-quality layout segmentation remains to this day extremely challenging [5]. To highlight the variability in document layouts, we show a few example documents from the DocLayNet dataset in Figure 1.",
      "bbox": {
        "l": 0.0,
        "t": 11.0,
        "r": 100.0,
        "b": 20.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "A key problem in the process of document conversion is to understand the structure of a single document page, i.e. which segments of text should be grouped together in a unit. To train models for this task, there are currently two large datasets available to the community, PubLayNet [6] and DocBank [7]. They were introduced in 2019 and 2020 respectively and significantly accelerated the implementation of layout detection and segmentation models due to their sizes of 300K and 500K ground-truth pages. These sizes were achieved by leveraging an automation approach. The benefit of automated ground-truth generation is obvious: one can generate large ground-truth datasets at virtually no cost. However, the automation introduces a constraint on the variability in the dataset, because corresponding structured source data must be available. PubLayNet and DocBank were both generated from scientific document repositories (PubMed and arXiv), which provide XML or TEX sources. For scientific documents or pre-style layouts, we see sub-par prediction quality from these models, which we demonstrate in Section 5.",
      "bbox": {
        "l": 0.0,
        "t": 21.0,
        "r": 100.0,
        "b": 35.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "In this paper, we present the DocLayNet dataset. It provides page-by-page layout annotation ground-truth using bounding-boxes for 11 distinct class labels on 80863 unique document pages, of which a fraction carry double- or triple-annotations. DocLayNet is similar in spirit to PubLayNet and DocBank and will likewise be made available to the public1 in order to stimulate the document-layout analysis community. It distinguishes itself in the following aspects:",
      "bbox": {
        "l": 0.0,
        "t": 36.0,
        "r": 100.0,
        "b": 45.0
      },
      "confidence": 0.94
    },
    {
      "label": "list_item",
      "text": "(1) Human Annotation: In contrast to PubLayNet and DocBank, we relied on human annotation instead of automation approaches to generate the dataset.",
      "bbox": {
        "l": 0.0,
        "t": 46.0,
        "r": 100.0,
        "b": 50.0
      },
      "confidence": 0.94
    },
    {
      "label": "list_item",
      "text": "(2) Large Layout Variability: We include diverse and complex layouts from a large variety of public sources.",
      "bbox": {
        "l": 0.0,
        "t": 51.0,
        "r": 100.0,
        "b": 55.0
      },
      "confidence": 0.94
    },
    {
      "label": "list_item",
      "text": "(3) Detailed Label Set: We define 11 class labels to distinguish layout features in high detail. PubLayNet provides 5 labels; DocBank provides 13, although not a superset of ours.",
      "bbox": {
        "l": 0.0,
        "t": 56.0,
        "r": 100.0,
        "b": 60.0
      },
      "confidence": 0.94
    },
    {
      "label": "list_item",
      "text": "(4) Redundant Annotations: A fraction of the pages in the DocLayNet data set carry more than one human annotation.",
      "bbox": {
        "l": 0.0,
        "t": 61.0,
        "r": 100.0,
        "b": 65.0
      },
      "confidence": 0.94
    },
    {
      "label": "footnote",
      "text": "1https://developer.ibm.com/exchanges/data/all/doclaynet",
      "bbox": {
        "l": 0.0,
        "t": 66.0,
        "r": 100.0,
        "b": 68.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "This enables experimentation with annotation uncertainty and quality control analysis.",
      "bbox": {
        "l": 0.0,
        "t": 69.0,
        "r": 100.0,
        "b": 71.0
      },
      "confidence": 0.94
    },
    {
      "label": "list_item",
      "text": "(5) Pre-defined Train-, Test- & Validation-set: Like DocBank, we provide fixed train-, test- & validation-sets to ensure proportional representation of the class-labels. Further, we prevent leakage of unique layouts across sets, which has a large effect on model accuracy scores.",
      "bbox": {
        "l": 0.0,
        "t": 72.0,
        "r": 100.0,
        "b": 78.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "All aspects outlined above are detailed in Section 3. In Section 4, we will elaborate on how we designed and executed this large-scale human annotation campaign. We will also share key insights and lessons learned that might prove helpful for other parties planning to set up annotation campaigns.",
      "bbox": {
        "l": 0.0,
        "t": 79.0,
        "r": 100.0,
        "b": 85.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "In Section 5, we will present baseline accuracy numbers for a variety of object detection methods (Faster R-CNN, Mask R-CNN and YOLOv5) trained on DocLayNet. We further show how the model performance is impacted by varying the DocLayNet dataset size, reducing the label set and modifying the train/test-split. Last but not least, we compare the performance of models trained on PubLayNet, DocBank and DocLayNet and demonstrate that a model trained on DocLayNet provides overall more robust layout recovery.",
      "bbox": {
        "l": 0.0,
        "t": 86.0,
        "r": 100.0,
        "b": 95.0
      },
      "confidence": 0.94
    },
    {
      "label": "section_header",
      "text": "2 RELATED WORK",
      "bbox": {
        "l": 0.0,
        "t": 96.0,
        "r": 100.0,
        "b": 100.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "While early approaches in document-layout analysis used rule-based algorithms and heuristics [8], the problem is lately addressed with deep learning methods. The most common approach is to leverage object detection methods [9]. In recent years, the accuracy and speed of these models has increased dramatically. Furthermore, most state-of-the-art object detection methods can be trained and applied with very little work, thanks to a standardization effort of the ground-truth data format [16] and common deep-learning frameworks [17]. Reference data sets such as PubLayNet [6] and DocBank provide their data in the commonly accepted COCO format [16].",
      "bbox": {
        "l": 0.0,
        "t": 101.0,
        "r": 100.0,
        "b": 110.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Lately, new types of ML models for document-layout analysis have emerged in the community [18–21]. These models do not approach the problem of layout analysis purely based on an image representation of the page, as computer vision methods do. Instead, they combine the text tokens and image representation of a page in order to obtain a segmentation. While the reported accuracies appear to be promising, a broadly accepted data format which links geometric and textual features has yet to establish.",
      "bbox": {
        "l": 0.0,
        "t": 111.0,
        "r": 100.0,
        "b": 120.0
      },
      "confidence": 0.94
    },
    {
      "label": "section_header",
      "text": "3 THE DOCLAYNET DATASET",
      "bbox": {
        "l": 0.0,
        "t": 121.0,
        "r": 100.0,
        "b": 125.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "DocLayNet contains 80863 PDF pages. Among these, 7059 carry two instances of human annotations, and 1591 carry three. This amounts to 91104 total annotation instances. The annotations provide layout information in the shape of labeled, rectangular bounding-boxes. We define 11 distinct labels for layout features, namely Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, and Title. Our reasoning for picking this particular label set is detailed in Section 4.",
      "bbox": {
        "l": 0.0,
        "t": 126.0,
        "r": 100.0,
        "b": 135.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "In addition to open intellectual property constraints for the source documents, we required that the documents in DocLayNet adhere to a few conditions. Firstly, we kept scanned documents",
      "bbox": {
        "l": 0.0,
        "t": 136.0,
        "r": 100.0,
        "b": 140.0
      },
      "confidence": 0.94
    }
  ],
  "reading_order": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19
  ],
  "agreement_scores": [
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0
  ],
  "sources": [
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble"
  ]
}