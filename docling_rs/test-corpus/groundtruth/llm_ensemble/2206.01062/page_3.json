{
  "page_number": 3,
  "elements": [
    {
      "label": "page_header",
      "text": "DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis    KDD '22, August 14–18, 2022, Washington, DC, USA",
      "bbox": {
        "l": 0.0,
        "t": 0.0,
        "r": 100.0,
        "b": 5.0
      },
      "confidence": 0.94
    },
    {
      "label": "picture",
      "text": "Pie chart showing distribution of DocLayNet pages: Financial 32%, Scientific 17%, Patents 8%, Tenders 6%, Laws 16%, Manuals 21%",
      "bbox": {
        "l": 10.0,
        "t": 10.0,
        "r": 50.0,
        "b": 40.0
      },
      "confidence": 0.94
    },
    {
      "label": "caption",
      "text": "Figure 2: Distribution of DocLayNet pages across document categories.",
      "bbox": {
        "l": 10.0,
        "t": 41.0,
        "r": 50.0,
        "b": 45.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents (> 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing “text in the wild”.",
      "bbox": {
        "l": 55.0,
        "t": 10.0,
        "r": 100.0,
        "b": 20.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports, Manuals, Scientific Articles, Laws & Regulations, Patents and Government Tenders. Each document category was sourced from various repositories. For example, Financial Reports contain both free-style annual reports2 which expose company-specific, artistic layouts as well as the more formal SEC filings. The two largest categories (Financial Reports and Manuals) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variability by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.",
      "bbox": {
        "l": 55.0,
        "t": 21.0,
        "r": 100.0,
        "b": 35.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95%) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.0%). While the document language has negligible impact on the performance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.",
      "bbox": {
        "l": 55.0,
        "t": 36.0,
        "r": 100.0,
        "b": 50.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "To ensure that future benchmarks in the document-layout analysis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avoid spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal proportions.",
      "bbox": {
        "l": 55.0,
        "t": 51.0,
        "r": 100.0,
        "b": 65.0
      },
      "confidence": 0.94
    },
    {
      "label": "footnote",
      "text": "2e.g. AAPL Form https://www.annualreports.com/",
      "bbox": {
        "l": 55.0,
        "t": 66.0,
        "r": 100.0,
        "b": 70.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Table 1 shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This avoids that pages of the same document are spread over train, test and validation sets, which can give an undesired evaluation advantage to models and lead to overestimation of their precision accuracy. We will show the impact of this decision in Section 5.",
      "bbox": {
        "l": 0.0,
        "t": 71.0,
        "r": 50.0,
        "b": 85.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images (in PNG format, 1025x1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates (in JSON). All additional files are linked to the primary page images by their matching filenames.",
      "bbox": {
        "l": 0.0,
        "t": 86.0,
        "r": 50.0,
        "b": 100.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Despite being cost-intensive and far less scalable than automation, human annotation has several benefits over automated ground-truth generation. The first and most obvious reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use human annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, “invisible” tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when annotators decide to space them as “invisible” list elements without bullet symbols. A third reason to gather ground-truth through human annotation is to estimate a “natural” upper bound on the segmentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have different but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.",
      "bbox": {
        "l": 0.0,
        "t": 101.0,
        "r": 50.0,
        "b": 150.0
      },
      "confidence": 0.94
    },
    {
      "label": "section_header",
      "text": "4 ANNOTATION CAMPAIGN",
      "bbox": {
        "l": 0.0,
        "t": 151.0,
        "r": 50.0,
        "b": 155.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,",
      "bbox": {
        "l": 0.0,
        "t": 156.0,
        "r": 50.0,
        "b": 170.0
      },
      "confidence": 0.94
    }
  ],
  "reading_order": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12
  ],
  "agreement_scores": [
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0
  ],
  "sources": [
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble"
  ]
}