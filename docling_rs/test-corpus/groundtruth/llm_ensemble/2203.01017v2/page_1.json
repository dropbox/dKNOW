{
  "page_number": 1,
  "elements": [
    {
      "label": "title",
      "text": "TableFormer: Table Structure Understanding with Transformers.",
      "bbox": {
        "l": 10.0,
        "t": 5.0,
        "r": 90.0,
        "b": 10.0
      },
      "confidence": 0.94
    },
    {
      "label": "text",
      "text": "Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar",
      "bbox": {
        "l": 10.0,
        "t": 11.0,
        "r": 90.0,
        "b": 13.0
      },
      "confidence": 0.94
    },
    {
      "label": "text",
      "text": "IBM Research",
      "bbox": {
        "l": 10.0,
        "t": 13.0,
        "r": 90.0,
        "b": 14.0
      },
      "confidence": 0.94
    },
    {
      "label": "text",
      "text": "{ahn,nli,mly,taa}@zurich.ibm.com",
      "bbox": {
        "l": 10.0,
        "t": 14.0,
        "r": 90.0,
        "b": 15.0
      },
      "confidence": 0.94
    },
    {
      "label": "section_header",
      "text": "Abstract",
      "bbox": {
        "l": 10.0,
        "t": 16.0,
        "r": 90.0,
        "b": 17.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph’s, etc, since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multiline rows, different variety of separation lines, missing entries, etc. As such, the correct identification of the table-structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-dual-decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF’s directly from the PDF source and avoid the training of the custom OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to tackle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.",
      "bbox": {
        "l": 10.0,
        "t": 18.0,
        "r": 90.0,
        "b": 30.0
      },
      "confidence": 0.94
    },
    {
      "label": "section_header",
      "text": "1. Introduction",
      "bbox": {
        "l": 10.0,
        "t": 31.0,
        "r": 90.0,
        "b": 32.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "The occurrence of tables in documents is ubiquitous. They often summarise quantitative or factual data, which is cumbersome to describe in verbose text but nevertheless extremely valuable. Unfortunately, this compact representation is often not easy to parse by machines. There are many implicit conventions used to obtain a compact table representation. For example, tables often have complex column- and row-headers in order to reduce duplicated cell content. Lines of different shapes and sizes are leveraged to separate content or indicate a tree structure. Additionally, tables can also have empty/missing table-entries or multi-row textual table-entries. Fig. 1 shows a table which presents all these issues.",
      "bbox": {
        "l": 10.0,
        "t": 33.0,
        "r": 90.0,
        "b": 42.0
      },
      "confidence": 0.94
    },
    {
      "label": "picture",
      "text": "Picture of a table with subtle, complex features such as (1) multi-column headers, (2) cell with multi-row text and (3) cells with no content.",
      "bbox": {
        "l": 55.0,
        "t": 18.0,
        "r": 90.0,
        "b": 30.0
      },
      "confidence": 0.94
    },
    {
      "label": "caption",
      "text": "Figure 1: Picture of a table with subtle, complex features such as (1) multi-column headers, (2) cell with multi-row text and (3) cells with no content. Image from PubTabNet evaluation set, filename: ‘PMC2944238.004.02’.",
      "bbox": {
        "l": 55.0,
        "t": 31.0,
        "r": 90.0,
        "b": 32.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Recently, significant progress has been made with vision based approaches to extract tables in documents. For the sake of completeness, the issue of table extraction from documents is typically decomposed into two separate challenges, i.e. (1) finding the location of the table(s) on a document-page and (2) finding the structure of a given table in the document. The first problem is called table-location and has been previously addressed [30, 38, 19, 21, 23, 26, 8] with state-of-the-art object-detection networks (e.g. YOLO and later on Mask-RCNN [9]). For all practical purposes, it can be",
      "bbox": {
        "l": 10.0,
        "t": 43.0,
        "r": 90.0,
        "b": 50.0
      },
      "confidence": 0.94
    }
  ],
  "reading_order": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10
  ],
  "agreement_scores": [
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0
  ],
  "sources": [
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble"
  ]
}