{
  "page_number": 4,
  "elements": [
    {
      "label": "page_header",
      "text": "4 M. Lysak, et al.",
      "bbox": {
        "l": 0.0,
        "t": 0.0,
        "r": 100.0,
        "b": 5.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Other work [20] aims at predicting a grid for each table and deciding which cells must be merged using an attention network. Im2Seq methods cast the problem as a sequence generation task [4,5,9,22], and therefore need an internal table-structure representation language, which is often implemented with standard markup languages (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage over the OD and GNN methods by virtue of directly predicting the table-structure. As such, no post-processing or rules are needed in order to obtain the table-structure, which is necessary with OD and GNN approaches. In practice, this is not entirely true, because a predicted sequence of table-structure markup does not necessarily have to be syntactically correct. Hence, depending on the quality of the predicted sequence, some post-processing needs to be performed to ensure a syntactically valid (let alone correct) sequence.",
      "bbox": {
        "l": 0.0,
        "t": 6.0,
        "r": 100.0,
        "b": 30.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Within the Im2Seq method, we find several popular models, namely the encoder-dual-decoder model (EDD) [22], TableFormer [9], TabSplitter [2] and Ye et al. [19]. EDD uses two consecutive long short-term memory (LSTM) decoders to predict a table in HTML representation. The tag decoder predicts a sequence of HTML tags. For each decoded table cell (<td>), the attention is passed to the cell decoder to predict the content with an embedded OCR approach. The latter makes it susceptible to transcription errors in the cell content of the table. TableFormer address this reliance on OCR and uses two transformer decoders for HTML structure and cell bounding box prediction in an end-to-end architecture. The predicted cell bounding box is then used to extract text tokens from an originating (digital) PDF page, circumventing any need for OCR. TabSplitter [2] proposes a compact double-matrix representation of table rows and columns to do error detection and error correction of HTML structure sequences based on predictions from [19]. This compact double-matrix representation can not be used directly by the Im2seq model training, so the model uses HTML as an intermediate form. Chi et. al. [4] introduce a data set and a baseline method using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet transformers to predict LaTeX code, and a separate OCR module to extract content.",
      "bbox": {
        "l": 0.0,
        "t": 31.0,
        "r": 100.0,
        "b": 70.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "Im2Seq approaches have shown to be well-suited for the TSR task and allow a full end-to-end network design that can output the final table structure without pre- or post-processing logic. Furthermore, Im2Seq models have demonstrated to deliver state-of-the-art prediction accuracy [9]. This motivated the authors to investigate if the performance (both in accuracy and inference time) can be further improved by optimising the table structure representation language. We believe this is a necessary step before further improving neural network architectures for this task.",
      "bbox": {
        "l": 0.0,
        "t": 71.0,
        "r": 100.0,
        "b": 90.0
      },
      "confidence": 0.94
    },
    {
      "label": "section_header",
      "text": "3 Problem Statement",
      "bbox": {
        "l": 0.0,
        "t": 91.0,
        "r": 100.0,
        "b": 95.0
      },
      "confidence": 0.94
    },
    {
      "label": "paragraph",
      "text": "All known Im2Seq based models for TSR fundamentally work in similar ways. Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a finite vocab-",
      "bbox": {
        "l": 0.0,
        "t": 96.0,
        "r": 100.0,
        "b": 100.0
      },
      "confidence": 0.94
    }
  ],
  "reading_order": [
    0,
    1,
    2,
    3,
    4,
    5
  ],
  "agreement_scores": [
    1.0,
    1.0,
    1.0,
    1.0,
    1.0,
    1.0
  ],
  "sources": [
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble",
    "ensemble"
  ]
}