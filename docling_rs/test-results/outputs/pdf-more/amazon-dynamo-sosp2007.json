{"schema_name": "DoclingDocument", "version": "1.7.0", "name": "amazon-dynamo-sosp2007", "origin": {"mimetype": "application/pdf", "binary_hash": 17585019155916262356, "filename": "amazon-dynamo-sosp2007.pdf"}, "furniture": {"self_ref": "#/furniture", "children": [], "content_layer": "furniture", "name": "_root_", "label": "unspecified"}, "body": {"self_ref": "#/body", "children": [{"$ref": "#/texts/0"}, {"$ref": "#/texts/1"}, {"$ref": "#/texts/2"}, {"$ref": "#/texts/3"}, {"$ref": "#/texts/4"}, {"$ref": "#/texts/5"}, {"$ref": "#/texts/6"}, {"$ref": "#/texts/7"}, {"$ref": "#/texts/8"}, {"$ref": "#/texts/9"}, {"$ref": "#/texts/10"}, {"$ref": "#/texts/11"}, {"$ref": "#/texts/12"}, {"$ref": "#/texts/13"}, {"$ref": "#/texts/14"}, {"$ref": "#/texts/15"}, {"$ref": "#/texts/16"}, {"$ref": "#/texts/17"}, {"$ref": "#/texts/18"}, {"$ref": "#/texts/19"}, {"$ref": "#/texts/20"}, {"$ref": "#/texts/21"}, {"$ref": "#/texts/22"}, {"$ref": "#/texts/23"}, {"$ref": "#/texts/24"}, {"$ref": "#/texts/25"}, {"$ref": "#/texts/26"}, {"$ref": "#/texts/27"}, {"$ref": "#/texts/28"}, {"$ref": "#/texts/29"}, {"$ref": "#/texts/30"}, {"$ref": "#/texts/31"}, {"$ref": "#/texts/32"}, {"$ref": "#/texts/33"}, {"$ref": "#/texts/34"}, {"$ref": "#/texts/35"}, {"$ref": "#/texts/36"}, {"$ref": "#/texts/37"}, {"$ref": "#/texts/38"}, {"$ref": "#/texts/39"}, {"$ref": "#/texts/40"}, {"$ref": "#/texts/41"}, {"$ref": "#/pictures/0"}, {"$ref": "#/texts/51"}, {"$ref": "#/texts/52"}, {"$ref": "#/texts/53"}, {"$ref": "#/texts/54"}, {"$ref": "#/texts/55"}, {"$ref": "#/texts/56"}, {"$ref": "#/texts/57"}, {"$ref": "#/texts/58"}, {"$ref": "#/texts/59"}, {"$ref": "#/texts/60"}, {"$ref": "#/texts/61"}, {"$ref": "#/texts/62"}, {"$ref": "#/texts/63"}, {"$ref": "#/texts/64"}, {"$ref": "#/texts/65"}, {"$ref": "#/texts/66"}, {"$ref": "#/texts/67"}, {"$ref": "#/texts/68"}, {"$ref": "#/texts/69"}, {"$ref": "#/texts/70"}, {"$ref": "#/texts/71"}, {"$ref": "#/texts/72"}, {"$ref": "#/texts/73"}, {"$ref": "#/texts/74"}, {"$ref": "#/texts/75"}, {"$ref": "#/texts/76"}, {"$ref": "#/pictures/1"}, {"$ref": "#/texts/92"}, {"$ref": "#/texts/93"}, {"$ref": "#/texts/94"}, {"$ref": "#/texts/95"}, {"$ref": "#/texts/96"}, {"$ref": "#/tables/0"}, {"$ref": "#/texts/98"}, {"$ref": "#/texts/99"}, {"$ref": "#/texts/100"}, {"$ref": "#/texts/101"}, {"$ref": "#/texts/102"}, {"$ref": "#/texts/103"}, {"$ref": "#/texts/104"}, {"$ref": "#/texts/105"}, {"$ref": "#/texts/106"}, {"$ref": "#/texts/107"}, {"$ref": "#/groups/0"}, {"$ref": "#/texts/111"}, {"$ref": "#/texts/112"}, {"$ref": "#/texts/113"}, {"$ref": "#/texts/114"}, {"$ref": "#/texts/115"}, {"$ref": "#/texts/116"}, {"$ref": "#/texts/117"}, {"$ref": "#/texts/118"}, {"$ref": "#/texts/119"}, {"$ref": "#/texts/120"}, {"$ref": "#/texts/121"}, {"$ref": "#/texts/122"}, {"$ref": "#/texts/123"}, {"$ref": "#/texts/124"}, {"$ref": "#/pictures/2"}, {"$ref": "#/texts/132"}, {"$ref": "#/texts/133"}, {"$ref": "#/texts/134"}, {"$ref": "#/texts/135"}, {"$ref": "#/texts/136"}, {"$ref": "#/texts/137"}, {"$ref": "#/texts/138"}, {"$ref": "#/texts/139"}, {"$ref": "#/texts/140"}, {"$ref": "#/texts/141"}, {"$ref": "#/texts/142"}, {"$ref": "#/texts/143"}, {"$ref": "#/texts/144"}, {"$ref": "#/texts/145"}, {"$ref": "#/texts/146"}, {"$ref": "#/texts/147"}, {"$ref": "#/texts/148"}, {"$ref": "#/texts/149"}, {"$ref": "#/texts/150"}, {"$ref": "#/texts/151"}, {"$ref": "#/texts/152"}, {"$ref": "#/texts/153"}, {"$ref": "#/texts/154"}, {"$ref": "#/texts/155"}, {"$ref": "#/texts/156"}, {"$ref": "#/texts/157"}, {"$ref": "#/texts/158"}, {"$ref": "#/texts/159"}, {"$ref": "#/texts/160"}, {"$ref": "#/texts/161"}, {"$ref": "#/texts/162"}, {"$ref": "#/texts/163"}, {"$ref": "#/texts/164"}, {"$ref": "#/texts/165"}, {"$ref": "#/texts/166"}, {"$ref": "#/texts/167"}, {"$ref": "#/texts/168"}, {"$ref": "#/texts/169"}, {"$ref": "#/texts/170"}, {"$ref": "#/texts/171"}, {"$ref": "#/texts/172"}, {"$ref": "#/texts/173"}, {"$ref": "#/texts/174"}, {"$ref": "#/texts/175"}, {"$ref": "#/texts/176"}, {"$ref": "#/texts/177"}, {"$ref": "#/texts/178"}, {"$ref": "#/texts/179"}, {"$ref": "#/texts/180"}, {"$ref": "#/texts/181"}, {"$ref": "#/texts/182"}, {"$ref": "#/texts/183"}, {"$ref": "#/texts/184"}, {"$ref": "#/texts/185"}, {"$ref": "#/texts/186"}, {"$ref": "#/texts/187"}, {"$ref": "#/texts/188"}, {"$ref": "#/texts/189"}, {"$ref": "#/texts/190"}, {"$ref": "#/texts/191"}, {"$ref": "#/texts/192"}, {"$ref": "#/texts/193"}, {"$ref": "#/pictures/3"}, {"$ref": "#/texts/206"}, {"$ref": "#/texts/207"}, {"$ref": "#/texts/208"}, {"$ref": "#/texts/209"}, {"$ref": "#/groups/1"}, {"$ref": "#/texts/211"}, {"$ref": "#/pictures/4"}, {"$ref": "#/groups/2"}, {"$ref": "#/texts/216"}, {"$ref": "#/texts/217"}, {"$ref": "#/texts/218"}, {"$ref": "#/texts/219"}, {"$ref": "#/texts/220"}, {"$ref": "#/texts/221"}, {"$ref": "#/texts/222"}, {"$ref": "#/texts/223"}, {"$ref": "#/texts/224"}, {"$ref": "#/texts/225"}, {"$ref": "#/texts/226"}, {"$ref": "#/texts/227"}, {"$ref": "#/texts/228"}, {"$ref": "#/texts/229"}, {"$ref": "#/pictures/5"}, {"$ref": "#/texts/235"}, {"$ref": "#/texts/236"}, {"$ref": "#/texts/237"}, {"$ref": "#/texts/238"}, {"$ref": "#/texts/239"}, {"$ref": "#/texts/240"}, {"$ref": "#/texts/241"}, {"$ref": "#/texts/242"}, {"$ref": "#/texts/243"}, {"$ref": "#/texts/244"}, {"$ref": "#/texts/245"}, {"$ref": "#/texts/246"}, {"$ref": "#/texts/247"}, {"$ref": "#/texts/248"}, {"$ref": "#/texts/249"}, {"$ref": "#/texts/250"}, {"$ref": "#/texts/251"}, {"$ref": "#/pictures/6"}, {"$ref": "#/texts/255"}, {"$ref": "#/texts/256"}, {"$ref": "#/texts/257"}, {"$ref": "#/texts/258"}, {"$ref": "#/texts/259"}, {"$ref": "#/texts/260"}, {"$ref": "#/texts/261"}, {"$ref": "#/texts/262"}, {"$ref": "#/texts/263"}, {"$ref": "#/pictures/7"}, {"$ref": "#/texts/285"}, {"$ref": "#/texts/286"}, {"$ref": "#/texts/287"}, {"$ref": "#/texts/288"}, {"$ref": "#/texts/289"}, {"$ref": "#/texts/290"}, {"$ref": "#/texts/291"}, {"$ref": "#/texts/292"}, {"$ref": "#/texts/293"}, {"$ref": "#/texts/294"}, {"$ref": "#/texts/295"}, {"$ref": "#/texts/296"}, {"$ref": "#/tables/1"}, {"$ref": "#/texts/298"}, {"$ref": "#/texts/299"}, {"$ref": "#/texts/300"}, {"$ref": "#/texts/301"}, {"$ref": "#/texts/302"}, {"$ref": "#/texts/303"}, {"$ref": "#/texts/304"}, {"$ref": "#/texts/305"}, {"$ref": "#/texts/306"}, {"$ref": "#/texts/307"}, {"$ref": "#/texts/308"}, {"$ref": "#/texts/309"}, {"$ref": "#/texts/310"}, {"$ref": "#/texts/311"}, {"$ref": "#/texts/312"}, {"$ref": "#/texts/313"}, {"$ref": "#/texts/314"}, {"$ref": "#/groups/3"}, {"$ref": "#/texts/336"}, {"$ref": "#/groups/4"}, {"$ref": "#/texts/341"}], "content_layer": "body", "name": "_root_", "label": "unspecified"}, "groups": [{"self_ref": "#/groups/0", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/108"}, {"$ref": "#/texts/109"}, {"$ref": "#/texts/110"}], "content_layer": "body", "name": "list", "label": "list"}, {"self_ref": "#/groups/1", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/210"}], "content_layer": "body", "name": "list", "label": "list"}, {"self_ref": "#/groups/2", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/214"}, {"$ref": "#/texts/215"}], "content_layer": "body", "name": "list", "label": "list"}, {"self_ref": "#/groups/3", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/315"}, {"$ref": "#/texts/316"}, {"$ref": "#/texts/317"}, {"$ref": "#/texts/318"}, {"$ref": "#/texts/319"}, {"$ref": "#/texts/320"}, {"$ref": "#/texts/321"}, {"$ref": "#/texts/322"}, {"$ref": "#/texts/323"}, {"$ref": "#/texts/324"}, {"$ref": "#/texts/325"}, {"$ref": "#/texts/326"}, {"$ref": "#/texts/327"}, {"$ref": "#/texts/328"}, {"$ref": "#/texts/329"}, {"$ref": "#/texts/330"}, {"$ref": "#/texts/331"}, {"$ref": "#/texts/332"}, {"$ref": "#/texts/333"}, {"$ref": "#/texts/334"}, {"$ref": "#/texts/335"}], "content_layer": "body", "name": "list", "label": "list"}, {"self_ref": "#/groups/4", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/337"}, {"$ref": "#/texts/338"}, {"$ref": "#/texts/339"}, {"$ref": "#/texts/340"}], "content_layer": "body", "name": "list", "label": "list"}], "texts": [{"self_ref": "#/texts/0", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 79.92, "t": 716.124, "r": 537.057, "b": 700.187, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 49]}], "orig": "Dynamo: Amazon's Highly Available Key-value Store", "text": "Dynamo: Amazon's Highly Available Key-value Store", "level": 1}, {"self_ref": "#/texts/1", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 90.18, "t": 690.816, "r": 528.521, "b": 652.591, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 167]}], "orig": "Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash Lakshman, Alex Pilchin, Swaminathan Sivasubramanian, Peter Vosshall and Werner Vogels", "text": "Giuseppe DeCandia, Deniz Hastorun, Madan Jampani, Gunavardhan Kakulapati, Avinash Lakshman, Alex Pilchin, Swaminathan Sivasubramanian, Peter Vosshall and Werner Vogels"}, {"self_ref": "#/texts/2", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 276.78, "t": 643.854, "r": 337.94, "b": 634.983, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Amazon.com", "text": "Amazon.com"}, {"self_ref": "#/texts/3", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 318.24, "t": 606.324, "r": 560.321, "b": 474.848, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 882]}], "orig": "One  of  the  lessons  our  organization  has  learned  from  operating Amazon's  platform  is  that  the  reliability  and  scalability  of  a system  is  dependent  on  how  its  application  state  is  managed. Amazon  uses  a  highly  decentralized,  loosely  coupled,  service oriented  architecture  consisting  of  hundreds  of  services.  In  this environment  there  is  a  particular  need  for  storage  technologies that are always available. For example, customers should be able to  view  and  add  items  to  their  shopping  cart  even  if  disks  are failing,  network  routes  are  flapping,  or  data  centers  are  being destroyed  by  tornados.  Therefore,  the  service  responsible  for managing shopping carts requires that it can always write to and read  from  its  data  store,  and  that  its  data  needs  to  be  available across multiple data centers.", "text": "One  of  the  lessons  our  organization  has  learned  from  operating Amazon's  platform  is  that  the  reliability  and  scalability  of  a system  is  dependent  on  how  its  application  state  is  managed. Amazon  uses  a  highly  decentralized,  loosely  coupled,  service oriented  architecture  consisting  of  hundreds  of  services.  In  this environment  there  is  a  particular  need  for  storage  technologies that are always available. For example, customers should be able to  view  and  add  items  to  their  shopping  cart  even  if  disks  are failing,  network  routes  are  flapping,  or  data  centers  are  being destroyed  by  tornados.  Therefore,  the  service  responsible  for managing shopping carts requires that it can always write to and read  from  its  data  store,  and  that  its  data  needs  to  be  available across multiple data centers."}, {"self_ref": "#/texts/4", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 603.252, "r": 122.317, "b": 593.472, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 8]}], "orig": "ABSTRACT", "text": "ABSTRACT", "level": 1}, {"self_ref": "#/texts/5", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 590.544, "r": 296.124, "b": 479.709, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 687]}], "orig": "Reliability  at  massive  scale  is  one  of  the  biggest  challenges  we face at Amazon.com, one of the largest e-commerce operations in the  world;  even  the  slightest  outage  has  significant  financial consequences  and  impacts  customer  trust.  The  Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers  and  network  components  located  in  many  datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these  failures  drives  the  reliability  and  scalability of  the software systems.", "text": "Reliability  at  massive  scale  is  one  of  the  biggest  challenges  we face at Amazon.com, one of the largest e-commerce operations in the  world;  even  the  slightest  outage  has  significant  financial consequences  and  impacts  customer  trust.  The  Amazon.com platform, which provides services for many web sites worldwide, is implemented on top of an infrastructure of tens of thousands of servers  and  network  components  located  in  many  datacenters around the world. At this scale, small and large components fail continuously and the way persistent state is managed in the face of these  failures  drives  the  reliability  and  scalability of  the software systems."}, {"self_ref": "#/texts/6", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 54.001, "t": 470.724, "r": 296.107, "b": 401.289, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 466]}], "orig": "This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core  services  use  to  provide  an  'always-on'  experience.    To achieve  this  level  of  availability,  Dynamo  sacrifices  consistency under  certain  failure  scenarios.  It  makes  extensive  use  of  object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use.", "text": "This paper presents the design and implementation of Dynamo, a highly available key-value storage system that some of Amazon's core  services  use  to  provide  an  'always-on'  experience.    To achieve  this  level  of  availability,  Dynamo  sacrifices  consistency under  certain  failure  scenarios.  It  makes  extensive  use  of  object versioning and application-assisted conflict resolution in a manner that provides a novel interface for developers to use."}, {"self_ref": "#/texts/7", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 391.212, "r": 234.986, "b": 381.432, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 34]}], "orig": "Categories and Subject Descriptors", "text": "Categories and Subject Descriptors", "level": 1}, {"self_ref": "#/texts/8", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 378.444, "r": 285.933, "b": 350.409, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 132]}], "orig": "D.4.2 [ Operating Systems ]: Storage Management; D.4.5 [ Operating Systems ]: Reliability; D.4.2 [ Operating Systems ]: Performance;", "text": "D.4.2 [ Operating Systems ]: Storage Management; D.4.5 [ Operating Systems ]: Reliability; D.4.2 [ Operating Systems ]: Performance;"}, {"self_ref": "#/texts/9", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 340.392, "r": 131.718, "b": 330.612, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "General Terms", "text": "General Terms", "level": 1}, {"self_ref": "#/texts/10", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 327.624, "r": 284.966, "b": 309.968, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 70]}], "orig": "Algorithms, Management, Measurement, Performance, Design, Reliability.", "text": "Algorithms, Management, Measurement, Performance, Design, Reliability."}, {"self_ref": "#/texts/11", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 299.892, "r": 173.928, "b": 290.112, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "1. INTRODUCTION", "text": "1. INTRODUCTION", "level": 1}, {"self_ref": "#/texts/12", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 54.0, "t": 287.124, "r": 296.06, "b": 186.66899999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 636]}], "orig": "Amazon runs a world-wide e-commerce platform that serves tens of  millions  customers  at  peak  times  using  tens  of  thousands  of servers located in many data centers around the world. There are strict operational requirements on Amazon's platform in terms of performance, reliability and efficiency, and to support continuous growth the platform needs to be highly scalable. Reliability is one of  the  most  important  requirements  because  even  the  slightest outage has significant financial consequences and impacts customer  trust.  In  addition,  to  support  continuous  growth,  the platform needs to be highly scalable.", "text": "Amazon runs a world-wide e-commerce platform that serves tens of  millions  customers  at  peak  times  using  tens  of  thousands  of servers located in many data centers around the world. There are strict operational requirements on Amazon's platform in terms of performance, reliability and efficiency, and to support continuous growth the platform needs to be highly scalable. Reliability is one of  the  most  important  requirements  because  even  the  slightest outage has significant financial consequences and impacts customer  trust.  In  addition,  to  support  continuous  growth,  the platform needs to be highly scalable."}, {"self_ref": "#/texts/13", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 58.14, "t": 146.47500000000002, "r": 294.178, "b": 93.95000000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 444]}], "orig": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not  made  or  distributed  for  profit  or  commercial  advantage  and  that copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, requires prior specific permission and/or a fee.", "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not  made  or  distributed  for  profit  or  commercial  advantage  and  that copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, requires prior specific permission and/or a fee."}, {"self_ref": "#/texts/14", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 58.14, "t": 91.274, "r": 262.762, "b": 75.59000000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 111]}], "orig": "SOSP'07, October 14-17, 2007, Stevenson, Washington, USA. Copyright 2007 ACM 978-1-59593-591-5/07/0010...$5.00.", "text": "SOSP'07, October 14-17, 2007, Stevenson, Washington, USA. Copyright 2007 ACM 978-1-59593-591-5/07/0010...$5.00."}, {"self_ref": "#/texts/15", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 318.24, "t": 465.803, "r": 560.317, "b": 396.368, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 430]}], "orig": "Dealing with failures in an infrastructure comprised of millions of components is our standard mode of operation; there are always a small  but  significant  number  of  server  and  network  components that  are  failing  at  any  given  time.  As  such  Amazon's  software systems  need  to  be  constructed  in  a  manner  that  treats  failure handling  as  the  normal  case  without  impacting  availability  or performance.", "text": "Dealing with failures in an infrastructure comprised of millions of components is our standard mode of operation; there are always a small  but  significant  number  of  server  and  network  components that  are  failing  at  any  given  time.  As  such  Amazon's  software systems  need  to  be  constructed  in  a  manner  that  treats  failure handling  as  the  normal  case  without  impacting  availability  or performance."}, {"self_ref": "#/texts/16", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 318.24, "t": 387.383, "r": 560.337, "b": 235.14800000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 987]}], "orig": "To meet the reliability and scaling needs, Amazon has developed a  number of storage technologies, of which the Amazon Simple Storage Service (also available outside of Amazon and known as Amazon S3), is probably the best known. This paper presents the design and implementation of Dynamo, another highly available and  scalable  distributed  data  store  built  for  Amazon's  platform. Dynamo is  used  to  manage  the  state  of  services  that  have  very high  reliability  requirements  and  need  tight  control  over  the tradeoffs between availability, consistency, cost-effectiveness and performance.  Amazon's  platform  has  a  very  diverse set of applications  with  different  storage  requirements.  A  select  set  of applications requires a storage technology that is flexible enough to let application designers configure their data store appropriately based on these tradeoffs to achieve high availability and guaranteed performance in the most cost effective manner.", "text": "To meet the reliability and scaling needs, Amazon has developed a  number of storage technologies, of which the Amazon Simple Storage Service (also available outside of Amazon and known as Amazon S3), is probably the best known. This paper presents the design and implementation of Dynamo, another highly available and  scalable  distributed  data  store  built  for  Amazon's  platform. Dynamo is  used  to  manage  the  state  of  services  that  have  very high  reliability  requirements  and  need  tight  control  over  the tradeoffs between availability, consistency, cost-effectiveness and performance.  Amazon's  platform  has  a  very  diverse set of applications  with  different  storage  requirements.  A  select  set  of applications requires a storage technology that is flexible enough to let application designers configure their data store appropriately based on these tradeoffs to achieve high availability and guaranteed performance in the most cost effective manner."}, {"self_ref": "#/texts/17", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 318.24, "t": 226.10300000000007, "r": 560.341, "b": 146.34799999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 499]}], "orig": "There  are  many  services  on  Amazon's  platform  that  only  need primary-key  access  to  a  data  store.  For  many  services,  such  as those  that  provide  best  seller  lists,  shopping  carts,  customer preferences, session management, sales rank, and product catalog, the common pattern of using a relational database would lead to inefficiencies and limit scale and availability. Dynamo provides a simple  primary-key  only  interface  to  meet  the  requirements  of these applications.", "text": "There  are  many  services  on  Amazon's  platform  that  only  need primary-key  access  to  a  data  store.  For  many  services,  such  as those  that  provide  best  seller  lists,  shopping  carts,  customer preferences, session management, sales rank, and product catalog, the common pattern of using a relational database would lead to inefficiencies and limit scale and availability. Dynamo provides a simple  primary-key  only  interface  to  meet  the  requirements  of these applications."}, {"self_ref": "#/texts/18", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 1, "bbox": {"l": 318.24, "t": 137.36300000000006, "r": 560.334, "b": 78.24800000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 400]}], "orig": "Dynamo  uses  a  synthesis  of  well  known  techniques  to  achieve scalability  and  availability:  Data  is  partitioned  and  replicated using  consistent  hashing  [10],  and  consistency  is  facilitated  by object  versioning  [12].  The  consistency  among  replicas  during updates is maintained by a quorum-like technique and a decentralized replica synchronization protocol. Dynamo employs", "text": "Dynamo  uses  a  synthesis  of  well  known  techniques  to  achieve scalability  and  availability:  Data  is  partitioned  and  replicated using  consistent  hashing  [10],  and  consistency  is  facilitated  by object  versioning  [12].  The  consistency  among  replicas  during updates is maintained by a quorum-like technique and a decentralized replica synchronization protocol. Dynamo employs"}, {"self_ref": "#/texts/19", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 1, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "195 20", "text": "195 20"}, {"self_ref": "#/texts/20", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 54.0, "t": 717.744, "r": 296.071, "b": 669.009, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 292]}], "orig": "a  gossip  based  distributed  failure  detection  and  membership protocol.  Dynamo  is  a  completely  decentralized  system  with minimal  need  for  manual  administration.  Storage  nodes  can  be added and removed from Dynamo without requiring any manual partitioning or redistribution.", "text": "a  gossip  based  distributed  failure  detection  and  membership protocol.  Dynamo  is  a  completely  decentralized  system  with minimal  need  for  manual  administration.  Storage  nodes  can  be added and removed from Dynamo without requiring any manual partitioning or redistribution."}, {"self_ref": "#/texts/21", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 54.0, "t": 660.024, "r": 296.089, "b": 569.889, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 567]}], "orig": "In  the  past  year,  Dynamo  has  been  the  underlying  storage technology  for  a  number  of  the  core  services  in  Amazon's  ecommerce  platform.  It  was  able  to  scale  to  extreme  peak  loads efficiently without any downtime during the busy holiday shopping season. For example, the service that maintains shopping  cart  (Shopping  Cart  Service)  served  tens  of  millions requests that resulted in well over 3 million checkouts in a single day and the service that manages session state handled hundreds of thousands of concurrently active sessions.", "text": "In  the  past  year,  Dynamo  has  been  the  underlying  storage technology  for  a  number  of  the  core  services  in  Amazon's  ecommerce  platform.  It  was  able  to  scale  to  extreme  peak  loads efficiently without any downtime during the busy holiday shopping season. For example, the service that maintains shopping  cart  (Shopping  Cart  Service)  served  tens  of  millions requests that resulted in well over 3 million checkouts in a single day and the service that manages session state handled hundreds of thousands of concurrently active sessions."}, {"self_ref": "#/texts/22", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 54.0, "t": 560.844, "r": 296.072, "b": 491.409, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 454]}], "orig": "The main contribution of this work for the research community is the  evaluation  of  how  different  techniques  can  be  combined  to provide a single highly-available system. It demonstrates that an eventually-consistent  storage  system  can  be  used  in  production with  demanding  applications.  It  also  provides  insight  into  the tuning of these techniques to meet the requirements of production systems with very strict performance demands.", "text": "The main contribution of this work for the research community is the  evaluation  of  how  different  techniques  can  be  combined  to provide a single highly-available system. It demonstrates that an eventually-consistent  storage  system  can  be  used  in  production with  demanding  applications.  It  also  provides  insight  into  the tuning of these techniques to meet the requirements of production systems with very strict performance demands."}, {"self_ref": "#/texts/23", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 54.0, "t": 482.425, "r": 296.104, "b": 361.21, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 778]}], "orig": "The  paper is structured as follows. Section 2 presents the background  and  Section  3  presents  the  related  work.  Section  4 presents the system design and Section 5 describes the implementation.  Section  6  details  the  experiences  and  insights gained by running Dynamo in production and Section 7 concludes the  paper.  There  are  a  number  of  places  in  this  paper  where additional  information  may  have  been  appropriate  but  where protecting Amazon's business interests require us to reduce some level  of  detail.  For  this  reason,  the  intra-  and  inter-datacenter latencies in section 6, the absolute request rates in section 6.2 and outage lengths and workloads in section 6.3 are provided through aggregate measures instead of absolute details.", "text": "The  paper is structured as follows. Section 2 presents the background  and  Section  3  presents  the  related  work.  Section  4 presents the system design and Section 5 describes the implementation.  Section  6  details  the  experiences  and  insights gained by running Dynamo in production and Section 7 concludes the  paper.  There  are  a  number  of  places  in  this  paper  where additional  information  may  have  been  appropriate  but  where protecting Amazon's business interests require us to reduce some level  of  detail.  For  this  reason,  the  intra-  and  inter-datacenter latencies in section 6, the absolute request rates in section 6.2 and outage lengths and workloads in section 6.3 are provided through aggregate measures instead of absolute details."}, {"self_ref": "#/texts/24", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 2, "bbox": {"l": 54.0, "t": 351.192, "r": 165.936, "b": 341.412, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "2. BACKGROUND", "text": "2. BACKGROUND", "level": 1}, {"self_ref": "#/texts/25", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 54.0, "t": 338.424, "r": 296.127, "b": 227.58899999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 698]}], "orig": "Amazon's  e-commerce  platform  is  composed  of  hundreds  of services that work in concert to deliver functionality ranging from recommendations  to  order  fulfillment  to  fraud  detection.  Each service is exposed  through  a  well defined interface and  is accessible  over  the  network.  These  services  are  hosted  in  an infrastructure that consists of tens of thousands of servers located across many data centers world-wide. Some of these services are stateless  (i.e.,  services  which  aggregate  responses  from  other services)  and  some  are  stateful  (i.e.,  a  service  that  generates  its response  by  executing  business  logic  on  its state stored in persistent store).", "text": "Amazon's  e-commerce  platform  is  composed  of  hundreds  of services that work in concert to deliver functionality ranging from recommendations  to  order  fulfillment  to  fraud  detection.  Each service is exposed  through  a  well defined interface and  is accessible  over  the  network.  These  services  are  hosted  in  an infrastructure that consists of tens of thousands of servers located across many data centers world-wide. Some of these services are stateless  (i.e.,  services  which  aggregate  responses  from  other services)  and  some  are  stateful  (i.e.,  a  service  that  generates  its response  by  executing  business  logic  on  its state stored in persistent store)."}, {"self_ref": "#/texts/26", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 54.0, "t": 218.60400000000004, "r": 296.105, "b": 87.06899999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 823]}], "orig": "Traditionally  production  systems  store  their  state  in  relational databases. For many of the more common usage patterns of state persistence, however, a relational database is a solution that is far from ideal. Most of these services only store and retrieve data by primary  key  and  do  not  require  the  complex  querying  and management  functionality  offered  by  an  RDBMS.  This  excess functionality  requires  expensive  hardware  and  highly  skilled personnel for its operation, making it a very inefficient solution. In addition, the available replication technologies are limited and typically  choose  consistency  over  availability.  Although  many advances have been made in the recent years, it is still not easy to scale-out  databases  or  use  smart  partitioning  schemes  for  load balancing.", "text": "Traditionally  production  systems  store  their  state  in  relational databases. For many of the more common usage patterns of state persistence, however, a relational database is a solution that is far from ideal. Most of these services only store and retrieve data by primary  key  and  do  not  require  the  complex  querying  and management  functionality  offered  by  an  RDBMS.  This  excess functionality  requires  expensive  hardware  and  highly  skilled personnel for its operation, making it a very inefficient solution. In addition, the available replication technologies are limited and typically  choose  consistency  over  availability.  Although  many advances have been made in the recent years, it is still not easy to scale-out  databases  or  use  smart  partitioning  schemes  for  load balancing."}, {"self_ref": "#/texts/27", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 717.744, "r": 560.355, "b": 648.309, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 451]}], "orig": "This  paper  describes  Dynamo,  a  highly  available  data  storage technology that addresses the needs of these important classes of services.  Dynamo  has  a  simple  key/value  interface,  is  highly available with a clearly defined consistency window, is efficient in its resource usage, and has a simple scale out scheme to address growth  in  data  set  size  or  request  rates.  Each  service  that  uses Dynamo runs its own Dynamo instances.", "text": "This  paper  describes  Dynamo,  a  highly  available  data  storage technology that addresses the needs of these important classes of services.  Dynamo  has  a  simple  key/value  interface,  is  highly available with a clearly defined consistency window, is efficient in its resource usage, and has a simple scale out scheme to address growth  in  data  set  size  or  request  rates.  Each  service  that  uses Dynamo runs its own Dynamo instances."}, {"self_ref": "#/texts/28", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 638.292, "r": 553.044, "b": 628.512, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 39]}], "orig": "2.1 System Assumptions and Requirements", "text": "2.1 System Assumptions and Requirements", "level": 1}, {"self_ref": "#/texts/29", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 625.524, "r": 560.157, "b": 607.809, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 87]}], "orig": "The  storage  system  for  this  class  of  services  has  the  following requirements:", "text": "The  storage  system  for  this  class  of  services  has  the  following requirements:"}, {"self_ref": "#/texts/30", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 598.825, "r": 560.355, "b": 508.689, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 592]}], "orig": "Query Model : simple read and write operations to a data item that is  uniquely  identified  by  a  key.  State  is  stored  as  binary  objects (i.e., blobs) identified by unique keys. No  operations span multiple  data  items  and  there  is  no  need  for  relational  schema. This  requirement  is  based  on  the  observation  that  a  significant portion  of  Amazon's  services  can  work  with  this  simple  query model  and  do  not  need  any  relational  schema.  Dynamo  targets applications  that  need  to  store  objects  that  are  relatively  small (usually less than 1 MB).", "text": "Query Model : simple read and write operations to a data item that is  uniquely  identified  by  a  key.  State  is  stored  as  binary  objects (i.e., blobs) identified by unique keys. No  operations span multiple  data  items  and  there  is  no  need  for  relational  schema. This  requirement  is  based  on  the  observation  that  a  significant portion  of  Amazon's  services  can  work  with  this  simple  query model  and  do  not  need  any  relational  schema.  Dynamo  targets applications  that  need  to  store  objects  that  are  relatively  small (usually less than 1 MB)."}, {"self_ref": "#/texts/31", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 499.645, "r": 560.305, "b": 388.809, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 692]}], "orig": "ACID Properties: ACID ( Atomicity, Consistency, Isolation, Durability )  is  a  set  of  properties  that  guarantee  that  database transactions are processed reliably. In the context of databases, a single logical  operation  on  the  data  is called a transaction. Experience  at  Amazon  has  shown  that  data  stores  that  provide ACID  guarantees  tend  to  have  poor  availability.  This  has  been widely  acknowledged  by  both  the  industry  and  academia  [5]. Dynamo targets applications that operate with weaker consistency (the  'C'  in  ACID)  if  this  results  in  high  availability.  Dynamo does not provide any isolation guarantees and permits only single key updates.", "text": "ACID Properties: ACID ( Atomicity, Consistency, Isolation, Durability )  is  a  set  of  properties  that  guarantee  that  database transactions are processed reliably. In the context of databases, a single logical  operation  on  the  data  is called a transaction. Experience  at  Amazon  has  shown  that  data  stores  that  provide ACID  guarantees  tend  to  have  poor  availability.  This  has  been widely  acknowledged  by  both  the  industry  and  academia  [5]. Dynamo targets applications that operate with weaker consistency (the  'C'  in  ACID)  if  this  results  in  high  availability.  Dynamo does not provide any isolation guarantees and permits only single key updates."}, {"self_ref": "#/texts/32", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 379.825, "r": 560.435, "b": 279.36799999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 634]}], "orig": "Efficiency : The system needs to function on  a  commodity hardware  infrastructure.  In  Amazon's  platform,  services  have stringent  latency  requirements  which  are  in  general  measured  at the  99.9 th percentile  of  the  distribution.  Given  that  state  access plays a crucial role in service operation the storage system must be  capable  of  meeting  such  stringent  SLAs  (see  Section  2.2 below). Services must be able to configure Dynamo such that they consistently  achieve  their  latency  and  throughput  requirements. The tradeoffs are in performance, cost efficiency, availability, and durability guarantees.", "text": "Efficiency : The system needs to function on  a  commodity hardware  infrastructure.  In  Amazon's  platform,  services  have stringent  latency  requirements  which  are  in  general  measured  at the  99.9 th percentile  of  the  distribution.  Given  that  state  access plays a crucial role in service operation the storage system must be  capable  of  meeting  such  stringent  SLAs  (see  Section  2.2 below). Services must be able to configure Dynamo such that they consistently  achieve  their  latency  and  throughput  requirements. The tradeoffs are in performance, cost efficiency, availability, and durability guarantees."}, {"self_ref": "#/texts/33", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 270.323, "r": 560.357, "b": 190.56799999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 498]}], "orig": "Other Assumptions: Dynamo is used only by Amazon's internal services.  Its  operation  environment is assumed to be non-hostile and there are no security related requirements such as authentication  and  authorization.  Moreover,  since  each  service uses  its  distinct  instance  of  Dynamo,  its  initial  design  targets  a scale  of  up  to  hundreds  of  storage  hosts.  We  will  discuss  the scalability limitations of Dynamo and possible scalability related extensions in later sections.", "text": "Other Assumptions: Dynamo is used only by Amazon's internal services.  Its  operation  environment is assumed to be non-hostile and there are no security related requirements such as authentication  and  authorization.  Moreover,  since  each  service uses  its  distinct  instance  of  Dynamo,  its  initial  design  targets  a scale  of  up  to  hundreds  of  storage  hosts.  We  will  discuss  the scalability limitations of Dynamo and possible scalability related extensions in later sections."}, {"self_ref": "#/texts/34", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 180.49199999999996, "r": 517.04, "b": 170.712, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 34]}], "orig": "2.2 Service Level Agreements (SLA)", "text": "2.2 Service Level Agreements (SLA)", "level": 1}, {"self_ref": "#/texts/35", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 2, "bbox": {"l": 318.24, "t": 167.72399999999993, "r": 560.34, "b": 77.58899999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 612]}], "orig": "To guarantee that the application can deliver its functionality in a bounded time, each and every dependency in the platform needs to  deliver  its  functionality  with even tighter  bounds.  Clients  and services engage in a Service Level Agreement (SLA), a formally negotiated contract where a client and a service agree on several system-related  characteristics,  which  most  prominently  include the client's expected request rate distribution for a particular API and  the  expected  service  latency  under  those  conditions.  An example  of  a  simple  SLA  is  a  service  guaranteeing  that  it  will", "text": "To guarantee that the application can deliver its functionality in a bounded time, each and every dependency in the platform needs to  deliver  its  functionality  with even tighter  bounds.  Clients  and services engage in a Service Level Agreement (SLA), a formally negotiated contract where a client and a service agree on several system-related  characteristics,  which  most  prominently  include the client's expected request rate distribution for a particular API and  the  expected  service  latency  under  those  conditions.  An example  of  a  simple  SLA  is  a  service  guaranteeing  that  it  will"}, {"self_ref": "#/texts/36", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 2, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "196 20", "text": "196 20"}, {"self_ref": "#/texts/37", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 71.88750073218746, "t": 780.0000000444444, "r": 115.81875073218748, "b": 774.6666667111111, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "Client Requests", "text": "Client Requests"}, {"self_ref": "#/texts/38", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 31.949999542382827, "t": 564.6666666, "r": 83.20312454238284, "b": 559.3333332666667, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 16]}], "orig": "Dynamo instances", "text": "Dynamo instances"}, {"self_ref": "#/texts/39", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 167.05201482990495, "t": 743.3813585303428, "r": 183.06673487642516, "b": 736.618641601225, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 4]}], "orig": "Page", "text": "Page"}, {"self_ref": "#/texts/40", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 161.02442144730045, "t": 736.3251542300877, "r": 189.75995423176244, "b": 730.3415122877457, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 9]}], "orig": "Rendering", "text": "Rendering"}, {"self_ref": "#/texts/41", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 157.73079052636888, "t": 730.1346708180356, "r": 193.05358439157274, "b": 723.8653290313331, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Components", "text": "Components"}, {"self_ref": "#/texts/42", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 3, "bbox": {"l": 60.42, "t": 480.804, "r": 288.083, "b": 463.148, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 60]}], "orig": "Figure 1: Service-oriented architecture of Amazon's platform", "text": "Figure 1: Service-oriented architecture of Amazon's platform"}, {"self_ref": "#/texts/43", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 73.8843753328125, "t": 702.0000000238095, "r": 120.4781253328125, "b": 696.0000000238095, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "Request Routing", "text": "Request Routing"}, {"self_ref": "#/texts/44", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 77.21249940925782, "t": 659.3333333, "r": 124.47187440925781, "b": 652.6666666333333, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "Request Routing", "text": "Request Routing"}, {"self_ref": "#/texts/45", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 134.44052834877385, "t": 594.7387641552045, "r": 152.4438470102252, "b": 590.5945692343469, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "Amazon", "text": "Amazon"}, {"self_ref": "#/texts/46", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 139.78125004754466, "t": 590.0000000285714, "r": 146.43750004754466, "b": 584.6666666952381, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "S3", "text": "S3"}, {"self_ref": "#/texts/47", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 169.73437526624997, "t": 680.0000000388889, "r": 201.68437526624996, "b": 674.0000000388889, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Aggregator", "text": "Aggregator"}, {"self_ref": "#/texts/48", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 173.0624997087891, "t": 674.000000075, "r": 197.6906247087891, "b": 668.000000075, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 8]}], "orig": "Services", "text": "Services"}, {"self_ref": "#/texts/49", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 181.04203236216665, "t": 653.3747299720508, "r": 205.02046811694265, "b": 648.6252700696053, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 8]}], "orig": "Services", "text": "Services"}, {"self_ref": "#/texts/50", "parent": {"$ref": "#/pictures/0"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 159.08437538273438, "t": 566.6666666944444, "r": 205.01250038273437, "b": 562.0000000277778, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 16]}], "orig": "Other datastores", "text": "Other datastores"}, {"self_ref": "#/texts/51", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 54.0, "t": 452.964, "r": 296.04, "b": 435.308, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 108]}], "orig": "provide a response within 300ms for 99.9% of its requests for a peak client load of 500 requests per second.", "text": "provide a response within 300ms for 99.9% of its requests for a peak client load of 500 requests per second."}, {"self_ref": "#/texts/52", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 54.0, "t": 426.324, "r": 296.097, "b": 325.809, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 609]}], "orig": "In  Amazon's  decentralized  service  oriented  infrastructure,  SLAs play an important role. For example a page request to one of the e-commerce  sites  typically requires the rendering engine to construct  its  response  by  sending  requests  to  over  150  services. These services often have multiple dependencies, which frequently are other services, and as such it is not uncommon for the call graph of an application to have more than one level. To ensure that the page rendering engine can maintain a clear bound on page delivery each service within the call chain must obey its performance contract.", "text": "In  Amazon's  decentralized  service  oriented  infrastructure,  SLAs play an important role. For example a page request to one of the e-commerce  sites  typically requires the rendering engine to construct  its  response  by  sending  requests  to  over  150  services. These services often have multiple dependencies, which frequently are other services, and as such it is not uncommon for the call graph of an application to have more than one level. To ensure that the page rendering engine can maintain a clear bound on page delivery each service within the call chain must obey its performance contract."}, {"self_ref": "#/texts/53", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 54.0, "t": 316.825, "r": 296.081, "b": 237.01, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 517]}], "orig": "Figure 1 shows an abstract view of the architecture of Amazon's platform,  where  dynamic  web  content  is  generated  by  page rendering components which in turn query many other services. A service can use different data stores to manage its state and these data stores are only accessible within its service boundaries. Some services  act  as  aggregators  by  using  several  other  services  to produce a composite response. Typically, the aggregator services are stateless, although they use extensive caching.", "text": "Figure 1 shows an abstract view of the architecture of Amazon's platform,  where  dynamic  web  content  is  generated  by  page rendering components which in turn query many other services. A service can use different data stores to manage its state and these data stores are only accessible within its service boundaries. Some services  act  as  aggregators  by  using  several  other  services  to produce a composite response. Typically, the aggregator services are stateless, although they use extensive caching."}, {"self_ref": "#/texts/54", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 54.0, "t": 228.02499999999998, "r": 296.1, "b": 75.78899999999999, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1009]}, {"page_no": 3, "bbox": {"l": 318.24, "t": 717.743, "r": 560.305, "b": 689.708, "coord_origin": "BOTTOMLEFT"}, "charspan": [1010, 1189]}], "orig": "A common approach in the industry  for  forming  a  performance oriented SLA is to describe it using average, median and expected variance.  At  Amazon  we  have  found  that  these  metrics  are  not good enough if the goal is to build a system where all customers have  a  good  experience,  rather  than  just  the  majority.   For example  if  extensive  personalization  techniques  are  used  then customers  with  longer  histories  require  more  processing  which impacts performance at the high-end of the distribution. An SLA stated in terms of mean or median response times will not address the performance of this important customer segment. To address this  issue,  at  Amazon,  SLAs  are  expressed  and  measured  at  the 99.9 th percentile of the distribution. The choice for 99.9% over an even  higher  percentile  has  been  made  based  on  a  cost-benefit analysis  which  demonstrated  a  significant  increase  in  cost  to improve  performance  that  much.  Experiences  with  Amazon's production  systems  have  shown  that  this  approach  provides  a better  overall  experience  compared  to  those  systems  that  meet SLAs defined based on the mean or median.", "text": "A common approach in the industry  for  forming  a  performance oriented SLA is to describe it using average, median and expected variance.  At  Amazon  we  have  found  that  these  metrics  are  not good enough if the goal is to build a system where all customers have  a  good  experience,  rather  than  just  the  majority.   For example  if  extensive  personalization  techniques  are  used  then customers  with  longer  histories  require  more  processing  which impacts performance at the high-end of the distribution. An SLA stated in terms of mean or median response times will not address the performance of this important customer segment. To address this  issue,  at  Amazon,  SLAs  are  expressed  and  measured  at  the 99.9 th percentile of the distribution. The choice for 99.9% over an even  higher  percentile  has  been  made  based  on  a  cost-benefit analysis  which  demonstrated  a  significant  increase  in  cost  to improve  performance  that  much.  Experiences  with  Amazon's production  systems  have  shown  that  this  approach  provides  a better  overall  experience  compared  to  those  systems  that  meet SLAs defined based on the mean or median."}, {"self_ref": "#/texts/55", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 318.24, "t": 682.776, "r": 560.342, "b": 590.589, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 555]}], "orig": "In this paper there are many references to this 99.9 th percentile of distributions,  which  reflects  Amazon  engineers'  relentless  focus on performance from the perspective of the customers' experience. Many papers report on averages, so these are included where  it  makes  sense  for  comparison  purposes.  Nevertheless, Amazon's engineering and optimization efforts are not focused on averages. Several techniques, such as the load balanced selection of write coordinators, are purely targeted at controlling performance at the 99.9 th percentile.", "text": "In this paper there are many references to this 99.9 th percentile of distributions,  which  reflects  Amazon  engineers'  relentless  focus on performance from the perspective of the customers' experience. Many papers report on averages, so these are included where  it  makes  sense  for  comparison  purposes.  Nevertheless, Amazon's engineering and optimization efforts are not focused on averages. Several techniques, such as the load balanced selection of write coordinators, are purely targeted at controlling performance at the 99.9 th percentile."}, {"self_ref": "#/texts/56", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 318.24, "t": 581.544, "r": 560.327, "b": 491.409, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 540]}], "orig": "Storage  systems  often  play  an  important  role  in  establishing  a service's SLA,  especially  if the business logic is relatively lightweight,  as  is  the  case  for  many  Amazon  services.  State management  then  becomes  the  main  component  of  a  service's SLA.  One  of  the  main  design  considerations  for  Dynamo  is  to give services control over their system properties, such as durability  and  consistency,  and  to  let  services  make  their  own tradeoffs between functionality, performance and costeffectiveness.", "text": "Storage  systems  often  play  an  important  role  in  establishing  a service's SLA,  especially  if the business logic is relatively lightweight,  as  is  the  case  for  many  Amazon  services.  State management  then  becomes  the  main  component  of  a  service's SLA.  One  of  the  main  design  considerations  for  Dynamo  is  to give services control over their system properties, such as durability  and  consistency,  and  to  let  services  make  their  own tradeoffs between functionality, performance and costeffectiveness."}, {"self_ref": "#/texts/57", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 3, "bbox": {"l": 318.24, "t": 481.392, "r": 464.952, "b": 471.612, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 25]}], "orig": "2.3 Design Considerations", "text": "2.3 Design Considerations", "level": 1}, {"self_ref": "#/texts/58", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 318.24, "t": 468.624, "r": 560.336, "b": 337.089, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 837]}], "orig": "Data replication algorithms used in commercial systems traditionally perform synchronous replica coordination in order to provide a strongly consistent data access interface. To achieve this level  of  consistency,  these  algorithms  are  forced  to  tradeoff  the availability of the data under certain failure scenarios. For instance, rather than dealing with the uncertainty of the correctness of an answer, the data is made unavailable until it is absolutely certain that it is correct. From the very early replicated database  works,  it  is  well  known  that  when  dealing  with  the possibility  of  network  failures,  strong  consistency  and  high  data availability  cannot  be  achieved  simultaneously  [2,  11].  As  such systems and applications need to be aware which properties can be achieved under which conditions.", "text": "Data replication algorithms used in commercial systems traditionally perform synchronous replica coordination in order to provide a strongly consistent data access interface. To achieve this level  of  consistency,  these  algorithms  are  forced  to  tradeoff  the availability of the data under certain failure scenarios. For instance, rather than dealing with the uncertainty of the correctness of an answer, the data is made unavailable until it is absolutely certain that it is correct. From the very early replicated database  works,  it  is  well  known  that  when  dealing  with  the possibility  of  network  failures,  strong  consistency  and  high  data availability  cannot  be  achieved  simultaneously  [2,  11].  As  such systems and applications need to be aware which properties can be achieved under which conditions."}, {"self_ref": "#/texts/59", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 318.24, "t": 328.104, "r": 560.32, "b": 237.96900000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 596]}], "orig": "For systems prone to server and network failures, availability can be  increased  by  using  optimistic  replication  techniques,  where changes are allowed to propagate to replicas in the background, and  concurrent,  disconnected  work  is  tolerated.  The  challenge with this approach is that it can lead to conflicting changes which must be detected and resolved.  This process of conflict resolution introduces two problems: when to resolve them and who resolves them.  Dynamo  is  designed  to  be  an  eventually  consistent  data store; that is all updates reach all replicas eventually.", "text": "For systems prone to server and network failures, availability can be  increased  by  using  optimistic  replication  techniques,  where changes are allowed to propagate to replicas in the background, and  concurrent,  disconnected  work  is  tolerated.  The  challenge with this approach is that it can lead to conflicting changes which must be detected and resolved.  This process of conflict resolution introduces two problems: when to resolve them and who resolves them.  Dynamo  is  designed  to  be  an  eventually  consistent  data store; that is all updates reach all replicas eventually."}, {"self_ref": "#/texts/60", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 3, "bbox": {"l": 318.24, "t": 228.92399999999998, "r": 560.362, "b": 76.68899999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 980]}], "orig": "An important design consideration is to decide when to  perform the  process  of  resolving  update  conflicts,  i.e.,  whether  conflicts should be resolved during reads or writes. Many traditional data stores execute conflict resolution during writes and keep the read complexity simple [7]. In such systems, writes may be rejected if the data store cannot reach all (or a majority of) the replicas at a given time. On the other hand, Dynamo targets the design space of an 'always writeable' data store (i.e., a data store that is highly available for writes). For a number of Amazon services, rejecting customer updates could result in a poor customer experience. For instance,  the  shopping  cart  service  must  allow  customers  to  add and remove items from their shopping cart even amidst network and  server  failures.  This  requirement  forces  us  to  push  the complexity of conflict  resolution  to  the  reads  in  order  to  ensure that writes are never rejected.", "text": "An important design consideration is to decide when to  perform the  process  of  resolving  update  conflicts,  i.e.,  whether  conflicts should be resolved during reads or writes. Many traditional data stores execute conflict resolution during writes and keep the read complexity simple [7]. In such systems, writes may be rejected if the data store cannot reach all (or a majority of) the replicas at a given time. On the other hand, Dynamo targets the design space of an 'always writeable' data store (i.e., a data store that is highly available for writes). For a number of Amazon services, rejecting customer updates could result in a poor customer experience. For instance,  the  shopping  cart  service  must  allow  customers  to  add and remove items from their shopping cart even amidst network and  server  failures.  This  requirement  forces  us  to  push  the complexity of conflict  resolution  to  the  reads  in  order  to  ensure that writes are never rejected."}, {"self_ref": "#/texts/61", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 3, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "197 20", "text": "197 20"}, {"self_ref": "#/texts/62", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 717.744, "r": 296.092, "b": 575.888, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 930]}], "orig": "The  next  design  choice  is who performs  the  process  of  conflict resolution. This can be done by the data store or the application. If conflict resolution is done by the data store, its choices are rather limited. In such cases, the data store can only use simple policies, such as 'last write wins' [22], to resolve conflicting updates. On the other hand, since the application is aware of the data schema it can decide on the conflict resolution method that is best suited for its client's experience. For instance, the application that maintains customer  shopping  carts  can  choose  to  'merge'  the  conflicting versions  and  return  a  single  unified  shopping  cart.  Despite  this flexibility,  some  application  developers  may  not  want  to  write their  own  conflict  resolution  mechanisms  and  choose  to  push  it down to the data store, which in turn chooses a simple policy such as 'last write wins'.", "text": "The  next  design  choice  is who performs  the  process  of  conflict resolution. This can be done by the data store or the application. If conflict resolution is done by the data store, its choices are rather limited. In such cases, the data store can only use simple policies, such as 'last write wins' [22], to resolve conflicting updates. On the other hand, since the application is aware of the data schema it can decide on the conflict resolution method that is best suited for its client's experience. For instance, the application that maintains customer  shopping  carts  can  choose  to  'merge'  the  conflicting versions  and  return  a  single  unified  shopping  cart.  Despite  this flexibility,  some  application  developers  may  not  want  to  write their  own  conflict  resolution  mechanisms  and  choose  to  push  it down to the data store, which in turn chooses a simple policy such as 'last write wins'."}, {"self_ref": "#/texts/63", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 566.844, "r": 231.002, "b": 559.508, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 48]}], "orig": "Other key principles embraced in the design are:", "text": "Other key principles embraced in the design are:"}, {"self_ref": "#/texts/64", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 550.524, "r": 296.056, "b": 512.109, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 212]}], "orig": "Incremental scalability :  Dynamo should be able to scale out one storage  host  (henceforth,  referred  to  as  ' node' )  at  a  time,  with minimal impact on both operators of the system and the system itself.", "text": "Incremental scalability :  Dynamo should be able to scale out one storage  host  (henceforth,  referred  to  as  ' node' )  at  a  time,  with minimal impact on both operators of the system and the system itself."}, {"self_ref": "#/texts/65", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 503.125, "r": 296.088, "b": 454.389, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 297]}], "orig": "Symmetry :  Every  node  in  Dynamo  should  have  the  same  set  of responsibilities as its peers; there should be no distinguished node or nodes that take special roles or extra set of responsibilities. In our experience, symmetry  simplifies the process of system provisioning and maintenance.", "text": "Symmetry :  Every  node  in  Dynamo  should  have  the  same  set  of responsibilities as its peers; there should be no distinguished node or nodes that take special roles or extra set of responsibilities. In our experience, symmetry  simplifies the process of system provisioning and maintenance."}, {"self_ref": "#/texts/66", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 445.345, "r": 296.099, "b": 396.609, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 312]}], "orig": "Decentralization :  An  extension  of  symmetry,  the  design  should favor decentralized peer-to-peer techniques over centralized control. In the past, centralized control has resulted in outages and the goal is to avoid it as much as possible. This leads to a simpler, more scalable, and more available system.", "text": "Decentralization :  An  extension  of  symmetry,  the  design  should favor decentralized peer-to-peer techniques over centralized control. In the past, centralized control has resulted in outages and the goal is to avoid it as much as possible. This leads to a simpler, more scalable, and more available system."}, {"self_ref": "#/texts/67", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 387.625, "r": 296.049, "b": 338.89, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 319]}], "orig": "Heterogeneity : The system needs to be able to exploit heterogeneity  in  the  infrastructure  it  runs  on.  e.g.  the  work distribution must be proportional to the capabilities of the individual  servers.  This  is  essential  in  adding  new  nodes  with higher capacity without having to upgrade all hosts at once.", "text": "Heterogeneity : The system needs to be able to exploit heterogeneity  in  the  infrastructure  it  runs  on.  e.g.  the  work distribution must be proportional to the capabilities of the individual  servers.  This  is  essential  in  adding  new  nodes  with higher capacity without having to upgrade all hosts at once."}, {"self_ref": "#/texts/68", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 328.812, "r": 175.354, "b": 319.032, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "3. RELATED WORK", "text": "3. RELATED WORK", "level": 1}, {"self_ref": "#/texts/69", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 313.032, "r": 192.858, "b": 303.252, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 24]}], "orig": "3.1 Peer to Peer Systems", "text": "3.1 Peer to Peer Systems", "level": 1}, {"self_ref": "#/texts/70", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 54.0, "t": 300.264, "r": 296.135, "b": 106.62900000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1223]}], "orig": "There are several peer-to-peer (P2P) systems that have looked at the problem of data storage and distribution. The first generation of P2P systems, such as Freenet and Gnutella 1 , were predominantly used as file sharing systems. These were examples of  unstructured  P2P  networks  where  the  overlay  links  between peers  were  established  arbitrarily.  In  these  networks,  a  search query  is  usually  flooded  through  the  network  to  find  as  many peers as possible that share the data. P2P systems evolved to the next  generation  into  what  is  widely  known  as  structured  P2P networks. These networks employ a globally consistent protocol to  ensure  that  any  node  can  efficiently  route  a  search  query  to some peer that has the desired data. Systems like Pastry [16] and Chord [20] use routing mechanisms to ensure that queries can be answered  within  a  bounded  number  of  hops.  To  reduce  the additional  latency  introduced  by  multi-hop  routing,  some  P2P systems (e.g., [14]) employ  O(1) routing where each peer maintains enough routing information locally so that it can route requests  (to  access  a  data  item)  to  the  appropriate  peer  within  a constant number of hops.", "text": "There are several peer-to-peer (P2P) systems that have looked at the problem of data storage and distribution. The first generation of P2P systems, such as Freenet and Gnutella 1 , were predominantly used as file sharing systems. These were examples of  unstructured  P2P  networks  where  the  overlay  links  between peers  were  established  arbitrarily.  In  these  networks,  a  search query  is  usually  flooded  through  the  network  to  find  as  many peers as possible that share the data. P2P systems evolved to the next  generation  into  what  is  widely  known  as  structured  P2P networks. These networks employ a globally consistent protocol to  ensure  that  any  node  can  efficiently  route  a  search  query  to some peer that has the desired data. Systems like Pastry [16] and Chord [20] use routing mechanisms to ensure that queries can be answered  within  a  bounded  number  of  hops.  To  reduce  the additional  latency  introduced  by  multi-hop  routing,  some  P2P systems (e.g., [14]) employ  O(1) routing where each peer maintains enough routing information locally so that it can route requests  (to  access  a  data  item)  to  the  appropriate  peer  within  a constant number of hops."}, {"self_ref": "#/texts/71", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "footnote", "prov": [{"page_no": 4, "bbox": {"l": 56.7, "t": 91.71600000000001, "r": 243.425, "b": 82.32899999999995, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 53]}], "orig": "1 http://freenetproject.org/, http://www.gnutella.org", "text": "1 http://freenetproject.org/, http://www.gnutella.org"}, {"self_ref": "#/texts/72", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 318.24, "t": 717.744, "r": 560.334, "b": 565.509, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 970]}], "orig": "Various storage systems, such as Oceanstore [9] and PAST [17] were built on top of these routing overlays. Oceanstore provides a global, transactional, persistent storage service that supports serialized updates on widely replicated data. To allow for concurrent updates while avoiding many of the problems inherent with wide-area locking, it uses an update model based on conflict resolution.  Conflict  resolution  was  introduced  in  [21]  to  reduce the number of transaction aborts. Oceanstore resolves conflicts by processing a series of updates, choosing a total order among them, and then applying them atomically in that order. It is built for an environment where the data is replicated on an untrusted infrastructure. By comparison, PAST provides a simple abstraction  layer  on  top  of  Pastry  for  persistent  and  immutable objects.  It  assumes  that  the  application  can  build  the  necessary storage semantics (such as mutable files) on top of it.", "text": "Various storage systems, such as Oceanstore [9] and PAST [17] were built on top of these routing overlays. Oceanstore provides a global, transactional, persistent storage service that supports serialized updates on widely replicated data. To allow for concurrent updates while avoiding many of the problems inherent with wide-area locking, it uses an update model based on conflict resolution.  Conflict  resolution  was  introduced  in  [21]  to  reduce the number of transaction aborts. Oceanstore resolves conflicts by processing a series of updates, choosing a total order among them, and then applying them atomically in that order. It is built for an environment where the data is replicated on an untrusted infrastructure. By comparison, PAST provides a simple abstraction  layer  on  top  of  Pastry  for  persistent  and  immutable objects.  It  assumes  that  the  application  can  build  the  necessary storage semantics (such as mutable files) on top of it."}, {"self_ref": "#/texts/73", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 4, "bbox": {"l": 318.24, "t": 555.492, "r": 552.708, "b": 545.712, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 42]}], "orig": "3.2 Distributed File Systems and Databases", "text": "3.2 Distributed File Systems and Databases", "level": 1}, {"self_ref": "#/texts/74", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 318.24, "t": 542.724, "r": 560.313, "b": 369.789, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1110]}], "orig": "Distributing data for performance, availability and durability has been  widely  studied  in  the  file  system  and  database  systems community. Compared to P2P storage systems that only support flat namespaces, distributed file systems typically support hierarchical namespaces. Systems like Ficus [15] and Coda [19] replicate files for high availability at the expense of consistency. Update conflicts are typically managed using specialized conflict resolution procedures. The Farsite system [1] is a distributed file system that does not use any centralized server like NFS. Farsite achieves  high  availability  and  scalability  using  replication.  The Google File System [6] is another distributed file system built for hosting  the  state  of  Google's  internal  applications.  GFS  uses  a simple  design  with  a  single  master  server  for  hosting  the  entire metadata  and  where  the  data  is  split  into  chunks  and  stored  in chunkservers.  Bayou  is  a  distributed  relational  database  system that  allows  disconnected  operations  and  provides  eventual  data consistency [21].", "text": "Distributing data for performance, availability and durability has been  widely  studied  in  the  file  system  and  database  systems community. Compared to P2P storage systems that only support flat namespaces, distributed file systems typically support hierarchical namespaces. Systems like Ficus [15] and Coda [19] replicate files for high availability at the expense of consistency. Update conflicts are typically managed using specialized conflict resolution procedures. The Farsite system [1] is a distributed file system that does not use any centralized server like NFS. Farsite achieves  high  availability  and  scalability  using  replication.  The Google File System [6] is another distributed file system built for hosting  the  state  of  Google's  internal  applications.  GFS  uses  a simple  design  with  a  single  master  server  for  hosting  the  entire metadata  and  where  the  data  is  split  into  chunks  and  stored  in chunkservers.  Bayou  is  a  distributed  relational  database  system that  allows  disconnected  operations  and  provides  eventual  data consistency [21]."}, {"self_ref": "#/texts/75", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 4, "bbox": {"l": 318.24, "t": 360.744, "r": 560.358, "b": 73.98900000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1904]}], "orig": "Among these systems, Bayou, Coda and Ficus allow disconnected operations  and  are  resilient  to  issues  such  as  network  partitions and  outages.  These  systems  differ  on  their  conflict  resolution procedures.  For  instance,  Coda  and  Ficus  perform  system  level conflict resolution and Bayou allows application level resolution. All of them, however, guarantee eventual consistency. Similar to these  systems,  Dynamo  allows  read  and  write  operations  to continue  even  during  network  partitions  and  resolves  updated conflicts using different conflict resolution mechanisms. Distributed  block  storage  systems  like  FAB  [18]  split  large  size objects  into  smaller  blocks  and  stores  each  block  in  a  highly available  manner.  In  comparison  to  these  systems,  a  key-value store  is  more  suitable  in  this  case  because:  (a)  it  is  intended  to store relatively small objects (size < 1M) and (b) key-value stores are  easier  to  configure  on  a  per-application  basis.  Antiquity  is  a wide-area distributed storage system designed to handle multiple server failures [23]. It uses a secure log to preserve data integrity, replicates  each  log  on  multiple  servers  for  durability,  and  uses Byzantine fault tolerance protocols to ensure data consistency. In contrast to Antiquity, Dynamo does not focus on the problem of data integrity and security and is built for a trusted environment. Bigtable  is  a  distributed  storage  system  for  managing  structured data.  It  maintains  a  sparse,  multi-dimensional  sorted  map  and allows  applications  to  access  their  data  using  multiple  attributes [2].  Compared  to  Bigtable,  Dynamo  targets  applications  that require only key/value access with primary focus on high availability  where  updates  are  not  rejected  even  in  the  wake  of network partitions or server failures.", "text": "Among these systems, Bayou, Coda and Ficus allow disconnected operations  and  are  resilient  to  issues  such  as  network  partitions and  outages.  These  systems  differ  on  their  conflict  resolution procedures.  For  instance,  Coda  and  Ficus  perform  system  level conflict resolution and Bayou allows application level resolution. All of them, however, guarantee eventual consistency. Similar to these  systems,  Dynamo  allows  read  and  write  operations  to continue  even  during  network  partitions  and  resolves  updated conflicts using different conflict resolution mechanisms. Distributed  block  storage  systems  like  FAB  [18]  split  large  size objects  into  smaller  blocks  and  stores  each  block  in  a  highly available  manner.  In  comparison  to  these  systems,  a  key-value store  is  more  suitable  in  this  case  because:  (a)  it  is  intended  to store relatively small objects (size < 1M) and (b) key-value stores are  easier  to  configure  on  a  per-application  basis.  Antiquity  is  a wide-area distributed storage system designed to handle multiple server failures [23]. It uses a secure log to preserve data integrity, replicates  each  log  on  multiple  servers  for  durability,  and  uses Byzantine fault tolerance protocols to ensure data consistency. In contrast to Antiquity, Dynamo does not focus on the problem of data integrity and security and is built for a trusted environment. Bigtable  is  a  distributed  storage  system  for  managing  structured data.  It  maintains  a  sparse,  multi-dimensional  sorted  map  and allows  applications  to  access  their  data  using  multiple  attributes [2].  Compared  to  Bigtable,  Dynamo  targets  applications  that require only key/value access with primary focus on high availability  where  updates  are  not  rejected  even  in  the  wake  of network partitions or server failures."}, {"self_ref": "#/texts/76", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 4, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "198 20", "text": "198 20"}, {"self_ref": "#/texts/77", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 5, "bbox": {"l": 60.96, "t": 560.964, "r": 288.559, "b": 543.308, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 62]}], "orig": "Figure 2: Partitioning and replication of keys in Dynamo ring.", "text": "Figure 2: Partitioning and replication of keys in Dynamo ring."}, {"self_ref": "#/texts/78", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 139.02, "t": 693.535, "r": 149.059, "b": 685.565, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "A", "text": "A"}, {"self_ref": "#/texts/79", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 177.96, "t": 671.515, "r": 187.401, "b": 663.545, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "B", "text": "B"}, {"self_ref": "#/texts/80", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 185.7, "t": 634.855, "r": 195.26, "b": 626.885, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "C", "text": "C"}, {"self_ref": "#/texts/81", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 162.36, "t": 598.195, "r": 172.459, "b": 590.225, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "D", "text": "D"}, {"self_ref": "#/texts/82", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 107.88, "t": 598.195, "r": 116.783, "b": 590.225, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "E", "text": "E"}, {"self_ref": "#/texts/83", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 84.54, "t": 634.855, "r": 92.84, "b": 626.885, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "F", "text": "F"}, {"self_ref": "#/texts/84", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 100.08, "t": 678.895, "r": 110.179, "b": 670.925, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "G", "text": "G"}, {"self_ref": "#/texts/85", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 183.84, "t": 710.455, "r": 213.739, "b": 702.485, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "Key K", "text": "Key K"}, {"self_ref": "#/texts/86", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 215.94, "t": 659.804, "r": 258.239, "b": 653.189, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Nodes B, C", "text": "Nodes B, C"}, {"self_ref": "#/texts/87", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 216.0, "t": 650.504, "r": 258.065, "b": 643.889, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "and D store", "text": "and D store"}, {"self_ref": "#/texts/88", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 223.68, "t": 641.084, "r": 250.442, "b": 634.469, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 7]}], "orig": "keys in", "text": "keys in"}, {"self_ref": "#/texts/89", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 215.28, "t": 631.724, "r": 258.781, "b": 625.109, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "range (A,B)", "text": "range (A,B)"}, {"self_ref": "#/texts/90", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 219.661, "t": 622.363, "r": 254.406, "b": 615.749, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 9]}], "orig": "including", "text": "including"}, {"self_ref": "#/texts/91", "parent": {"$ref": "#/pictures/1"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 231.781, "t": 612.943, "r": 242.339, "b": 606.328, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "K.", "text": "K."}, {"self_ref": "#/texts/92", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 54.0, "t": 530.844, "r": 296.097, "b": 461.409, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 432]}], "orig": "Traditional  replicated  relational  database  systems  focus  on  the problem  of  guaranteeing  strong  consistency  to  replicated  data. Although  strong  consistency  provides  the  application  writer  a convenient  programming  model,  these  systems  are  limited  in scalability and availability [7]. These systems are not capable of handling network partitions because they typically provide strong consistency guarantees.", "text": "Traditional  replicated  relational  database  systems  focus  on  the problem  of  guaranteeing  strong  consistency  to  replicated  data. Although  strong  consistency  provides  the  application  writer  a convenient  programming  model,  these  systems  are  limited  in scalability and availability [7]. These systems are not capable of handling network partitions because they typically provide strong consistency guarantees."}, {"self_ref": "#/texts/93", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 5, "bbox": {"l": 54.0, "t": 451.392, "r": 139.848, "b": 441.612, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 14]}], "orig": "3.3 Discussion", "text": "3.3 Discussion", "level": 1}, {"self_ref": "#/texts/94", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 54.0, "t": 438.624, "r": 296.146, "b": 213.909, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1468]}], "orig": "Dynamo  differs  from  the  aforementioned  decentralized  storage systems  in  terms  of  its  target  requirements.  First,  Dynamo  is targeted  mainly  at  applications  that  need  an  'always  writeable' data  store  where  no  updates  are  rejected  due  to  failures  or concurrent writes. This is a crucial requirement for many Amazon applications.  Second,  as  noted  earlier,  Dynamo  is  built  for  an infrastructure  within  a  single  administrative  domain  where  all nodes  are  assumed  to  be  trusted.  Third,  applications  that  use Dynamo  do  not  require  support  for  hierarchical  namespaces  (a norm  in many  file systems) or complex  relational  schema (supported by traditional databases). Fourth, Dynamo is built for latency  sensitive  applications  that  require  at  least  99.9%  of  read and  write  operations  to  be  performed  within  a  few  hundred milliseconds. To meet these stringent latency requirements, it was imperative for us to avoid routing requests through multiple nodes (which  is  the  typical  design  adopted  by  several  distributed  hash table  systems  such  as  Chord and  Pastry).  This  is  because  multihop  routing increases variability in response times, thereby increasing  the  latency  at  higher  percentiles.  Dynamo  can  be characterized  as  a  zero-hop  DHT,  where  each  node  maintains enough  routing  information  locally  to  route  a  request  to  the appropriate node directly.", "text": "Dynamo  differs  from  the  aforementioned  decentralized  storage systems  in  terms  of  its  target  requirements.  First,  Dynamo  is targeted  mainly  at  applications  that  need  an  'always  writeable' data  store  where  no  updates  are  rejected  due  to  failures  or concurrent writes. This is a crucial requirement for many Amazon applications.  Second,  as  noted  earlier,  Dynamo  is  built  for  an infrastructure  within  a  single  administrative  domain  where  all nodes  are  assumed  to  be  trusted.  Third,  applications  that  use Dynamo  do  not  require  support  for  hierarchical  namespaces  (a norm  in many  file systems) or complex  relational  schema (supported by traditional databases). Fourth, Dynamo is built for latency  sensitive  applications  that  require  at  least  99.9%  of  read and  write  operations  to  be  performed  within  a  few  hundred milliseconds. To meet these stringent latency requirements, it was imperative for us to avoid routing requests through multiple nodes (which  is  the  typical  design  adopted  by  several  distributed  hash table  systems  such  as  Chord and  Pastry).  This  is  because  multihop  routing increases variability in response times, thereby increasing  the  latency  at  higher  percentiles.  Dynamo  can  be characterized  as  a  zero-hop  DHT,  where  each  node  maintains enough  routing  information  locally  to  route  a  request  to  the appropriate node directly."}, {"self_ref": "#/texts/95", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 5, "bbox": {"l": 54.0, "t": 203.89200000000005, "r": 225.283, "b": 194.11199999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 22]}], "orig": "4. SYSTEM ARCHITECTURE", "text": "4. SYSTEM ARCHITECTURE", "level": 1}, {"self_ref": "#/texts/96", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 54.0, "t": 191.12400000000002, "r": 296.079, "b": 80.28899999999999, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 752]}], "orig": "The  architecture  of  a  storage  system  that  needs  to  operate  in  a production  setting  is  complex.  In  addition  to  the  actual  data persistence  component,  the  system  needs  to  have  scalable  and robust  solutions  for  load  balancing,  membership  and  failure detection, failure recovery, replica synchronization, overload handling, state transfer, concurrency and job scheduling, request marshalling,  request  routing,  system  monitoring  and  alarming, and configuration management. Describing the details of each of the  solutions  is  not  possible,  so  this  paper  focuses  on  the  core distributed  systems  techniques  used  in  Dynamo:  partitioning, replication, versioning, membership, failure handling and scaling.", "text": "The  architecture  of  a  storage  system  that  needs  to  operate  in  a production  setting  is  complex.  In  addition  to  the  actual  data persistence  component,  the  system  needs  to  have  scalable  and robust  solutions  for  load  balancing,  membership  and  failure detection, failure recovery, replica synchronization, overload handling, state transfer, concurrency and job scheduling, request marshalling,  request  routing,  system  monitoring  and  alarming, and configuration management. Describing the details of each of the  solutions  is  not  possible,  so  this  paper  focuses  on  the  core distributed  systems  techniques  used  in  Dynamo:  partitioning, replication, versioning, membership, failure handling and scaling."}, {"self_ref": "#/texts/97", "parent": {"$ref": "#/tables/0"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 5, "bbox": {"l": 334.319, "t": 719.184, "r": 541.58, "b": 701.528, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 67]}], "orig": "Table 1: Summary of techniques used in Dynamo and their advantages.", "text": "Table 1: Summary of techniques used in Dynamo and their advantages."}, {"self_ref": "#/texts/98", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 318.24, "t": 440.964, "r": 560.283, "b": 423.308, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 97]}], "orig": "Table 1 presents a summary of the list of techniques Dynamo uses and their respective advantages.", "text": "Table 1 presents a summary of the list of techniques Dynamo uses and their respective advantages."}, {"self_ref": "#/texts/99", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 5, "bbox": {"l": 318.24, "t": 413.292, "r": 439.398, "b": 403.512, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 20]}], "orig": "4.1 System Interface", "text": "4.1 System Interface", "level": 1}, {"self_ref": "#/texts/100", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 318.239, "t": 400.524, "r": 560.332, "b": 279.30899999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 814]}], "orig": "Dynamo  stores  objects  associated  with  a  key  through  a  simple interface; it exposes two operations: get() and put(). The get( key ) operation locates the object replicas associated with the key in the storage system and returns a single object or a list of objects with conflicting  versions  along  with  a context .  The  put( key,  context, object )  operation  determines  where  the  replicas  of  the object should  be  placed  based  on  the  associated key ,  and  writes  the replicas to disk. The context encodes system metadata about the object that is opaque to the caller and includes information such as the version of the object. The context information is stored along with  the  object  so  that  the  system  can  verify  the  validity  of  the context object supplied in the put request.", "text": "Dynamo  stores  objects  associated  with  a  key  through  a  simple interface; it exposes two operations: get() and put(). The get( key ) operation locates the object replicas associated with the key in the storage system and returns a single object or a list of objects with conflicting  versions  along  with  a context .  The  put( key,  context, object )  operation  determines  where  the  replicas  of  the object should  be  placed  based  on  the  associated key ,  and  writes  the replicas to disk. The context encodes system metadata about the object that is opaque to the caller and includes information such as the version of the object. The context information is stored along with  the  object  so  that  the  system  can  verify  the  validity  of  the context object supplied in the put request."}, {"self_ref": "#/texts/101", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 318.241, "t": 270.32500000000005, "r": 560.316, "b": 231.90999999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 256]}], "orig": "Dynamo treats both the key and the object supplied by the caller as an opaque array of bytes. It applies a MD5 hash on the key to generate  a  128-bit  identifier,  which  is  used  to  determine  the storage nodes that are responsible for serving the key.", "text": "Dynamo treats both the key and the object supplied by the caller as an opaque array of bytes. It applies a MD5 hash on the key to generate  a  128-bit  identifier,  which  is  used  to  determine  the storage nodes that are responsible for serving the key."}, {"self_ref": "#/texts/102", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 5, "bbox": {"l": 318.24, "t": 221.89200000000005, "r": 468.384, "b": 212.11199999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 26]}], "orig": "4.2 Partitioning Algorithm", "text": "4.2 Partitioning Algorithm", "level": 1}, {"self_ref": "#/texts/103", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 5, "bbox": {"l": 318.24, "t": 209.12400000000002, "r": 560.363, "b": 77.58899999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 908]}], "orig": "One of  the  key  design  requirements  for  Dynamo  is  that  it  must scale  incrementally.  This  requires  a  mechanism  to  dynamically partition the data over the set of nodes (i.e., storage hosts) in the system. Dynamo's partitioning scheme relies on consistent hashing  to  distribute  the  load  across  multiple  storage  hosts.  In consistent  hashing  [10],  the  output  range  of  a  hash  function  is treated  as  a  fixed  circular  space  or  'ring'  (i.e.  the  largest  hash value wraps around to the smallest hash value). Each node in the system  is  assigned  a  random  value  within  this  space  which represents its 'position' on the ring. Each data item identified by a key is  assigned to a node by hashing the data item's key to yield its  position  on  the  ring,  and  then  walking  the  ring  clockwise  to find the first node with a position larger than the item's position.", "text": "One of  the  key  design  requirements  for  Dynamo  is  that  it  must scale  incrementally.  This  requires  a  mechanism  to  dynamically partition the data over the set of nodes (i.e., storage hosts) in the system. Dynamo's partitioning scheme relies on consistent hashing  to  distribute  the  load  across  multiple  storage  hosts.  In consistent  hashing  [10],  the  output  range  of  a  hash  function  is treated  as  a  fixed  circular  space  or  'ring'  (i.e.  the  largest  hash value wraps around to the smallest hash value). Each node in the system  is  assigned  a  random  value  within  this  space  which represents its 'position' on the ring. Each data item identified by a key is  assigned to a node by hashing the data item's key to yield its  position  on  the  ring,  and  then  walking  the  ring  clockwise  to find the first node with a position larger than the item's position."}, {"self_ref": "#/texts/104", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 5, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "199 20", "text": "199 20"}, {"self_ref": "#/texts/105", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 717.744, "r": 296.115, "b": 669.009, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 298]}], "orig": "Thus,  each  node  becomes  responsible  for  the  region  in  the  ring between  it  and  its  predecessor  node  on  the  ring.  The  principle advantage  of  consistent  hashing  is  that  departure  or  arrival  of  a node only affects its immediate neighbors and other nodes remain unaffected.", "text": "Thus,  each  node  becomes  responsible  for  the  region  in  the  ring between  it  and  its  predecessor  node  on  the  ring.  The  principle advantage  of  consistent  hashing  is  that  departure  or  arrival  of  a node only affects its immediate neighbors and other nodes remain unaffected."}, {"self_ref": "#/texts/106", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 660.024, "r": 296.114, "b": 518.109, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 919]}], "orig": "The basic consistent hashing algorithm presents some challenges. First,  the  random  position  assignment  of  each  node  on  the  ring leads to non-uniform data and load distribution. Second, the basic algorithm is oblivious to the heterogeneity in the performance of nodes. To  address these issues, Dynamo  uses  a  variant  of consistent hashing (similar to the one used in [10, 20]): instead of mapping  a  node  to  a  single  point  in  the  circle,  each  node  gets assigned to multiple points in the ring. To this end, Dynamo uses the concept of 'virtual nodes'. A virtual node looks like a single node  in  the  system,  but  each  node  can  be  responsible  for  more than one virtual node. Effectively, when a new node is added to the system, it is assigned multiple positions (henceforth, 'tokens') in  the  ring.  The  process  of  fine-tuning  Dynamo's  partitioning scheme is discussed in Section 6.", "text": "The basic consistent hashing algorithm presents some challenges. First,  the  random  position  assignment  of  each  node  on  the  ring leads to non-uniform data and load distribution. Second, the basic algorithm is oblivious to the heterogeneity in the performance of nodes. To  address these issues, Dynamo  uses  a  variant  of consistent hashing (similar to the one used in [10, 20]): instead of mapping  a  node  to  a  single  point  in  the  circle,  each  node  gets assigned to multiple points in the ring. To this end, Dynamo uses the concept of 'virtual nodes'. A virtual node looks like a single node  in  the  system,  but  each  node  can  be  responsible  for  more than one virtual node. Effectively, when a new node is added to the system, it is assigned multiple positions (henceforth, 'tokens') in  the  ring.  The  process  of  fine-tuning  Dynamo's  partitioning scheme is discussed in Section 6."}, {"self_ref": "#/texts/107", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 509.125, "r": 236.295, "b": 501.79, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 49]}], "orig": "Using virtual nodes has the following advantages:", "text": "Using virtual nodes has the following advantages:"}, {"self_ref": "#/texts/108", "parent": {"$ref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 495.226, "r": 296.12, "b": 464.109, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 178]}], "orig": "\u00b7 If  a  node  becomes  unavailable  (due  to  failures  or  routine maintenance),  the  load  handled  by  this  node  is  evenly dispersed across the remaining available nodes.", "text": "If  a  node  becomes  unavailable  (due  to  failures  or  routine maintenance),  the  load  handled  by  this  node  is  evenly dispersed across the remaining available nodes.", "enumerated": false, "marker": "\u00b7"}, {"self_ref": "#/texts/109", "parent": {"$ref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 457.546, "r": 296.072, "b": 416.049, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 213]}], "orig": "\u00b7 When  a  node  becomes  available  again,  or  a  new  node  is added  to  the  system,  the  newly  available  node  accepts  a roughly  equivalent  amount  of  load  from  each  of  the  other available nodes.", "text": "When  a  node  becomes  available  again,  or  a  new  node  is added  to  the  system,  the  newly  available  node  accepts  a roughly  equivalent  amount  of  load  from  each  of  the  other available nodes.", "enumerated": false, "marker": "\u00b7"}, {"self_ref": "#/texts/110", "parent": {"$ref": "#/groups/0"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 409.486, "r": 296.046, "b": 378.369, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 159]}], "orig": "\u00b7 The number of virtual nodes that a node is responsible can decided  based  on  its  capacity,  accounting  for  heterogeneity in the physical infrastructure.", "text": "The number of virtual nodes that a node is responsible can decided  based  on  its  capacity,  accounting  for  heterogeneity in the physical infrastructure.", "enumerated": false, "marker": "\u00b7"}, {"self_ref": "#/texts/111", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 368.292, "r": 144.468, "b": 358.512, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 15]}], "orig": "4.3 Replication", "text": "4.3 Replication", "level": 1}, {"self_ref": "#/texts/112", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 355.524, "r": 296.105, "b": 234.36900000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 844]}], "orig": "To achieve high availability and durability, Dynamo replicates its data  on  multiple  hosts.  Each  data  item  is  replicated  at  N  hosts, where N is a parameter configured 'per-instance' . Each key, k , is assigned to a coordinator node (described in the previous section). The coordinator is in charge of the replication of the data items that  fall  within  its  range.  In  addition  to  locally  storing  each  key within its range, the coordinator replicates these keys at the N-1 clockwise  successor  nodes  in  the  ring.  This  results  in  a  system where each node is responsible for the region of the ring between it and its N th predecessor. In Figure 2, node B replicates the key k at  nodes  C  and  D  in  addition  to  storing  it  locally.  Node  D  will store the keys that fall in the ranges (A, B], (B, C], and (C, D].", "text": "To achieve high availability and durability, Dynamo replicates its data  on  multiple  hosts.  Each  data  item  is  replicated  at  N  hosts, where N is a parameter configured 'per-instance' . Each key, k , is assigned to a coordinator node (described in the previous section). The coordinator is in charge of the replication of the data items that  fall  within  its  range.  In  addition  to  locally  storing  each  key within its range, the coordinator replicates these keys at the N-1 clockwise  successor  nodes  in  the  ring.  This  results  in  a  system where each node is responsible for the region of the ring between it and its N th predecessor. In Figure 2, node B replicates the key k at  nodes  C  and  D  in  addition  to  storing  it  locally.  Node  D  will store the keys that fall in the ranges (A, B], (B, C], and (C, D]."}, {"self_ref": "#/texts/113", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 225.32400000000007, "r": 296.119, "b": 114.48900000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 780]}], "orig": "The list of nodes that is responsible for storing a particular key is called  the preference  list .  The  system  is  designed,  as  will  be explained  in  Section  4.8,  so  that  every  node  in  the  system  can determine  which  nodes  should  be  in  this  list  for  any  particular key.  To account for node failures, preference list contains more than N nodes. Note that with the use of virtual nodes, it is possible that  the  first  N  successor  positions  for  a  particular  key  may  be owned  by  less  than  N  distinct  physical  nodes  (i.e.  a  node  may hold more than one of the first N positions). To address this, the preference list for a key is constructed by skipping positions in the ring to ensure that the list contains only distinct physical nodes.", "text": "The list of nodes that is responsible for storing a particular key is called  the preference  list .  The  system  is  designed,  as  will  be explained  in  Section  4.8,  so  that  every  node  in  the  system  can determine  which  nodes  should  be  in  this  list  for  any  particular key.  To account for node failures, preference list contains more than N nodes. Note that with the use of virtual nodes, it is possible that  the  first  N  successor  positions  for  a  particular  key  may  be owned  by  less  than  N  distinct  physical  nodes  (i.e.  a  node  may hold more than one of the first N positions). To address this, the preference list for a key is constructed by skipping positions in the ring to ensure that the list contains only distinct physical nodes."}, {"self_ref": "#/texts/114", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 104.47199999999998, "r": 172.447, "b": 94.69200000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 19]}], "orig": "4.4 Data Versioning", "text": "4.4 Data Versioning", "level": 1}, {"self_ref": "#/texts/115", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 54.0, "t": 91.70399999999995, "r": 296.006, "b": 74.048, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 128]}, {"page_no": 6, "bbox": {"l": 318.24, "t": 717.744, "r": 560.345, "b": 648.309, "coord_origin": "BOTTOMLEFT"}, "charspan": [129, 612]}], "orig": "Dynamo provides eventual consistency, which allows for updates to be propagated to all replicas asynchronously. A put() call may return  to  its  caller  before  the  update  has  been  applied  at  all  the replicas,  which  can  result  in  scenarios  where  a  subsequent  get() operation  may  return  an  object  that  does  not  have  the  latest updates..  If  there  are  no  failures  then  there  is  a  bound  on  the update propagation times. However, under certain failure scenarios (e.g., server outages or network partitions), updates may not arrive at all replicas for an extended period of time.", "text": "Dynamo provides eventual consistency, which allows for updates to be propagated to all replicas asynchronously. A put() call may return  to  its  caller  before  the  update  has  been  applied  at  all  the replicas,  which  can  result  in  scenarios  where  a  subsequent  get() operation  may  return  an  object  that  does  not  have  the  latest updates..  If  there  are  no  failures  then  there  is  a  bound  on  the update propagation times. However, under certain failure scenarios (e.g., server outages or network partitions), updates may not arrive at all replicas for an extended period of time."}, {"self_ref": "#/texts/116", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 318.24, "t": 639.324, "r": 560.36, "b": 497.409, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 954]}], "orig": "There is a category of applications in Amazon's platform that can tolerate  such  inconsistencies  and  can  be  constructed  to  operate under these conditions. For example, the shopping cart application requires that an ' Add to Cart' operation can never be forgotten or rejected. If the most recent state of the cart is unavailable, and a user makes changes to an older version of the cart, that change is still meaningful and should be preserved. But at the same time it shouldn't  supersede  the  currently  unavailable  state  of  the  cart, which itself may contain changes that should be preserved. Note that both ' add to cart ' and ' delete item from cart ' operations are translated into put requests to Dynamo. When a customer wants to add  an  item  to  (or  remove  from)  a  shopping  cart  and  the  latest version is not available, the item is added to (or removed from) the older version and the divergent versions are reconciled later.", "text": "There is a category of applications in Amazon's platform that can tolerate  such  inconsistencies  and  can  be  constructed  to  operate under these conditions. For example, the shopping cart application requires that an ' Add to Cart' operation can never be forgotten or rejected. If the most recent state of the cart is unavailable, and a user makes changes to an older version of the cart, that change is still meaningful and should be preserved. But at the same time it shouldn't  supersede  the  currently  unavailable  state  of  the  cart, which itself may contain changes that should be preserved. Note that both ' add to cart ' and ' delete item from cart ' operations are translated into put requests to Dynamo. When a customer wants to add  an  item  to  (or  remove  from)  a  shopping  cart  and  the  latest version is not available, the item is added to (or removed from) the older version and the divergent versions are reconciled later."}, {"self_ref": "#/texts/117", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 318.24, "t": 488.425, "r": 562.606, "b": 325.869, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1027]}], "orig": "In  order  to  provide  this  kind  of  guarantee,  Dynamo  treats  the result of each modification as a new and immutable version of the data. It allows for multiple versions of an object to be present in the  system  at  the  same  time.  Most  of  the  time,  new  versions subsume  the  previous  version(s), and  the system  itself can determine the authoritative version (syntactic reconciliation). However,  version  branching  may  happen,  in  the  presence  of failures combined with concurrent updates, resulting in conflicting versions of an object. In these cases, the system cannot reconcile the multiple versions of the same object and the client must  perform  the  reconciliation  in  order  to collapse multiple branches of data evolution back into one (semantic reconciliation).  A  typical  example  of  a  collapse  operation  is 'merging' different versions of a customer's shopping cart. Using this reconciliation mechanism, an 'add to cart' operation is never lost. However, deleted items can resurface.", "text": "In  order  to  provide  this  kind  of  guarantee,  Dynamo  treats  the result of each modification as a new and immutable version of the data. It allows for multiple versions of an object to be present in the  system  at  the  same  time.  Most  of  the  time,  new  versions subsume  the  previous  version(s), and  the system  itself can determine the authoritative version (syntactic reconciliation). However,  version  branching  may  happen,  in  the  presence  of failures combined with concurrent updates, resulting in conflicting versions of an object. In these cases, the system cannot reconcile the multiple versions of the same object and the client must  perform  the  reconciliation  in  order  to collapse multiple branches of data evolution back into one (semantic reconciliation).  A  typical  example  of  a  collapse  operation  is 'merging' different versions of a customer's shopping cart. Using this reconciliation mechanism, an 'add to cart' operation is never lost. However, deleted items can resurface."}, {"self_ref": "#/texts/118", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 318.24, "t": 316.825, "r": 560.283, "b": 237.06899999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 544]}], "orig": "It  is  important  to  understand  that  certain  failure  modes  can potentially  result  in  the  system  having  not  just  two  but  several versions  of  the  same  data.  Updates  in  the  presence  of  network partitions  and  node  failures  can  potentially  result  in  an  object having distinct version sub-histories, which the system will need to reconcile in the future. This requires us to design applications that explicitly acknowledge the possibility of multiple versions of the same data (in order to never lose any updates).", "text": "It  is  important  to  understand  that  certain  failure  modes  can potentially  result  in  the  system  having  not  just  two  but  several versions  of  the  same  data.  Updates  in  the  presence  of  network partitions  and  node  failures  can  potentially  result  in  an  object having distinct version sub-histories, which the system will need to reconcile in the future. This requires us to design applications that explicitly acknowledge the possibility of multiple versions of the same data (in order to never lose any updates)."}, {"self_ref": "#/texts/119", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 318.24, "t": 228.024, "r": 560.337, "b": 127.56899999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 654]}], "orig": "Dynamo  uses  vector  clocks  [12]  in  order  to  capture  causality between different versions of the same object. A vector clock is effectively  a  list  of  (node,  counter)  pairs.  One  vector  clock  is associated with every version of every object. One can determine whether two versions of an object are on parallel branches or have a causal ordering, by examine their vector clocks. If the counters on the first object's clock are less-than-or-equal to all of the nodes in the second clock, then the first is an ancestor of the second and can be forgotten. Otherwise, the two changes are considered to be in conflict and require reconciliation.", "text": "Dynamo  uses  vector  clocks  [12]  in  order  to  capture  causality between different versions of the same object. A vector clock is effectively  a  list  of  (node,  counter)  pairs.  One  vector  clock  is associated with every version of every object. One can determine whether two versions of an object are on parallel branches or have a causal ordering, by examine their vector clocks. If the counters on the first object's clock are less-than-or-equal to all of the nodes in the second clock, then the first is an ancestor of the second and can be forgotten. Otherwise, the two changes are considered to be in conflict and require reconciliation."}, {"self_ref": "#/texts/120", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 6, "bbox": {"l": 318.24, "t": 118.524, "r": 560.323, "b": 80.168, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 273]}], "orig": "In  Dynamo,  when  a  client  wishes  to  update  an  object,  it  must specify which version it is updating. This is done by passing the context it obtained from an earlier read operation, which contains the  vector  clock  information.  Upon processing a read request, if", "text": "In  Dynamo,  when  a  client  wishes  to  update  an  object,  it  must specify which version it is updating. This is done by passing the context it obtained from an earlier read operation, which contains the  vector  clock  information.  Upon processing a read request, if"}, {"self_ref": "#/texts/121", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 6, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "200 1", "text": "200 1"}, {"self_ref": "#/texts/122", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 108.66666682222221, "t": 778.021875044375, "r": 127.33333348888887, "b": 771.365625044375, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}, {"page_no": 7, "bbox": {"l": 87.33333380555554, "t": 772.6968750055469, "r": 144.00000047222224, "b": 764.0437500055469, "coord_origin": "BOTTOMLEFT"}, "charspan": [6, 19]}], "orig": "write handled by Sx", "text": "write handled by Sx"}, {"self_ref": "#/texts/123", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 65.30021947059214, "t": 751.5562076828785, "r": 116.03311436805308, "b": 740.587542100937, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 12]}], "orig": "D1 ([Sx, 1])", "text": "D1 ([Sx, 1])"}, {"self_ref": "#/texts/124", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 110.62266134422346, "t": 732.2313475702452, "r": 135.37733816307113, "b": 723.968652370994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "write", "text": "write"}, {"self_ref": "#/texts/125", "parent": {"$ref": "#/pictures/2"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 67.28458701579768, "t": 699.7343064045436, "r": 118.71541383871435, "b": 689.2375687038428, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "D2 ([Sx,2])", "text": "D2 ([Sx,2])"}, {"self_ref": "#/texts/126", "parent": {"$ref": "#/pictures/2"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 123.3333335111111, "t": 680.8406250388281, "r": 144.66666684444445, "b": 674.8500000388282, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "write", "text": "write"}, {"self_ref": "#/texts/127", "parent": {"$ref": "#/pictures/2"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 107.33333288333334, "t": 674.1843749611719, "r": 161.33333288333333, "b": 666.8624999611719, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "handled by Sz", "text": "handled by Sz"}, {"self_ref": "#/texts/128", "parent": {"$ref": "#/pictures/2"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 110.6666671074074, "t": 650.8875000332813, "r": 190.00000044074076, "b": 640.9031250332813, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 19]}], "orig": "D4 ([Sx,2],[Sz, 1])", "text": "D4 ([Sx,2],[Sz, 1])"}, {"self_ref": "#/texts/129", "parent": {"$ref": "#/pictures/2"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 109.33333288666667, "t": 627.5906249600625, "r": 153.9999995533333, "b": 622.2656249600625, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "reconciled", "text": "reconciled"}, {"self_ref": "#/texts/130", "parent": {"$ref": "#/pictures/2"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 106.64399771856996, "t": 621.1229708803622, "r": 160.68933666713767, "b": 614.0895286617605, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 14]}], "orig": "and written by", "text": "and written by"}, {"self_ref": "#/texts/131", "parent": {"$ref": "#/pictures/2"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 130.66666658333335, "t": 612.9468749722656, "r": 140.66666658333335, "b": 606.9562499722656, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "SX", "text": "SX"}, {"self_ref": "#/texts/132", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 95.95356235263289, "t": 725.671241624077, "r": 152.0464375153933, "b": 713.8881335386831, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}, {"page_no": 7, "bbox": {"l": 30.000000220000004, "t": 680.8406250332812, "r": 52.000000220000004, "b": 673.5187500332812, "coord_origin": "BOTTOMLEFT"}, "charspan": [14, 19]}], "orig": "handled by Sx write", "text": "handled by Sx write"}, {"self_ref": "#/texts/133", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 14.584732306586526, "t": 680.4559401678514, "r": 89.4152681952584, "b": 657.9284350799988, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 13]}], "orig": "handled by sy", "text": "handled by sy"}, {"self_ref": "#/texts/134", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 9.999999504166672, "t": 650.8874999791992, "r": 89.33333283750001, "b": 640.9031249791992, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 19]}], "orig": "D3 ([Sx,2],[Sy, 1])", "text": "D3 ([Sx,2],[Sy, 1])"}, {"self_ref": "#/texts/135", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 40.666666085185206, "t": 597.6374999741146, "r": 145.33333275185186, "b": 587.6531249741146, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 26]}], "orig": "D5 ([Sx,3],[Sy, 1][Sz, 1])", "text": "D5 ([Sx,3],[Sy, 1][Sz, 1])"}, {"self_ref": "#/texts/136", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 7, "bbox": {"l": 64.26, "t": 513.684, "r": 259.789, "b": 506.349, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 51]}], "orig": "Figure 3: Version evolution of an object over time.", "text": "Figure 3: Version evolution of an object over time."}, {"self_ref": "#/texts/137", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 54.0, "t": 493.464, "r": 296.109, "b": 434.408, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 353]}], "orig": "Dynamo has access to multiple branches that cannot be syntactically reconciled, it will return all the objects at the leaves, with  the  corresponding  version  information  in  the  context.  An update  using  this  context  is  considered  to  have  reconciled  the divergent  versions  and  the  branches  are  collapsed  into  a  single new version.", "text": "Dynamo has access to multiple branches that cannot be syntactically reconciled, it will return all the objects at the leaves, with  the  corresponding  version  information  in  the  context.  An update  using  this  context  is  considered  to  have  reconciled  the divergent  versions  and  the  branches  are  collapsed  into  a  single new version."}, {"self_ref": "#/texts/138", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 54.0, "t": 425.424, "r": 296.08, "b": 293.889, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 866]}], "orig": "To illustrate the use of vector clocks, let us consider the example shown in Figure 3.  A client writes a new object. The node (say Sx)  that  handles  the  write  for  this  key  increases  its  sequence number and uses it to create the data's vector clock. The system now  has  the  object  D1  and  its  associated  clock  [(Sx,  1)].  The client  updates  the  object.  Assume  the  same  node  handles  this request  as  well.  The  system  now  also  has  object  D2  and  its associated  clock  [(Sx,  2)].  D2 descends from  D1  and  therefore over-writes D1, however there may be replicas of D1 lingering at nodes  that  have  not  yet  seen  D2.  Let  us  assume  that  the  same client  updates  the  object  again  and  a  different  server  (say  Sy) handles  the  request.  The  system  now  has  data  D3  and  its associated clock [(Sx, 2), (Sy, 1)].", "text": "To illustrate the use of vector clocks, let us consider the example shown in Figure 3.  A client writes a new object. The node (say Sx)  that  handles  the  write  for  this  key  increases  its  sequence number and uses it to create the data's vector clock. The system now  has  the  object  D1  and  its  associated  clock  [(Sx,  1)].  The client  updates  the  object.  Assume  the  same  node  handles  this request  as  well.  The  system  now  also  has  object  D2  and  its associated  clock  [(Sx,  2)].  D2 descends from  D1  and  therefore over-writes D1, however there may be replicas of D1 lingering at nodes  that  have  not  yet  seen  D2.  Let  us  assume  that  the  same client  updates  the  object  again  and  a  different  server  (say  Sy) handles  the  request.  The  system  now  has  data  D3  and  its associated clock [(Sx, 2), (Sy, 1)]."}, {"self_ref": "#/texts/139", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 54.001, "t": 284.844, "r": 296.075, "b": 184.38800000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 680]}], "orig": "Next assume a different client reads D2 and then tries to update it, and another node (say Sz) does the write. The system now has D4 (descendant  of  D2)  whose  version  clock  is  [(Sx,  2),  (Sz,  1)].  A node that is aware of D1 or D2 could determine, upon receiving D4 and its clock, that D1 and D2 are overwritten by the new data and  can  be  garbage  collected.  A  node  that  is  aware  of  D3  and receives  D4  will  find  that  there  is  no  causal  relation  between them. In other words, there are changes in D3 and D4 that are not reflected in each other. Both versions of the data must be kept and presented to a client (upon a read) for semantic reconciliation.", "text": "Next assume a different client reads D2 and then tries to update it, and another node (say Sz) does the write. The system now has D4 (descendant  of  D2)  whose  version  clock  is  [(Sx,  2),  (Sz,  1)].  A node that is aware of D1 or D2 could determine, upon receiving D4 and its clock, that D1 and D2 are overwritten by the new data and  can  be  garbage  collected.  A  node  that  is  aware  of  D3  and receives  D4  will  find  that  there  is  no  causal  relation  between them. In other words, there are changes in D3 and D4 that are not reflected in each other. Both versions of the data must be kept and presented to a client (upon a read) for semantic reconciliation."}, {"self_ref": "#/texts/140", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 54.001, "t": 175.34400000000005, "r": 296.141, "b": 105.96799999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 419]}], "orig": "Now assume some client reads both D3 and D4 (the context will reflect  that  both  values  were  found  by  the  read).  The  read's context is a summary of the clocks of D3 and D4, namely [(Sx, 2), (Sy, 1), (Sz, 1)]. If the client performs the reconciliation and node Sx coordinates the write, Sx will update its sequence number in the clock. The new data D5 will have the following clock: [(Sx, 3), (Sy, 1), (Sz, 1)].", "text": "Now assume some client reads both D3 and D4 (the context will reflect  that  both  values  were  found  by  the  read).  The  read's context is a summary of the clocks of D3 and D4, namely [(Sx, 2), (Sy, 1), (Sz, 1)]. If the client performs the reconciliation and node Sx coordinates the write, Sx will update its sequence number in the clock. The new data D5 will have the following clock: [(Sx, 3), (Sy, 1), (Sz, 1)]."}, {"self_ref": "#/texts/141", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 54.001, "t": 96.923, "r": 296.082, "b": 79.26800000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 143]}, {"page_no": 7, "bbox": {"l": 318.241, "t": 717.743, "r": 560.383, "b": 565.508, "coord_origin": "BOTTOMLEFT"}, "charspan": [144, 1164]}], "orig": "A  possible  issue  with  vector  clocks  is  that  the  size  of  vector clocks  may  grow  if  many  servers  coordinate  the  writes  to  an object. In practice, this is not likely because the writes are usually handled by one of the top N nodes in the preference list. In case of network partitions or multiple server failures, write requests may be  handled  by  nodes  that  are  not  in  the  top  N  nodes  in  the preference list causing the size of vector clock to grow. In these scenarios, it is desirable to limit the size of vector clock. To this end,  Dynamo  employs  the  following  clock  truncation  scheme: Along with each (node, counter) pair, Dynamo stores a timestamp that indicates the last time the node updated the data item. When the number of (node, counter) pairs in the vector clock reaches a threshold  (say  10),  the  oldest  pair  is  removed  from  the  clock. Clearly,  this  truncation  scheme  can  lead  to  inefficiencies  in reconciliation  as  the  descendant  relationships  cannot  be  derived accurately. However, this problem has not surfaced in production and therefore this issue has not been thoroughly investigated.", "text": "A  possible  issue  with  vector  clocks  is  that  the  size  of  vector clocks  may  grow  if  many  servers  coordinate  the  writes  to  an object. In practice, this is not likely because the writes are usually handled by one of the top N nodes in the preference list. In case of network partitions or multiple server failures, write requests may be  handled  by  nodes  that  are  not  in  the  top  N  nodes  in  the preference list causing the size of vector clock to grow. In these scenarios, it is desirable to limit the size of vector clock. To this end,  Dynamo  employs  the  following  clock  truncation  scheme: Along with each (node, counter) pair, Dynamo stores a timestamp that indicates the last time the node updated the data item. When the number of (node, counter) pairs in the vector clock reaches a threshold  (say  10),  the  oldest  pair  is  removed  from  the  clock. Clearly,  this  truncation  scheme  can  lead  to  inefficiencies  in reconciliation  as  the  descendant  relationships  cannot  be  derived accurately. However, this problem has not surfaced in production and therefore this issue has not been thoroughly investigated."}, {"self_ref": "#/texts/142", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 7, "bbox": {"l": 318.24, "t": 555.492, "r": 554.486, "b": 545.712, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 45]}], "orig": "4.5 Execution of get () and put () operations", "text": "4.5 Execution of get () and put () operations", "level": 1}, {"self_ref": "#/texts/143", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 318.24, "t": 542.724, "r": 560.328, "b": 493.989, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 312]}], "orig": "Any storage node in Dynamo is eligible to receive client get and put operations for any key. In this section, for sake of simplicity, we describe how these operations are performed in a failure-free environment and in the subsequent section we describe how read and write operations are executed during failures.", "text": "Any storage node in Dynamo is eligible to receive client get and put operations for any key. In this section, for sake of simplicity, we describe how these operations are performed in a failure-free environment and in the subsequent section we describe how read and write operations are executed during failures."}, {"self_ref": "#/texts/144", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 318.24, "t": 484.944, "r": 560.337, "b": 384.489, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 626]}], "orig": "Both get and put operations are invoked using  Amazon's infrastructure-specific request processing framework over HTTP. There are two strategies that a client can use to select a node: (1) route its request through a generic load balancer that will select a node based on load information, or (2) use a partition-aware client library that routes requests directly to the appropriate coordinator nodes. The advantage of the first approach is that the client does not have to link any code specific to Dynamo in its application, whereas the second strategy can achieve lower latency because it skips a potential forwarding step.", "text": "Both get and put operations are invoked using  Amazon's infrastructure-specific request processing framework over HTTP. There are two strategies that a client can use to select a node: (1) route its request through a generic load balancer that will select a node based on load information, or (2) use a partition-aware client library that routes requests directly to the appropriate coordinator nodes. The advantage of the first approach is that the client does not have to link any code specific to Dynamo in its application, whereas the second strategy can achieve lower latency because it skips a potential forwarding step."}, {"self_ref": "#/texts/145", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 318.24, "t": 375.444, "r": 560.357, "b": 295.688, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 554]}], "orig": "A  node  handling  a  read  or  write  operation  is  known  as  the coordinator . Typically, this is the first among the top N nodes in the  preference  list.  If  the  requests  are  received  through  a  load balancer, requests to access a key may be routed to any random node  in  the  ring.  In  this  scenario,  the  node  that  receives  the request will not coordinate it if the node is not in the top N of the requested key's preference list. Instead, that node will forward the request to the first among the top N nodes in the preference list.", "text": "A  node  handling  a  read  or  write  operation  is  known  as  the coordinator . Typically, this is the first among the top N nodes in the  preference  list.  If  the  requests  are  received  through  a  load balancer, requests to access a key may be routed to any random node  in  the  ring.  In  this  scenario,  the  node  that  receives  the request will not coordinate it if the node is not in the top N of the requested key's preference list. Instead, that node will forward the request to the first among the top N nodes in the preference list."}, {"self_ref": "#/texts/146", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 318.24, "t": 286.704, "r": 560.333, "b": 227.58899999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 355]}], "orig": "Read and write operations involve the first N healthy nodes in the preference list, skipping over those that are down or inaccessible. When all nodes are healthy, the top N nodes in a key's preference list  are  accessed.  When  there  are  node  failures  or  network partitions,  nodes  that  are  lower  ranked  in  the  preference  list  are accessed.", "text": "Read and write operations involve the first N healthy nodes in the preference list, skipping over those that are down or inaccessible. When all nodes are healthy, the top N nodes in a key's preference list  are  accessed.  When  there  are  node  failures  or  network partitions,  nodes  that  are  lower  ranked  in  the  preference  list  are accessed."}, {"self_ref": "#/texts/147", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 318.24, "t": 218.60400000000004, "r": 560.332, "b": 118.08899999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 652]}], "orig": "To  maintain  consistency  among  its  replicas,  Dynamo  uses  a consistency  protocol  similar  to  those  used  in  quorum  systems. This protocol has two key configurable values: R and W. R is the minimum number of nodes that must participate in a successful read  operation.  W  is  the  minimum  number  of  nodes  that  must participate in a successful write operation.  Setting R and W such that R + W > N yields a quorum-like system. In this model, the latency of a get (or put) operation is dictated by the slowest of the R  (or W)  replicas. For this  reason,  R  and  W  are  usually configured to be less than N, to provide better latency.", "text": "To  maintain  consistency  among  its  replicas,  Dynamo  uses  a consistency  protocol  similar  to  those  used  in  quorum  systems. This protocol has two key configurable values: R and W. R is the minimum number of nodes that must participate in a successful read  operation.  W  is  the  minimum  number  of  nodes  that  must participate in a successful write operation.  Setting R and W such that R + W > N yields a quorum-like system. In this model, the latency of a get (or put) operation is dictated by the slowest of the R  (or W)  replicas. For this  reason,  R  and  W  are  usually configured to be less than N, to provide better latency."}, {"self_ref": "#/texts/148", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 7, "bbox": {"l": 318.24, "t": 109.10500000000002, "r": 560.336, "b": 81.06999999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 204]}], "orig": "Upon receiving a put() request for a key, the coordinator generates the vector clock for the new version and writes the new version locally.  The  coordinator  then  sends  the  new  version  (along  with", "text": "Upon receiving a put() request for a key, the coordinator generates the vector clock for the new version and writes the new version locally.  The  coordinator  then  sends  the  new  version  (along  with"}, {"self_ref": "#/texts/149", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 7, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "201 1", "text": "201 1"}, {"self_ref": "#/texts/150", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 717.744, "r": 296.04, "b": 689.709, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 134]}], "orig": "the new vector clock) to the N highest-ranked reachable nodes. If at least W-1  nodes respond then the write is considered successful.", "text": "the new vector clock) to the N highest-ranked reachable nodes. If at least W-1  nodes respond then the write is considered successful."}, {"self_ref": "#/texts/151", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 680.724, "r": 296.103, "b": 600.909, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 534]}], "orig": "Similarly, for a get() request, the coordinator requests all existing versions of data for that key from the N highest-ranked reachable nodes  in  the  preference  list  for  that  key,  and  then  waits  for  R responses before returning the result to the client. If the coordinator  ends  up  gathering  multiple  versions  of  the  data,  it returns  all  the  versions  it  deems  to  be  causally  unrelated.  The divergent versions are then reconciled and the reconciled version superseding the current versions is written back.", "text": "Similarly, for a get() request, the coordinator requests all existing versions of data for that key from the N highest-ranked reachable nodes  in  the  preference  list  for  that  key,  and  then  waits  for  R responses before returning the result to the client. If the coordinator  ends  up  gathering  multiple  versions  of  the  data,  it returns  all  the  versions  it  deems  to  be  causally  unrelated.  The divergent versions are then reconciled and the reconciled version superseding the current versions is written back."}, {"self_ref": "#/texts/152", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 590.892, "r": 266.899, "b": 581.112, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 37]}], "orig": "4.6 Handling Failures: Hinted Handoff", "text": "4.6 Handling Failures: Hinted Handoff", "level": 1}, {"self_ref": "#/texts/153", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 578.124, "r": 296.106, "b": 498.309, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 527]}], "orig": "If  Dynamo  used  a  traditional  quorum  approach  it  would  be unavailable  during  server  failures  and  network  partitions,  and would have reduced durability even under the simplest of failure conditions.  To  remedy  this  it  does  not  enforce  strict  quorum membership and instead it uses a 'sloppy quorum'; all read and write operations are performed on the first N healthy nodes from the  preference  list,  which  may  not  always  be  the  first  N  nodes encountered while walking the consistent hashing ring.", "text": "If  Dynamo  used  a  traditional  quorum  approach  it  would  be unavailable  during  server  failures  and  network  partitions,  and would have reduced durability even under the simplest of failure conditions.  To  remedy  this  it  does  not  enforce  strict  quorum membership and instead it uses a 'sloppy quorum'; all read and write operations are performed on the first N healthy nodes from the  preference  list,  which  may  not  always  be  the  first  N  nodes encountered while walking the consistent hashing ring."}, {"self_ref": "#/texts/154", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 489.325, "r": 296.146, "b": 357.789, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 836]}], "orig": "Consider the example of Dynamo configuration given in Figure 2 with  N=3.  In  this  example,  if  node  A  is  temporarily  down  or unreachable  during  a  write  operation  then  a  replica  that  would normally have lived on A will now be sent to node D. This is done to maintain the desired availability and durability guarantees. The replica  sent  to  D  will  have  a  hint  in  its  metadata  that  suggests which node was the intended recipient of the replica (in this case A).    Nodes  that  receive  hinted  replicas  will  keep  them  in  a separate local database that is scanned periodically. Upon detecting  that  A  has  recovered,  D  will  attempt  to  deliver  the replica to A.  Once the transfer succeeds, D may delete the object from its local store without decreasing the total number of replicas in the system.", "text": "Consider the example of Dynamo configuration given in Figure 2 with  N=3.  In  this  example,  if  node  A  is  temporarily  down  or unreachable  during  a  write  operation  then  a  replica  that  would normally have lived on A will now be sent to node D. This is done to maintain the desired availability and durability guarantees. The replica  sent  to  D  will  have  a  hint  in  its  metadata  that  suggests which node was the intended recipient of the replica (in this case A).    Nodes  that  receive  hinted  replicas  will  keep  them  in  a separate local database that is scanned periodically. Upon detecting  that  A  has  recovered,  D  will  attempt  to  deliver  the replica to A.  Once the transfer succeeds, D may delete the object from its local store without decreasing the total number of replicas in the system."}, {"self_ref": "#/texts/155", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 348.745, "r": 296.116, "b": 248.289, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 638]}], "orig": "Using  hinted  handoff,  Dynamo  ensures  that  the  read  and  write operations  are  not  failed  due  to  temporary  node  or  network failures.  Applications  that  need  the  highest  level  of  availability can set W to 1, which ensures that a write is accepted as long as a single node in the system has durably written the key it to its local store.  Thus, the write request is only rejected if all nodes in the system  are  unavailable.  However,  in  practice,  most  Amazon services in production set a higher W to meet the desired level of durability. A more detailed discussion of configuring N, R and W follows in section 6.", "text": "Using  hinted  handoff,  Dynamo  ensures  that  the  read  and  write operations  are  not  failed  due  to  temporary  node  or  network failures.  Applications  that  need  the  highest  level  of  availability can set W to 1, which ensures that a write is accepted as long as a single node in the system has durably written the key it to its local store.  Thus, the write request is only rejected if all nodes in the system  are  unavailable.  However,  in  practice,  most  Amazon services in production set a higher W to meet the desired level of durability. A more detailed discussion of configuring N, R and W follows in section 6."}, {"self_ref": "#/texts/156", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 239.30500000000006, "r": 296.09, "b": 138.78999999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 669]}], "orig": "It is imperative that a highly available storage system be capable of  handling  the  failure  of  an  entire  data  center(s).  Data  center failures  happen  due  to  power  outages,  cooling  failures,  network failures,  and  natural  disasters.  Dynamo  is  configured  such  that each object is replicated across multiple data centers. In essence, the  preference  list  of  a  key  is  constructed  such  that  the  storage nodes are spread across multiple data centers. These datacenters are connected through high speed network links. This scheme of replicating across multiple datacenters allows us to handle entire data center failures without a data outage.", "text": "It is imperative that a highly available storage system be capable of  handling  the  failure  of  an  entire  data  center(s).  Data  center failures  happen  due  to  power  outages,  cooling  failures,  network failures,  and  natural  disasters.  Dynamo  is  configured  such  that each object is replicated across multiple data centers. In essence, the  preference  list  of  a  key  is  constructed  such  that  the  storage nodes are spread across multiple data centers. These datacenters are connected through high speed network links. This scheme of replicating across multiple datacenters allows us to handle entire data center failures without a data outage."}, {"self_ref": "#/texts/157", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 128.77199999999993, "r": 280.306, "b": 105.19200000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 56]}], "orig": "4.7 Handling permanent failures: Replica synchronization", "text": "4.7 Handling permanent failures: Replica synchronization", "level": 1}, {"self_ref": "#/texts/158", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 54.0, "t": 102.20399999999995, "r": 296.065, "b": 74.16899999999998, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 194]}, {"page_no": 8, "bbox": {"l": 318.24, "t": 717.744, "r": 560.317, "b": 689.708, "coord_origin": "BOTTOMLEFT"}, "charspan": [195, 383]}], "orig": "Hinted handoff works best if the system membership churn is low and node failures are transient. There are scenarios under which hinted replicas become unavailable before they can be returned to the  original  replica  node.  To  handle  this  and  other  threats  to durability, Dynamo implements an anti-entropy (replica synchronization) protocol to keep the replicas synchronized.", "text": "Hinted handoff works best if the system membership churn is low and node failures are transient. There are scenarios under which hinted replicas become unavailable before they can be returned to the  original  replica  node.  To  handle  this  and  other  threats  to durability, Dynamo implements an anti-entropy (replica synchronization) protocol to keep the replicas synchronized."}, {"self_ref": "#/texts/159", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 318.24, "t": 680.724, "r": 560.406, "b": 487.089, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1271]}], "orig": "To  detect  the  inconsistencies  between  replicas  faster  and  to minimize  the  amount  of  transferred  data,  Dynamo  uses  Merkle trees [13]. A Merkle tree is a hash tree where leaves are hashes of the values of individual keys. Parent nodes higher in the tree are hashes  of  their  respective  children.  The  principal  advantage  of Merkle  tree  is  that  each  branch  of  the  tree  can  be  checked independently without requiring nodes to download the entire tree or the entire data set. Moreover, Merkle trees help in reducing the amount  of  data  that  needs  to  be  transferred  while  checking  for inconsistencies among replicas. For instance, if the hash values of the root of two trees are equal, then the values of the leaf nodes in the tree are equal and the nodes require no synchronization. If not, it  implies  that  the  values  of  some  replicas  are  different.  In  such cases, the nodes may exchange the hash values of children and the process continues until it reaches the leaves of the trees, at which point the hosts can identify the keys that are 'out of sync'. Merkle trees minimize the amount of data that needs to be transferred for synchronization and reduce the number of disk reads performed during the anti-entropy process.", "text": "To  detect  the  inconsistencies  between  replicas  faster  and  to minimize  the  amount  of  transferred  data,  Dynamo  uses  Merkle trees [13]. A Merkle tree is a hash tree where leaves are hashes of the values of individual keys. Parent nodes higher in the tree are hashes  of  their  respective  children.  The  principal  advantage  of Merkle  tree  is  that  each  branch  of  the  tree  can  be  checked independently without requiring nodes to download the entire tree or the entire data set. Moreover, Merkle trees help in reducing the amount  of  data  that  needs  to  be  transferred  while  checking  for inconsistencies among replicas. For instance, if the hash values of the root of two trees are equal, then the values of the leaf nodes in the tree are equal and the nodes require no synchronization. If not, it  implies  that  the  values  of  some  replicas  are  different.  In  such cases, the nodes may exchange the hash values of children and the process continues until it reaches the leaves of the trees, at which point the hosts can identify the keys that are 'out of sync'. Merkle trees minimize the amount of data that needs to be transferred for synchronization and reduce the number of disk reads performed during the anti-entropy process."}, {"self_ref": "#/texts/160", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 318.24, "t": 478.044, "r": 560.362, "b": 346.509, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 858]}], "orig": "Dynamo uses Merkle trees for anti-entropy as follows: Each node maintains  a  separate  Merkle  tree  for  each  key  range  (the  set  of keys  covered  by  a  virtual  node)  it  hosts.  This  allows  nodes  to compare whether the keys within a key range are up-to-date. In this  scheme,  two  nodes  exchange  the  root  of  the  Merkle  tree corresponding  to  the  key  ranges  that  they  host  in  common. Subsequently, using the tree traversal scheme described above the nodes  determine  if  they  have  any  differences  and  perform  the appropriate  synchronization  action.  The  disadvantage  with  this scheme  is  that  many  key  ranges  change  when  a  node  joins  or leaves the system thereby requiring the tree(s) to be recalculated. This  issue  is  addressed,  however,  by  the  refined  partitioning scheme described in Section 6.2.", "text": "Dynamo uses Merkle trees for anti-entropy as follows: Each node maintains  a  separate  Merkle  tree  for  each  key  range  (the  set  of keys  covered  by  a  virtual  node)  it  hosts.  This  allows  nodes  to compare whether the keys within a key range are up-to-date. In this  scheme,  two  nodes  exchange  the  root  of  the  Merkle  tree corresponding  to  the  key  ranges  that  they  host  in  common. Subsequently, using the tree traversal scheme described above the nodes  determine  if  they  have  any  differences  and  perform  the appropriate  synchronization  action.  The  disadvantage  with  this scheme  is  that  many  key  ranges  change  when  a  node  joins  or leaves the system thereby requiring the tree(s) to be recalculated. This  issue  is  addressed,  however,  by  the  refined  partitioning scheme described in Section 6.2."}, {"self_ref": "#/texts/161", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 8, "bbox": {"l": 318.24, "t": 336.492, "r": 529.657, "b": 326.712, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 36]}], "orig": "4.8 Membership and Failure Detection", "text": "4.8 Membership and Failure Detection", "level": 1}, {"self_ref": "#/texts/162", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 8, "bbox": {"l": 318.24, "t": 320.943, "r": 435.523, "b": 311.994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 21]}], "orig": "4.8.1 Ring Membership", "text": "4.8.1 Ring Membership", "level": 1}, {"self_ref": "#/texts/163", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 318.24, "t": 309.084, "r": 560.338, "b": 115.44899999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1247]}], "orig": "In  Amazon's  environment  node  outages  (due  to  failures  and maintenance tasks) are often transient but may last for extended intervals.    A  node  outage  rarely  signifies  a  permanent  departure and  therefore  should  not  result  in  rebalancing  of  the  partition assignment  or  repair  of  the  unreachable  replicas.    Similarly, manual  error  could  result  in  the  unintentional  startup  of  new Dynamo nodes.   For these reasons, it was deemed appropriate to use an explicit mechanism to initiate the addition and removal of nodes  from  a  Dynamo  ring.  An  administrator  uses  a  command line tool or a browser to connect to a Dynamo node and issue a membership  change  to  join  a  node  to  a  ring  or  remove  a  node from  a  ring. The  node  that  serves  the  request  writes the membership change and its time of issue to persistent store. The membership  changes  form  a  history because nodes can be removed and added back multiple times. A gossip-based protocol propagates  membership  changes  and  maintains  an  eventually consistent view of membership. Each node contacts a peer chosen at  random  every  second  and  the  two  nodes  efficiently  reconcile their persisted membership change histories.", "text": "In  Amazon's  environment  node  outages  (due  to  failures  and maintenance tasks) are often transient but may last for extended intervals.    A  node  outage  rarely  signifies  a  permanent  departure and  therefore  should  not  result  in  rebalancing  of  the  partition assignment  or  repair  of  the  unreachable  replicas.    Similarly, manual  error  could  result  in  the  unintentional  startup  of  new Dynamo nodes.   For these reasons, it was deemed appropriate to use an explicit mechanism to initiate the addition and removal of nodes  from  a  Dynamo  ring.  An  administrator  uses  a  command line tool or a browser to connect to a Dynamo node and issue a membership  change  to  join  a  node  to  a  ring  or  remove  a  node from  a  ring. The  node  that  serves  the  request  writes the membership change and its time of issue to persistent store. The membership  changes  form  a  history because nodes can be removed and added back multiple times. A gossip-based protocol propagates  membership  changes  and  maintains  an  eventually consistent view of membership. Each node contacts a peer chosen at  random  every  second  and  the  two  nodes  efficiently  reconcile their persisted membership change histories."}, {"self_ref": "#/texts/164", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 8, "bbox": {"l": 318.24, "t": 106.46399999999994, "r": 560.296, "b": 78.42899999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 206]}], "orig": "When a node starts for the first time, it chooses its set of tokens (virtual  nodes  in  the  consistent  hash  space)  and  maps  nodes  to their respective token sets. The mapping is persisted on disk and", "text": "When a node starts for the first time, it chooses its set of tokens (virtual  nodes  in  the  consistent  hash  space)  and  maps  nodes  to their respective token sets. The mapping is persisted on disk and"}, {"self_ref": "#/texts/165", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 8, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "202 1", "text": "202 1"}, {"self_ref": "#/texts/166", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 717.744, "r": 296.059, "b": 637.988, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 475]}], "orig": "initially contains only the local node and token set.  The mappings stored at different Dynamo nodes are reconciled during the same communication exchange that reconciles the membership change histories.  Therefore,  partitioning  and  placement information also propagates via the gossip-based protocol and each storage node is aware of the token ranges handled by its peers. This allows each node to forward a key's read/write operations to the right set of nodes directly.", "text": "initially contains only the local node and token set.  The mappings stored at different Dynamo nodes are reconciled during the same communication exchange that reconciles the membership change histories.  Therefore,  partitioning  and  placement information also propagates via the gossip-based protocol and each storage node is aware of the token ranges handled by its peers. This allows each node to forward a key's read/write operations to the right set of nodes directly."}, {"self_ref": "#/texts/167", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 628.203, "r": 177.88, "b": 619.254, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 24]}], "orig": "4.8.2 External Discovery", "text": "4.8.2 External Discovery", "level": 1}, {"self_ref": "#/texts/168", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 616.284, "r": 298.248, "b": 484.749, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 829]}], "orig": "The  mechanism  described  above  could  temporarily  result  in  a logically partitioned Dynamo ring. For example, the administrator  could  contact  node  A  to  join  A  to  the  ring,  then contact node B to join B to the ring. In this scenario, nodes A and B  would  each  consider  itself  a  member  of  the  ring,  yet  neither would  be  immediately  aware  of  the  other.    To  prevent  logical partitions, some Dynamo nodes play the role of seeds.  Seeds are nodes  that  are  discovered  via  an  external  mechanism  and  are known to all nodes.  Because all nodes eventually reconcile their membership  with  a  seed,  logical  partitions  are  highly  unlikely. Seeds can be obtained either from static configuration or from a configuration service. Typically seeds are fully functional nodes in the Dynamo ring.", "text": "The  mechanism  described  above  could  temporarily  result  in  a logically partitioned Dynamo ring. For example, the administrator  could  contact  node  A  to  join  A  to  the  ring,  then contact node B to join B to the ring. In this scenario, nodes A and B  would  each  consider  itself  a  member  of  the  ring,  yet  neither would  be  immediately  aware  of  the  other.    To  prevent  logical partitions, some Dynamo nodes play the role of seeds.  Seeds are nodes  that  are  discovered  via  an  external  mechanism  and  are known to all nodes.  Because all nodes eventually reconcile their membership  with  a  seed,  logical  partitions  are  highly  unlikely. Seeds can be obtained either from static configuration or from a configuration service. Typically seeds are fully functional nodes in the Dynamo ring."}, {"self_ref": "#/texts/169", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 475.023, "r": 171.294, "b": 466.074, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 23]}], "orig": "4.8.3 Failure Detection", "text": "4.8.3 Failure Detection", "level": 1}, {"self_ref": "#/texts/170", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 463.104, "r": 298.337, "b": 310.869, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 971]}], "orig": "Failure detection in Dynamo  is  used  to avoid attempts to communicate  with  unreachable  peers  during  get()  and  put() operations  and  when  transferring  partitions  and  hinted  replicas. For the purpose of avoiding failed attempts at communication, a purely local notion of failure detection is entirely sufficient: node A may consider node B failed if node B does not respond to node A's messages (even if B is responsive to node C's messages).  In the  presence  of  a  steady  rate  of  client  requests  generating  internode  communication  in  the  Dynamo  ring,  a  node  A  quickly discovers that a node B is unresponsive when B fails to respond to a message; Node A then uses alternate nodes to service requests that map to B's partitions; A periodically retries B to check for the latter's recovery.  In the absence of client requests to drive traffic between two nodes, neither node really needs to know whether the other is reachable and responsive.", "text": "Failure detection in Dynamo  is  used  to avoid attempts to communicate  with  unreachable  peers  during  get()  and  put() operations  and  when  transferring  partitions  and  hinted  replicas. For the purpose of avoiding failed attempts at communication, a purely local notion of failure detection is entirely sufficient: node A may consider node B failed if node B does not respond to node A's messages (even if B is responsive to node C's messages).  In the  presence  of  a  steady  rate  of  client  requests  generating  internode  communication  in  the  Dynamo  ring,  a  node  A  quickly discovers that a node B is unresponsive when B fails to respond to a message; Node A then uses alternate nodes to service requests that map to B's partitions; A periodically retries B to check for the latter's recovery.  In the absence of client requests to drive traffic between two nodes, neither node really needs to know whether the other is reachable and responsive."}, {"self_ref": "#/texts/171", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 301.884, "r": 296.086, "b": 170.34899999999993, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 860]}], "orig": "Decentralized failure detection protocols use a simple gossip-style protocol  that  enable  each  node  in  the  system  to  learn  about  the arrival (or departure) of other nodes. For detailed information on decentralized failure  detectors  and  the  parameters  affecting  their accuracy, the interested reader is referred to [8]. Early designs of Dynamo  used  a  decentralized  failure  detector  to  maintain  a globally consistent view of failure state.  Later it was determined that the explicit node join and leave methods obviates the need for a global view of failure state. This is because nodes are notified of permanent node additions and removals by the explicit node join and  leave  methods  and  temporary  node  failures  are  detected  by the  individual  nodes  when  they  fail  to  communicate  with  others (while forwarding requests).", "text": "Decentralized failure detection protocols use a simple gossip-style protocol  that  enable  each  node  in  the  system  to  learn  about  the arrival (or departure) of other nodes. For detailed information on decentralized failure  detectors  and  the  parameters  affecting  their accuracy, the interested reader is referred to [8]. Early designs of Dynamo  used  a  decentralized  failure  detector  to  maintain  a globally consistent view of failure state.  Later it was determined that the explicit node join and leave methods obviates the need for a global view of failure state. This is because nodes are notified of permanent node additions and removals by the explicit node join and  leave  methods  and  temporary  node  failures  are  detected  by the  individual  nodes  when  they  fail  to  communicate  with  others (while forwarding requests)."}, {"self_ref": "#/texts/172", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 160.332, "r": 255.772, "b": 150.55200000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 33]}], "orig": "4.9 Adding/Removing Storage Nodes", "text": "4.9 Adding/Removing Storage Nodes", "level": 1}, {"self_ref": "#/texts/173", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 54.0, "t": 147.56399999999996, "r": 296.139, "b": 78.12900000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 478]}, {"page_no": 9, "bbox": {"l": 318.24, "t": 717.744, "r": 560.354, "b": 627.608, "coord_origin": "BOTTOMLEFT"}, "charspan": [479, 1018]}], "orig": "When  a  new  node  (say  X)  is  added  into  the  system,  it  gets assigned  a  number  of  tokens  that  are  randomly  scattered  on  the ring. For every key range that is assigned to node X, there may be a number of nodes (less than or equal to N) that are currently in charge of handling keys that fall within its token range. Due to the allocation of key ranges to X, some existing nodes no longer have to some of their keys and these nodes transfer those keys to X. Let us  consider  a  simple  bootstrapping  scenario  where  node  X  is added to the ring shown in Figure 2 between A and B. When X is added to the system, it is in charge of storing keys in the ranges (F, G], (G, A] and (A, X]. As a consequence, nodes B, C and D no longer have to store the keys in these respective ranges. Therefore, nodes B, C, and D will offer to and upon confirmation from  X  transfer  the  appropriate  set  of  keys.    When  a  node  is removed from the system, the reallocation of keys happens in a reverse process.", "text": "When  a  new  node  (say  X)  is  added  into  the  system,  it  gets assigned  a  number  of  tokens  that  are  randomly  scattered  on  the ring. For every key range that is assigned to node X, there may be a number of nodes (less than or equal to N) that are currently in charge of handling keys that fall within its token range. Due to the allocation of key ranges to X, some existing nodes no longer have to some of their keys and these nodes transfer those keys to X. Let us  consider  a  simple  bootstrapping  scenario  where  node  X  is added to the ring shown in Figure 2 between A and B. When X is added to the system, it is in charge of storing keys in the ranges (F, G], (G, A] and (A, X]. As a consequence, nodes B, C and D no longer have to store the keys in these respective ranges. Therefore, nodes B, C, and D will offer to and upon confirmation from  X  transfer  the  appropriate  set  of  keys.    When  a  node  is removed from the system, the reallocation of keys happens in a reverse process."}, {"self_ref": "#/texts/174", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 318.24, "t": 618.624, "r": 560.32, "b": 549.189, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 438]}], "orig": "Operational  experience  has  shown  that  this  approach  distributes the  load  of  key  distribution  uniformly  across  the  storage  nodes, which is important to meet the latency requirements and to ensure fast bootstrapping. Finally, by adding  a  confirmation  round between the  source  and  the  destination,  it  is  made  sure  that  the destination  node  does  not  receive  any  duplicate  transfers  for  a given key range.", "text": "Operational  experience  has  shown  that  this  approach  distributes the  load  of  key  distribution  uniformly  across  the  storage  nodes, which is important to meet the latency requirements and to ensure fast bootstrapping. Finally, by adding  a  confirmation  round between the  source  and  the  destination,  it  is  made  sure  that  the destination  node  does  not  receive  any  duplicate  transfers  for  a given key range."}, {"self_ref": "#/texts/175", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 9, "bbox": {"l": 318.24, "t": 539.112, "r": 453.895, "b": 529.332, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 17]}], "orig": "5. IMPLEMENTATION", "text": "5. IMPLEMENTATION", "level": 1}, {"self_ref": "#/texts/176", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 318.24, "t": 526.344, "r": 560.313, "b": 487.989, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 199]}], "orig": "In Dynamo, each storage node has three main software components: request coordination, membership and failure detection, and a local persistence engine.  All these components are implemented in Java.", "text": "In Dynamo, each storage node has three main software components: request coordination, membership and failure detection, and a local persistence engine.  All these components are implemented in Java."}, {"self_ref": "#/texts/177", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 318.24, "t": 478.944, "r": 560.327, "b": 357.788, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 736]}], "orig": "Dynamo's  local persistence component allows for different storage  engines  to  be  plugged  in.  Engines  that  are  in  use  are Berkeley  Database  (BDB)  Transactional  Data  Store 2 ,  BDB  Java Edition, MySQL,  and  an  in-memory  buffer with persistent backing store. The main reason for designing a pluggable persistence component is to choose the storage engine best suited for an application's access patterns. For instance, BDB can handle objects typically in the order of tens of kilobytes whereas MySQL can handle objects of larger sizes. Applications choose Dynamo's local  persistence  engine  based  on  their  object  size  distribution. The majority of Dynamo's production instances use  BDB Transactional Data Store.", "text": "Dynamo's  local persistence component allows for different storage  engines  to  be  plugged  in.  Engines  that  are  in  use  are Berkeley  Database  (BDB)  Transactional  Data  Store 2 ,  BDB  Java Edition, MySQL,  and  an  in-memory  buffer with persistent backing store. The main reason for designing a pluggable persistence component is to choose the storage engine best suited for an application's access patterns. For instance, BDB can handle objects typically in the order of tens of kilobytes whereas MySQL can handle objects of larger sizes. Applications choose Dynamo's local  persistence  engine  based  on  their  object  size  distribution. The majority of Dynamo's production instances use  BDB Transactional Data Store."}, {"self_ref": "#/texts/178", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 318.24, "t": 348.744, "r": 560.403, "b": 124.08799999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1498]}], "orig": "The request coordination component is built on top of an eventdriven messaging substrate where the message processing pipeline is split into multiple stages similar to the SEDA architecture [24]. All  communications  are  implemented  using  Java  NIO  channels. The coordinator executes the read and write requests on behalf of clients by collecting data from one or more nodes (in the case of reads)  or  storing  data  at  one  or  more  nodes  (for  writes).  Each client request results in the creation of a state machine on the node that received the client request. The state machine contains all the logic for identifying the nodes responsible for a key, sending the requests, waiting for responses, potentially doing retries, processing  the  replies  and  packaging  the  response  to  the  client. Each  state  machine  instance  handles  exactly  one  client  request. For  instance,  a  read  operation  implements  the  following  state machine:  (i) send  read  requests to the nodes,  (ii) wait for minimum  number  of  required  responses,  (iii)  if  too  few  replies were  received  within  a  given  time  bound,  fail  the  request,  (iv) otherwise gather all the data versions and determine the ones to be returned  and  (v)  if  versioning  is  enabled,  perform  syntactic reconciliation and generate an opaque write context that contains the vector clock that subsumes all the remaining versions. For the sake of brevity the failure handling and retry states are left out.", "text": "The request coordination component is built on top of an eventdriven messaging substrate where the message processing pipeline is split into multiple stages similar to the SEDA architecture [24]. All  communications  are  implemented  using  Java  NIO  channels. The coordinator executes the read and write requests on behalf of clients by collecting data from one or more nodes (in the case of reads)  or  storing  data  at  one  or  more  nodes  (for  writes).  Each client request results in the creation of a state machine on the node that received the client request. The state machine contains all the logic for identifying the nodes responsible for a key, sending the requests, waiting for responses, potentially doing retries, processing  the  replies  and  packaging  the  response  to  the  client. Each  state  machine  instance  handles  exactly  one  client  request. For  instance,  a  read  operation  implements  the  following  state machine:  (i) send  read  requests to the nodes,  (ii) wait for minimum  number  of  required  responses,  (iii)  if  too  few  replies were  received  within  a  given  time  bound,  fail  the  request,  (iv) otherwise gather all the data versions and determine the ones to be returned  and  (v)  if  versioning  is  enabled,  perform  syntactic reconciliation and generate an opaque write context that contains the vector clock that subsumes all the remaining versions. For the sake of brevity the failure handling and retry states are left out."}, {"self_ref": "#/texts/179", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 9, "bbox": {"l": 318.24, "t": 115.10400000000004, "r": 560.31, "b": 107.76800000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 76]}], "orig": "After  the  read  response  has  been  returned  to  the  caller  the  state", "text": "After  the  read  response  has  been  returned  to  the  caller  the  state"}, {"self_ref": "#/texts/180", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "footnote", "prov": [{"page_no": 9, "bbox": {"l": 320.94, "t": 98.01599999999996, "r": 508.286, "b": 88.62900000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 49]}], "orig": "2 http://www.oracle.com/database/berkeley-db.html", "text": "2 http://www.oracle.com/database/berkeley-db.html"}, {"self_ref": "#/texts/181", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 9, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "203 1", "text": "203 1"}, {"self_ref": "#/texts/182", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 13.021795696046306, "t": 754.4268788812628, "r": 19.915702016287856, "b": 647.8418975987108, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 32]}], "orig": "Client latencies in milliseconds", "text": "Client latencies in milliseconds"}, {"self_ref": "#/texts/183", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 22.213662781914167, "t": 775.8972332256484, "r": 37.53343046019546, "b": 769.7628458914605, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 4]}], "orig": "1000", "text": "1000"}, {"self_ref": "#/texts/184", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 22.213660787150072, "t": 719.1541509646704, "r": 29.873548120886174, "b": 682.3478256879778, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "(log scale)", "text": "(log scale)"}, {"self_ref": "#/texts/185", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 58.98110603563335, "t": 767.4624504073123, "r": 108.77034844852362, "b": 760.5612650537154, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "99.9 Writes", "text": "99.9 Writes"}, {"self_ref": "#/texts/186", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 69.7049415790158, "t": 760.6666667677969, "r": 108.77034814202352, "b": 754.3333334344636, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 12]}], "orig": "- 99.9 Reads", "text": "- 99.9 Reads"}, {"self_ref": "#/texts/187", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 56.683138810506954, "t": 752.9999999329759, "r": 107.23837113222567, "b": 746.6666665996427, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 18]}], "orig": "*..... Avg. writes", "text": "*..... Avg. writes"}, {"self_ref": "#/texts/188", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 71.98643584781993, "t": 746.0772422324142, "r": 107.25484372493541, "b": 739.0057617992679, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 9]}, {"page_no": 10, "bbox": {"l": 274.73852821940505, "t": 760.7798541393757, "r": 281.6983009925015, "b": 738.8931379325072, "coord_origin": "BOTTOMLEFT"}, "charspan": [10, 15]}], "orig": "Avg Reads times", "text": "Avg Reads times"}, {"self_ref": "#/texts/189", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 285.7136631644603, "t": 772.830039673346, "r": 298.73546492854217, "b": 765.9288536839662, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "200", "text": "200"}, {"self_ref": "#/texts/190", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 285.7136631644603, "t": 759.027668131844, "r": 298.73546492854217, "b": 752.1264821424642, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "180", "text": "180"}, {"self_ref": "#/texts/191", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 285.7136629784345, "t": 745.2252963084036, "r": 298.7354647425164, "b": 737.5573122996145, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "160", "text": "160"}, {"self_ref": "#/texts/192", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 41.720837474552496, "t": 728.7865166403575, "r": 252.52083340811416, "b": 702.6292137839122, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 11]}], "orig": "LAAMAAAMAAA", "text": "LAAMAAAMAAA"}, {"self_ref": "#/texts/193", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 285.7136631644603, "t": 730.6561266298678, "r": 298.73546492854217, "b": 723.7549406404879, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "140", "text": "140"}, {"self_ref": "#/texts/194", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 10, "bbox": {"l": 54.0, "t": 536.904, "r": 308.66, "b": 477.848, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 357]}], "orig": "Figure 4: Average and 99.9 percentiles of latencies for read and write requests during our peak request season of December 2006. The intervals between consecutive ticks in the x-axis correspond to  12  hours.  Latencies  follow  a  diurnal  pattern  similar  to  the request rate and  99.9 percentile latencies are an order of magnitude higher than averages", "text": "Figure 4: Average and 99.9 percentiles of latencies for read and write requests during our peak request season of December 2006. The intervals between consecutive ticks in the x-axis correspond to  12  hours.  Latencies  follow  a  diurnal  pattern  similar  to  the request rate and  99.9 percentile latencies are an order of magnitude higher than averages"}, {"self_ref": "#/texts/195", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 146.3037785277793, "t": 627.9051382155116, "r": 173.11337196477157, "b": 623.3043478738165, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 8]}], "orig": "Timeline", "text": "Timeline"}, {"self_ref": "#/texts/196", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 66.64098563886242, "t": 621.0039525220346, "r": 252.77616242167542, "b": 614.8695651878468, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 61]}], "orig": "(hourly plot of latencies during our peak seson in Dec. 2006)", "text": "(hourly plot of latencies during our peak seson in Dec. 2006)"}, {"self_ref": "#/texts/197", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 274.77355092122326, "t": 739.1408296333626, "r": 282.63279348889563, "b": 647.786184857972, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 26]}], "orig": "99.9th percentile response", "text": "99.9th percentile response"}, {"self_ref": "#/texts/198", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 282.6497080607624, "t": 717.6205537779814, "r": 289.54360580336055, "b": 691.549407130846, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "(msec)", "text": "(msec)"}, {"self_ref": "#/texts/199", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 289.54360464377464, "t": 717.6205534716744, "r": 298.7354651236672, "b": 709.1857708076932, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "20", "text": "20"}, {"self_ref": "#/texts/200", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 289.5436047094308, "t": 703.0513835468638, "r": 298.7354651893233, "b": 696.1501975574839, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "00", "text": "00"}, {"self_ref": "#/texts/201", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 289.5436047094308, "t": 689.2490118110575, "r": 298.7354651893233, "b": 681.5810278022684, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "80", "text": "80"}, {"self_ref": "#/texts/202", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 289.54360474421964, "t": 676.2134388471684, "r": 299.5014532267977, "b": 667.7786561831873, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "60", "text": "60"}, {"self_ref": "#/texts/203", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 289.543604890715, "t": 661.6442687225764, "r": 299.50145337329303, "b": 653.9762847137873, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "40", "text": "40"}, {"self_ref": "#/texts/204", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 289.5436047094308, "t": 647.0750989618837, "r": 298.7354651893233, "b": 640.1739129725038, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "20", "text": "20"}, {"self_ref": "#/texts/205", "parent": {"$ref": "#/pictures/3"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 294.1395348417275, "t": 633.2727273406117, "r": 298.7354650816738, "b": 627.1383400064238, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "0", "text": "0"}, {"self_ref": "#/texts/206", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 53.999, "t": 468.744, "r": 296.111, "b": 409.688, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 394]}], "orig": "machine waits for a small period of time to receive any outstanding  responses.  If  stale  versions  were  returned  in  any  of the responses, the coordinator updates those nodes with the latest version.  This  process  is  called read  repair because  it  repairs replicas that have missed a recent update at an opportunistic time and relieves the anti-entropy protocol from having to do it.", "text": "machine waits for a small period of time to receive any outstanding  responses.  If  stale  versions  were  returned  in  any  of the responses, the coordinator updates those nodes with the latest version.  This  process  is  called read  repair because  it  repairs replicas that have missed a recent update at an opportunistic time and relieves the anti-entropy protocol from having to do it."}, {"self_ref": "#/texts/207", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 53.999, "t": 400.644, "r": 296.132, "b": 238.08799999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1101]}], "orig": "As noted earlier, write requests are coordinated by one of the top N nodes in the preference list. Although it is desirable always to have  the  first  node  among  the  top  N  to  coordinate  the  writes thereby serializing all writes at a single location, this approach has led to uneven load distribution resulting in SLA violations. This is because  the  request  load  is  not  uniformly  distributed  across objects. To counter this, any of the top N nodes in the preference list  is  allowed  to  coordinate  the  writes.  In  particular,  since  each write usually follows a read operation, the coordinator for a write is  chosen to be the node that replied fastest to the previous read operation  which  is  stored  in  the  context  information  of  the request. This optimization enables us to pick the node that has the data  that  was  read  by  the  preceding  read  operation  thereby increasing the chances of getting 'read-your-writes' consistency. It  also  reduces  variability  in  the  performance  of  the  request handling which improves the performance at the 99.9 percentile.", "text": "As noted earlier, write requests are coordinated by one of the top N nodes in the preference list. Although it is desirable always to have  the  first  node  among  the  top  N  to  coordinate  the  writes thereby serializing all writes at a single location, this approach has led to uneven load distribution resulting in SLA violations. This is because  the  request  load  is  not  uniformly  distributed  across objects. To counter this, any of the top N nodes in the preference list  is  allowed  to  coordinate  the  writes.  In  particular,  since  each write usually follows a read operation, the coordinator for a write is  chosen to be the node that replied fastest to the previous read operation  which  is  stored  in  the  context  information  of  the request. This optimization enables us to pick the node that has the data  that  was  read  by  the  preceding  read  operation  thereby increasing the chances of getting 'read-your-writes' consistency. It  also  reduces  variability  in  the  performance  of  the  request handling which improves the performance at the 99.9 percentile."}, {"self_ref": "#/texts/208", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 10, "bbox": {"l": 54.0, "t": 228.01199999999994, "r": 291.912, "b": 218.23199999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 32]}], "orig": "6. EXPERIENCES & LESSONS LEARNED", "text": "6. EXPERIENCES & LESSONS LEARNED", "level": 1}, {"self_ref": "#/texts/209", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 54.0, "t": 215.24400000000003, "r": 296.028, "b": 176.889, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 242]}], "orig": "Dynamo is used by several services with different configurations. These  instances  differ  by  their  version  reconciliation  logic,  and read/write  quorum  characteristics.  The  following  are  the  main patterns in which Dynamo is used:", "text": "Dynamo is used by several services with different configurations. These  instances  differ  by  their  version  reconciliation  logic,  and read/write  quorum  characteristics.  The  following  are  the  main patterns in which Dynamo is used:"}, {"self_ref": "#/texts/210", "parent": {"$ref": "#/groups/1"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 10, "bbox": {"l": 54.0, "t": 170.32500000000005, "r": 296.091, "b": 97.80799999999999, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 423]}], "orig": "\u00b7 Business logic specific reconciliation: This is a popular use case  for  Dynamo.  Each  data  object  is  replicated  across multiple  nodes.  In  case  of  divergent  versions,  the  client application performs its own reconciliation logic. The shopping cart service discussed earlier is a prime example of this category. Its business logic reconciles objects by merging different versions of a customer's shopping cart.", "text": "Business logic specific reconciliation: This is a popular use case  for  Dynamo.  Each  data  object  is  replicated  across multiple  nodes.  In  case  of  divergent  versions,  the  client application performs its own reconciliation logic. The shopping cart service discussed earlier is a prime example of this category. Its business logic reconciles objects by merging different versions of a customer's shopping cart.", "enumerated": false, "marker": "\u00b7"}, {"self_ref": "#/texts/211", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 314.8212208318952, "t": 764.3952566796642, "r": 369.9723824404887, "b": 757.3333333347341, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 19]}, {"page_no": 10, "bbox": {"l": 310.2252905237737, "t": 753.6666667866442, "r": 363.07848748892985, "b": 746.6666667866442, "coord_origin": "BOTTOMLEFT"}, "charspan": [20, 35]}], "orig": "\u2014 direct BDB writes buffered writes", "text": "\u2014 direct BDB writes buffered writes"}, {"self_ref": "#/texts/212", "parent": {"$ref": "#/pictures/4"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 10, "bbox": {"l": 317.16, "t": 536.904, "r": 564.312, "b": 498.548, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 215]}], "orig": "Figure  5:  Comparison  of  performance  of  99.9th  percentile latencies for buffered vs. non-buffered writes over a period of 24 hours. The intervals between consecutive ticks in the x-axis correspond to one hour.", "text": "Figure  5:  Comparison  of  performance  of  99.9th  percentile latencies for buffered vs. non-buffered writes over a period of 24 hours. The intervals between consecutive ticks in the x-axis correspond to one hour."}, {"self_ref": "#/texts/213", "parent": {"$ref": "#/pictures/4"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 395.201769224534, "t": 626.5438050977598, "r": 422.107823662222, "b": 618.5312935265115, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 8]}], "orig": "Timeline", "text": "Timeline"}, {"self_ref": "#/texts/214", "parent": {"$ref": "#/groups/2"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 10, "bbox": {"l": 318.24, "t": 471.225, "r": 560.327, "b": 388.328, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 445]}], "orig": "\u00b7 Timestamp based reconciliation: This  case  differs  from  the previous one only in the reconciliation mechanism. In case of divergent  versions,  Dynamo  performs  simple  timestamp based reconciliation logic of 'last write wins'; i.e., the object with  the  largest  physical  timestamp  value  is  chosen  as  the correct version. The service that maintains customer's session information is a good example of a service that uses this mode.", "text": "Timestamp based reconciliation: This  case  differs  from  the previous one only in the reconciliation mechanism. In case of divergent  versions,  Dynamo  performs  simple  timestamp based reconciliation logic of 'last write wins'; i.e., the object with  the  largest  physical  timestamp  value  is  chosen  as  the correct version. The service that maintains customer's session information is a good example of a service that uses this mode.", "enumerated": false, "marker": "\u00b7"}, {"self_ref": "#/texts/215", "parent": {"$ref": "#/groups/2"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 10, "bbox": {"l": 318.24, "t": 381.765, "r": 560.417, "b": 257.4680000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 750]}], "orig": "\u00b7 High performance read engine: While Dynamo is built to be an 'always writeable' data store, a few services are tuning its quorum  characteristics  and  using  it  as  a  high  performance read  engine.  Typically,  these  services  have  a  high  read request  rate  and  only  a  small  number  of  updates.  In  this configuration, typically R is set to be 1 and W to be N. For these services, Dynamo provides the ability to partition and replicate  their  data  across  multiple  nodes  thereby  offering incremental scalability. Some of these instances function as the  authoritative  persistence  cache  for  data  stored  in  more heavy weight backing stores. Services that maintain product catalog and promotional items fit in this category.", "text": "High performance read engine: While Dynamo is built to be an 'always writeable' data store, a few services are tuning its quorum  characteristics  and  using  it  as  a  high  performance read  engine.  Typically,  these  services  have  a  high  read request  rate  and  only  a  small  number  of  updates.  In  this configuration, typically R is set to be 1 and W to be N. For these services, Dynamo provides the ability to partition and replicate  their  data  across  multiple  nodes  thereby  offering incremental scalability. Some of these instances function as the  authoritative  persistence  cache  for  data  stored  in  more heavy weight backing stores. Services that maintain product catalog and promotional items fit in this category.", "enumerated": false, "marker": "\u00b7"}, {"self_ref": "#/texts/216", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 318.24, "t": 248.48400000000004, "r": 560.329, "b": 199.74900000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 292]}], "orig": "The main advantage of Dynamo is that its client applications can tune the values of N, R and W to achieve their desired levels of performance, availability and durability. For instance, the value of N determines the durability of each object. A typical value of N used by Dynamo's users is 3.", "text": "The main advantage of Dynamo is that its client applications can tune the values of N, R and W to achieve their desired levels of performance, availability and durability. For instance, the value of N determines the durability of each object. A typical value of N used by Dynamo's users is 3."}, {"self_ref": "#/texts/217", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 10, "bbox": {"l": 318.24, "t": 190.70399999999995, "r": 560.369, "b": 90.24800000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 664]}], "orig": "The values of W and R impact object availability, durability and consistency.  For instance, if W is set to 1, then the system will never reject a write request as long as there is at least one node in the system that can successfully process a write request. However, low values of W and R can increase the risk of inconsistency as write  requests  are  deemed  successful  and  returned  to  the  clients even if they are not processed by a majority of the replicas. This also introduces a vulnerability window for durability when a write request  is  successfully  returned  to  the  client  even  though  it  has been persisted at only a small number of nodes.", "text": "The values of W and R impact object availability, durability and consistency.  For instance, if W is set to 1, then the system will never reject a write request as long as there is at least one node in the system that can successfully process a write request. However, low values of W and R can increase the risk of inconsistency as write  requests  are  deemed  successful  and  returned  to  the  clients even if they are not processed by a majority of the replicas. This also introduces a vulnerability window for durability when a write request  is  successfully  returned  to  the  client  even  though  it  has been persisted at only a small number of nodes."}, {"self_ref": "#/texts/218", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 10, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "204 1", "text": "204 1"}, {"self_ref": "#/texts/219", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 14.64781483512211, "t": 766.0557939581545, "r": 20.6401027528599, "b": 762.0643776491416, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.9", "text": "0.9"}, {"self_ref": "#/texts/220", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 9.296271665732853, "t": 758.1095479974609, "r": 15.332349991313363, "b": 672.8799448071821, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 32]}], "orig": "Fraction of nodes out-of-balance", "text": "Fraction of nodes out-of-balance"}, {"self_ref": "#/texts/221", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 14.64781487673522, "t": 669.5965665420839, "r": 20.64010279447301, "b": 666.2703862845732, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.1", "text": "0.1"}, {"self_ref": "#/texts/222", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 146.47814837452447, "t": 773.3733905246781, "r": 219.05141315601546, "b": 767.3862660611588, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 30]}], "orig": "-- - fraction of nodes out-of-", "text": "-- - fraction of nodes out-of-"}, {"self_ref": "#/texts/223", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 150.47300802501286, "t": 759.4034335429185, "r": 189.75578437462727, "b": 752.751073027897, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 14]}, {"page_no": 11, "bbox": {"l": 147.79080247918424, "t": 769.4326909569138, "r": 179.78760422326775, "b": 757.357008656559, "coord_origin": "BOTTOMLEFT"}, "charspan": [15, 22]}], "orig": "\u2014 request load balance", "text": "\u2014 request load balance"}, {"self_ref": "#/texts/224", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 225.70951162804624, "t": 778.0300429517167, "r": 231.70179954578404, "b": 773.3733905912018, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "50", "text": "50"}, {"self_ref": "#/texts/225", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.37532158427692, "t": 766.0557940135908, "r": 231.70179941589436, "b": 762.064377704578, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "45|", "text": "45|"}, {"self_ref": "#/texts/226", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.37532132566412, "t": 754.0815451309013, "r": 231.03598970612686, "b": 750.0901288218884, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "40", "text": "40"}, {"self_ref": "#/texts/227", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.37532130347043, "t": 741.4420601095953, "r": 231.03598968393317, "b": 738.1158798520846, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "35", "text": "35"}, {"self_ref": "#/texts/228", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.37532132566412, "t": 730.1330472768241, "r": 231.03598970612686, "b": 726.1416309678111, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "30", "text": "30"}, {"self_ref": "#/texts/229", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.37532132566412, "t": 718.1587983497855, "r": 231.03598970612686, "b": 714.1673820407725, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "25", "text": "25"}, {"self_ref": "#/texts/230", "parent": {"$ref": "#/pictures/5"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.3753214033419, "t": 705.5193133296674, "r": 231.70179955244214, "b": 702.1931330721567, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "201", "text": "201"}, {"self_ref": "#/texts/231", "parent": {"$ref": "#/pictures/5"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.37532130347043, "t": 693.5450644014409, "r": 231.03598968393317, "b": 690.2188841439302, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 2]}], "orig": "15", "text": "15"}, {"self_ref": "#/texts/232", "parent": {"$ref": "#/pictures/5"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 226.37532130347043, "t": 681.5708154714324, "r": 231.70179945257067, "b": 678.2446352139217, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "10|", "text": "10|"}, {"self_ref": "#/texts/233", "parent": {"$ref": "#/pictures/5"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 113.18766086812339, "t": 652.9656651761802, "r": 134.49357346452442, "b": 646.9785407126609, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 8]}], "orig": "Timeline", "text": "Timeline"}, {"self_ref": "#/texts/234", "parent": {"$ref": "#/pictures/5"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 235.58279045340817, "t": 699.6459574902685, "r": 241.5719149721483, "b": 676.0972346705596, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 7]}], "orig": "Request", "text": "Request"}, {"self_ref": "#/texts/235", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 235.28582753944042, "t": 764.1131064295342, "r": 241.12165832720697, "b": 701.4787305257947, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 22]}, {"page_no": 11, "bbox": {"l": 241.68894487926636, "t": 734.1244638685622, "r": 246.34961548210893, "b": 706.1845497054721, "coord_origin": "BOTTOMLEFT"}, "charspan": [23, 33]}], "orig": "Load (scaled down by a constant).", "text": "Load (scaled down by a constant)."}, {"self_ref": "#/texts/236", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 11, "bbox": {"l": 55.02, "t": 575.784, "r": 297.034, "b": 526.928, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 286]}], "orig": "Figure 6: Fraction of nodes that are out-of-balance (i.e., nodes whose  request  load  is  above  a  certain  threshold  from  the average  system  load)  and  their  corresponding  request  load. The  interval  between  ticks  in  x-axis  corresponds  to  a  time period of 30 minutes.", "text": "Figure 6: Fraction of nodes that are out-of-balance (i.e., nodes whose  request  load  is  above  a  certain  threshold  from  the average  system  load)  and  their  corresponding  request  load. The  interval  between  ticks  in  x-axis  corresponds  to  a  time period of 30 minutes."}, {"self_ref": "#/texts/237", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 54.0, "t": 514.464, "r": 296.116, "b": 455.408, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 389]}], "orig": "Traditional wisdom holds that durability and availability go handin-hand. However, this is not necessarily true here. For instance, the  vulnerability  window  for  durability  can  be  decreased  by increasing  W.  This  may  increase  the  probability  of  rejecting requests  (thereby  decreasing  availability)  because  more  storage hosts need to be alive to process a write request.", "text": "Traditional wisdom holds that durability and availability go handin-hand. However, this is not necessarily true here. For instance, the  vulnerability  window  for  durability  can  be  decreased  by increasing  W.  This  may  increase  the  probability  of  rejecting requests  (thereby  decreasing  availability)  because  more  storage hosts need to be alive to process a write request."}, {"self_ref": "#/texts/238", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 54.0, "t": 446.424, "r": 296.047, "b": 408.009, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 203]}], "orig": "The common (N,R,W) configuration used by several instances of Dynamo is (3,2,2). These values are chosen to meet the necessary levels  of  performance,  durability,  consistency,  and  availability SLAs.", "text": "The common (N,R,W) configuration used by several instances of Dynamo is (3,2,2). These values are chosen to meet the necessary levels  of  performance,  durability,  consistency,  and  availability SLAs."}, {"self_ref": "#/texts/239", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 54.0, "t": 399.025, "r": 296.093, "b": 288.189, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 712]}], "orig": "All  the  measurements  presented  in  this  section  were  taken  on  a live system operating with a configuration of (3,2,2) and running a couple hundred nodes with homogenous hardware configurations.  As  mentioned  earlier,  each  instance  of  Dynamo contains  nodes  that  are  located  in  multiple  datacenters.  These datacenters  are  typically  connected  through  high  speed  network links. Recall that to generate a successful get (or put) response R (or  W)  nodes  need  to  respond  to  the  coordinator.  Clearly,  the network  latencies  between  datacenters  affect  the  response  time and the nodes (and their datacenter locations) are chosen such that the applications target SLAs are met.", "text": "All  the  measurements  presented  in  this  section  were  taken  on  a live system operating with a configuration of (3,2,2) and running a couple hundred nodes with homogenous hardware configurations.  As  mentioned  earlier,  each  instance  of  Dynamo contains  nodes  that  are  located  in  multiple  datacenters.  These datacenters  are  typically  connected  through  high  speed  network links. Recall that to generate a successful get (or put) response R (or  W)  nodes  need  to  respond  to  the  coordinator.  Clearly,  the network  latencies  between  datacenters  affect  the  response  time and the nodes (and their datacenter locations) are chosen such that the applications target SLAs are met."}, {"self_ref": "#/texts/240", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 11, "bbox": {"l": 54.0, "t": 278.1120000000001, "r": 288.576, "b": 268.332, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 40]}], "orig": "6.1 Balancing Performance and Durability", "text": "6.1 Balancing Performance and Durability", "level": 1}, {"self_ref": "#/texts/241", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 54.0, "t": 265.34400000000005, "r": 297.785, "b": 195.909, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 470]}], "orig": "While  Dynamo's  principle  design  goal  is  to  build  a  highly available data store, performance is an equally important criterion in  Amazon's  platform.  As  noted  earlier,  to  provide  a  consistent customer  experience,  Amazon's  services  set  their  performance targets at higher percentiles (such as the 99.9 th or  99.99 th percentiles). A typical SLA required of services that use Dynamo is that 99.9% of the read and write requests execute within 300ms.", "text": "While  Dynamo's  principle  design  goal  is  to  build  a  highly available data store, performance is an equally important criterion in  Amazon's  platform.  As  noted  earlier,  to  provide  a  consistent customer  experience,  Amazon's  services  set  their  performance targets at higher percentiles (such as the 99.9 th or  99.99 th percentiles). A typical SLA required of services that use Dynamo is that 99.9% of the read and write requests execute within 300ms."}, {"self_ref": "#/texts/242", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 54.0, "t": 186.92399999999998, "r": 296.105, "b": 76.08899999999994, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 739]}, {"page_no": 11, "bbox": {"l": 318.24, "t": 717.744, "r": 560.337, "b": 648.309, "coord_origin": "BOTTOMLEFT"}, "charspan": [740, 1214]}], "orig": "Since Dynamo is run on standard commodity hardware components  that  have  far  less  I/O  throughput  than  high-end enterprise  servers,  providing  consistently  high  performance  for read and write operations is a non-trivial task. The involvement of multiple storage nodes in read and write operations makes it even more  challenging,  since  the  performance  of  these  operations  is limited by the slowest of the R or W replicas. Figure 4 shows the average  and  99.9 th percentile  latencies  of  Dynamo's  read  and write operations during a period of 30 days. As seen in the figure, the latencies exhibit a clear diurnal pattern which is a result of the diurnal  pattern  in  the  incoming  request  rate  (i.e.,  there  is  a significant  difference  in  request  rate  between  the  daytime  and night). Moreover, the write latencies are higher than read latencies obviously because write operations always results in disk access. Also, the 99.9 th percentile latencies are around 200 ms and are an order of magnitude higher than the averages. This is because the 99.9 th percentile  latencies are affected by several factors such as variability in request load, object sizes, and locality patterns.", "text": "Since Dynamo is run on standard commodity hardware components  that  have  far  less  I/O  throughput  than  high-end enterprise  servers,  providing  consistently  high  performance  for read and write operations is a non-trivial task. The involvement of multiple storage nodes in read and write operations makes it even more  challenging,  since  the  performance  of  these  operations  is limited by the slowest of the R or W replicas. Figure 4 shows the average  and  99.9 th percentile  latencies  of  Dynamo's  read  and write operations during a period of 30 days. As seen in the figure, the latencies exhibit a clear diurnal pattern which is a result of the diurnal  pattern  in  the  incoming  request  rate  (i.e.,  there  is  a significant  difference  in  request  rate  between  the  daytime  and night). Moreover, the write latencies are higher than read latencies obviously because write operations always results in disk access. Also, the 99.9 th percentile latencies are around 200 ms and are an order of magnitude higher than the averages. This is because the 99.9 th percentile  latencies are affected by several factors such as variability in request load, object sizes, and locality patterns."}, {"self_ref": "#/texts/243", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 318.24, "t": 639.325, "r": 560.335, "b": 538.81, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 635]}], "orig": "While  this  level  of  performance  is  acceptable  for  a  number  of services, a few customer-facing services required higher levels of performance. For these services, Dynamo provides the ability to trade-off durability guarantees for performance. In the optimization  each  storage  node  maintains  an  object  buffer  in  its main  memory.  Each  write  operation  is  stored  in  the  buffer  and gets  periodically  written  to  storage  by  a writer  thread .  In  this scheme, read operations first check if the requested key is present in the buffer. If so, the object is read from the buffer instead of the storage engine.", "text": "While  this  level  of  performance  is  acceptable  for  a  number  of services, a few customer-facing services required higher levels of performance. For these services, Dynamo provides the ability to trade-off durability guarantees for performance. In the optimization  each  storage  node  maintains  an  object  buffer  in  its main  memory.  Each  write  operation  is  stored  in  the  buffer  and gets  periodically  written  to  storage  by  a writer  thread .  In  this scheme, read operations first check if the requested key is present in the buffer. If so, the object is read from the buffer instead of the storage engine."}, {"self_ref": "#/texts/244", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 318.24, "t": 531.876, "r": 560.348, "b": 408.609, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 787]}], "orig": "This  optimization  has  resulted  in  lowering  the  99.9 th percentile latency by a factor of 5 during peak traffic even for a very small buffer of a thousand objects (see Figure 5). Also, as seen in the figure,  write  buffering  smoothes  out  higher  percentile  latencies. Obviously, this scheme trades durability for performance. In this scheme,  a  server  crash  can  result  in  missing  writes  that  were queued up in the buffer. To reduce the durability risk, the write operation is refined to have the coordinator choose one out of the N  replicas  to  perform  a  'durable  write'.  Since  the  coordinator waits only for W  responses, the performance  of the write operation is not affected by the performance of the durable write operation performed by a single replica.", "text": "This  optimization  has  resulted  in  lowering  the  99.9 th percentile latency by a factor of 5 during peak traffic even for a very small buffer of a thousand objects (see Figure 5). Also, as seen in the figure,  write  buffering  smoothes  out  higher  percentile  latencies. Obviously, this scheme trades durability for performance. In this scheme,  a  server  crash  can  result  in  missing  writes  that  were queued up in the buffer. To reduce the durability risk, the write operation is refined to have the coordinator choose one out of the N  replicas  to  perform  a  'durable  write'.  Since  the  coordinator waits only for W  responses, the performance  of the write operation is not affected by the performance of the durable write operation performed by a single replica."}, {"self_ref": "#/texts/245", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 11, "bbox": {"l": 318.24, "t": 398.592, "r": 537.707, "b": 388.812, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 38]}], "orig": "6.2 Ensuring Uniform Load distribution", "text": "6.2 Ensuring Uniform Load distribution", "level": 1}, {"self_ref": "#/texts/246", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 318.24, "t": 385.824, "r": 560.337, "b": 274.98900000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 687]}], "orig": "Dynamo uses consistent hashing to partition its key space across its replicas and to ensure uniform load distribution. A uniform key distribution can help us achieve uniform load distribution assuming the access distribution of keys is not highly skewed. In particular,  Dynamo's  design  assumes  that  even  where  there  is  a significant skew in the access distribution there are enough keys in the popular end of the distribution so that the load of handling popular  keys  can  be  spread  across  the  nodes  uniformly  through partitioning.  This  section  discusses  the  load  imbalance  seen  in Dynamo and the impact of different partitioning strategies on load distribution.", "text": "Dynamo uses consistent hashing to partition its key space across its replicas and to ensure uniform load distribution. A uniform key distribution can help us achieve uniform load distribution assuming the access distribution of keys is not highly skewed. In particular,  Dynamo's  design  assumes  that  even  where  there  is  a significant skew in the access distribution there are enough keys in the popular end of the distribution so that the load of handling popular  keys  can  be  spread  across  the  nodes  uniformly  through partitioning.  This  section  discusses  the  load  imbalance  seen  in Dynamo and the impact of different partitioning strategies on load distribution."}, {"self_ref": "#/texts/247", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 11, "bbox": {"l": 318.24, "t": 266.004, "r": 560.354, "b": 93.06899999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1140]}], "orig": "To study the load imbalance and its correlation with request load, the total number of requests received by each node was measured for  a  period  of  24  hours  -  broken  down  into  intervals  of  30 minutes. In a given time window, a node is considered to be 'inbalance', if the node's request load deviates from the average load by a value a less than a certain threshold (here 15%). Otherwise the  node  was  deemed  'out-of-balance'.  Figure  6  presents  the fraction of nodes that are 'out-of-balance' (henceforth, 'imbalance  ratio')  during  this  time  period.  For  reference,  the corresponding request load received by the entire system during this  time  period  is  also  plotted.  As  seen  in  the  figure,  the imbalance  ratio  decreases  with  increasing  load.  For  instance, during low loads the imbalance ratio is as high as 20% and during high loads it is close to 10%. Intuitively, this can be explained by the fact that under high loads, a large number of popular keys are accessed  and  due  to  uniform  distribution  of  keys  the  load  is evenly distributed. However, during low loads (where load is 1/8 th", "text": "To study the load imbalance and its correlation with request load, the total number of requests received by each node was measured for  a  period  of  24  hours  -  broken  down  into  intervals  of  30 minutes. In a given time window, a node is considered to be 'inbalance', if the node's request load deviates from the average load by a value a less than a certain threshold (here 15%). Otherwise the  node  was  deemed  'out-of-balance'.  Figure  6  presents  the fraction of nodes that are 'out-of-balance' (henceforth, 'imbalance  ratio')  during  this  time  period.  For  reference,  the corresponding request load received by the entire system during this  time  period  is  also  plotted.  As  seen  in  the  figure,  the imbalance  ratio  decreases  with  increasing  load.  For  instance, during low loads the imbalance ratio is as high as 20% and during high loads it is close to 10%. Intuitively, this can be explained by the fact that under high loads, a large number of popular keys are accessed  and  due  to  uniform  distribution  of  keys  the  load  is evenly distributed. However, during low loads (where load is 1/8 th"}, {"self_ref": "#/texts/248", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 11, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "205 1", "text": "205 1"}, {"self_ref": "#/texts/249", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 62.000000150000005, "t": 780.0000000916667, "r": 80.66666681666668, "b": 772.6666667583333, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "key k1", "text": "key k1"}, {"self_ref": "#/texts/250", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 42.64974500993534, "t": 640.0738494589882, "r": 86.0169223374093, "b": 629.9261501029091, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Strategy 1", "text": "Strategy 1"}, {"self_ref": "#/texts/251", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 218.66666686, "t": 781.3333333866667, "r": 238.00000019333334, "b": 774.66666672, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 6]}], "orig": "key k1", "text": "key k1"}, {"self_ref": "#/texts/252", "parent": {"$ref": "#/pictures/6"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 12, "bbox": {"l": 61.2, "t": 550.644, "r": 550.759, "b": 522.429, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 348]}], "orig": "Figure 7: Partitioning and placement of keys in the three strategies. A, B, and C depict the three unique nodes that form the preference list for the key k1 on the consistent hashing ring (N=3). The shaded area indicates the key range for which nodes A, B, and C form the preference list. Dark arrows indicate the token locations for various nodes.", "text": "Figure 7: Partitioning and placement of keys in the three strategies. A, B, and C depict the three unique nodes that form the preference list for the key k1 on the consistent hashing ring (N=3). The shaded area indicates the key range for which nodes A, B, and C form the preference list. Dark arrows indicate the token locations for various nodes."}, {"self_ref": "#/texts/253", "parent": {"$ref": "#/pictures/6"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 207.99999962, "t": 638.0000001066667, "r": 251.33333295333333, "b": 627.33333344, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Strategy 2", "text": "Strategy 2"}, {"self_ref": "#/texts/254", "parent": {"$ref": "#/pictures/6"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 359.9810457999926, "t": 638.7554785124141, "r": 403.3522870516202, "b": 629.2445214303016, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Strategy 3", "text": "Strategy 3"}, {"self_ref": "#/texts/255", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 54.0, "t": 496.884, "r": 296.038, "b": 479.228, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 107]}], "orig": "of  the  measured  peak  load),  fewer  popular  keys  are  accessed, resulting in a higher load imbalance.", "text": "of  the  measured  peak  load),  fewer  popular  keys  are  accessed, resulting in a higher load imbalance."}, {"self_ref": "#/texts/256", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 54.0, "t": 470.184, "r": 296.049, "b": 452.528, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 127]}], "orig": "This  section  discusses  how  Dynamo's  partitioning  scheme  has evolved over time and its implications on load distribution.", "text": "This  section  discusses  how  Dynamo's  partitioning  scheme  has evolved over time and its implications on load distribution."}, {"self_ref": "#/texts/257", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 54.0, "t": 443.483, "r": 296.146, "b": 311.949, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 845]}], "orig": "Strategy  1:  T  random  tokens  per  node  and  partition  by  token value :    This  was  the  initial  strategy  deployed in production (and described in Section 4.2). In this scheme, each node is assigned T tokens  (chosen  uniformly  at  random  from  the  hash  space).  The tokens  of  all  nodes  are  ordered  according  to  their  values  in  the hash space. Every two consecutive tokens define a range. The last token and the first token form a range that \"wraps\" around from the highest value to the lowest value in the hash space. Because the tokens are chosen randomly, the ranges vary in size. As nodes join and leave the system, the token set changes and consequently the  ranges  change.  Note  that  the  space  needed  to  maintain  the membership at each  node  increases  linearly  with  the  number  of nodes in the system.", "text": "Strategy  1:  T  random  tokens  per  node  and  partition  by  token value :    This  was  the  initial  strategy  deployed in production (and described in Section 4.2). In this scheme, each node is assigned T tokens  (chosen  uniformly  at  random  from  the  hash  space).  The tokens  of  all  nodes  are  ordered  according  to  their  values  in  the hash space. Every two consecutive tokens define a range. The last token and the first token form a range that \"wraps\" around from the highest value to the lowest value in the hash space. Because the tokens are chosen randomly, the ranges vary in size. As nodes join and leave the system, the token set changes and consequently the  ranges  change.  Note  that  the  space  needed  to  maintain  the membership at each  node  increases  linearly  with  the  number  of nodes in the system."}, {"self_ref": "#/texts/258", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 54.0, "t": 302.965, "r": 296.125, "b": 88.62900000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1420]}], "orig": "While using this strategy, the following problems were encountered. First, when a new node joins the system, it needs to 'steal'  its  key  ranges  from  other  nodes.  However,  the  nodes handing  the  key  ranges  off  to  the  new  node  have  to  scan  their local persistence store to retrieve the appropriate set of data items. Note that performing such a scan operation on a production node is tricky as scans are highly resource intensive operations and they need  to  be  executed  in  the  background  without  affecting  the customer performance. This requires us to run the bootstrapping task  at  the  lowest  priority.  However, this  significantly  slows  the bootstrapping process and during busy shopping season, when the nodes are handling millions of requests a day, the bootstrapping has  taken  almost  a  day  to  complete.  Second,  when  a  node joins/leaves  the  system,  the  key  ranges  handled  by  many  nodes change  and  the  Merkle  trees  for  the  new  ranges  need  to  be recalculated,  which  is  a  non-trivial  operation  to  perform  on  a production  system.  Finally,  there  was  no  easy  way  to  take  a snapshot  of  the  entire  key  space  due  to  the  randomness  in  key ranges, and this made the process of archival complicated. In this scheme, archiving the entire key space requires us to retrieve the keys from each node separately, which is highly inefficient.", "text": "While using this strategy, the following problems were encountered. First, when a new node joins the system, it needs to 'steal'  its  key  ranges  from  other  nodes.  However,  the  nodes handing  the  key  ranges  off  to  the  new  node  have  to  scan  their local persistence store to retrieve the appropriate set of data items. Note that performing such a scan operation on a production node is tricky as scans are highly resource intensive operations and they need  to  be  executed  in  the  background  without  affecting  the customer performance. This requires us to run the bootstrapping task  at  the  lowest  priority.  However, this  significantly  slows  the bootstrapping process and during busy shopping season, when the nodes are handling millions of requests a day, the bootstrapping has  taken  almost  a  day  to  complete.  Second,  when  a  node joins/leaves  the  system,  the  key  ranges  handled  by  many  nodes change  and  the  Merkle  trees  for  the  new  ranges  need  to  be recalculated,  which  is  a  non-trivial  operation  to  perform  on  a production  system.  Finally,  there  was  no  easy  way  to  take  a snapshot  of  the  entire  key  space  due  to  the  randomness  in  key ranges, and this made the process of archival complicated. In this scheme, archiving the entire key space requires us to retrieve the keys from each node separately, which is highly inefficient."}, {"self_ref": "#/texts/259", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 318.24, "t": 496.885, "r": 560.298, "b": 417.129, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 493]}], "orig": "The fundamental issue with this strategy is that the schemes for data partitioning and data placement are intertwined. For instance, in some cases, it is preferred to add more nodes to the system in order  to  handle  an  increase  in  request  load.  However,  in  this scenario,  it  is  not  possible  to  add  nodes  without  affecting  data partitioning. Ideally, it is desirable to use independent schemes for partitioning and placement. To this end, following strategies were evaluated:", "text": "The fundamental issue with this strategy is that the schemes for data partitioning and data placement are intertwined. For instance, in some cases, it is preferred to add more nodes to the system in order  to  handle  an  increase  in  request  load.  However,  in  this scenario,  it  is  not  possible  to  add  nodes  without  affecting  data partitioning. Ideally, it is desirable to use independent schemes for partitioning and placement. To this end, following strategies were evaluated:"}, {"self_ref": "#/texts/260", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 318.24, "t": 408.085, "r": 560.336, "b": 255.84899999999993, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 996]}], "orig": "Strategy 2: T random tokens per node and equal sized partitions: In  this  strategy,  the  hash  space  is  divided  into  Q  equally  sized partitions/ranges and each node is assigned T random tokens. Q is usually  set  such  that  Q  >>  N  and  Q  >>  S*T,  where  S  is  the number  of  nodes  in  the  system.  In  this  strategy,  the  tokens  are only used to build the function that maps values in the hash space to the ordered lists of nodes and not to decide the partitioning. A partition is placed on the first N unique nodes that are encountered while walking the consistent hashing ring clockwise from the end of the partition. Figure 7 illustrates this strategy for N=3. In this example, nodes A, B, C are encountered while walking the ring from the  end  of  the  partition  that  contains  key  k1.  The  primary advantages of this strategy are: (i) decoupling of partitioning and partition placement, and (ii) enabling the possibility of changing the placement scheme at runtime.", "text": "Strategy 2: T random tokens per node and equal sized partitions: In  this  strategy,  the  hash  space  is  divided  into  Q  equally  sized partitions/ranges and each node is assigned T random tokens. Q is usually  set  such  that  Q  >>  N  and  Q  >>  S*T,  where  S  is  the number  of  nodes  in  the  system.  In  this  strategy,  the  tokens  are only used to build the function that maps values in the hash space to the ordered lists of nodes and not to decide the partitioning. A partition is placed on the first N unique nodes that are encountered while walking the consistent hashing ring clockwise from the end of the partition. Figure 7 illustrates this strategy for N=3. In this example, nodes A, B, C are encountered while walking the ring from the  end  of  the  partition  that  contains  key  k1.  The  primary advantages of this strategy are: (i) decoupling of partitioning and partition placement, and (ii) enabling the possibility of changing the placement scheme at runtime."}, {"self_ref": "#/texts/261", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 318.24, "t": 246.86400000000003, "r": 560.338, "b": 156.72900000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 609]}], "orig": "Strategy 3: Q/S tokens per node, equal-sized partitions: Similar to strategy  2,  this  strategy  divides  the  hash  space  into  Q  equally sized partitions and the placement of partition is decoupled from the  partitioning  scheme.  Moreover,  each  node  is  assigned  Q/S tokens  where  S  is  the  number  of  nodes  in  the  system.  When  a node leaves the system, its tokens are randomly distributed to the remaining nodes such that these properties are preserved. Similarly,  when  a  node  joins  the  system  it  \"steals\"  tokens  from nodes in the system in a way that preserves these properties.", "text": "Strategy 3: Q/S tokens per node, equal-sized partitions: Similar to strategy  2,  this  strategy  divides  the  hash  space  into  Q  equally sized partitions and the placement of partition is decoupled from the  partitioning  scheme.  Moreover,  each  node  is  assigned  Q/S tokens  where  S  is  the  number  of  nodes  in  the  system.  When  a node leaves the system, its tokens are randomly distributed to the remaining nodes such that these properties are preserved. Similarly,  when  a  node  joins  the  system  it  \"steals\"  tokens  from nodes in the system in a way that preserves these properties."}, {"self_ref": "#/texts/262", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 12, "bbox": {"l": 318.241, "t": 147.74400000000003, "r": 560.302, "b": 78.30899999999997, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 468]}], "orig": "The efficiency of these three strategies is evaluated for a system with S=30 and N=3. However, comparing these different strategies  in  a  fair  manner  is  hard  as  different  strategies  have different configurations to tune their efficiency. For instance, the load distribution property of strategy 1 depends on the number of tokens  (i.e.,  T)  while  strategy  3  depends  on  the  number  of partitions (i.e., Q). One fair way to compare these strategies is to", "text": "The efficiency of these three strategies is evaluated for a system with S=30 and N=3. However, comparing these different strategies  in  a  fair  manner  is  hard  as  different  strategies  have different configurations to tune their efficiency. For instance, the load distribution property of strategy 1 depends on the number of tokens  (i.e.,  T)  while  strategy  3  depends  on  the  number  of partitions (i.e., Q). One fair way to compare these strategies is to"}, {"self_ref": "#/texts/263", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 12, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "206 1", "text": "206 1"}, {"self_ref": "#/texts/264", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 13, "bbox": {"l": 51.0, "t": 559.824, "r": 290.817, "b": 500.529, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 324]}], "orig": "Figure  8:  Comparison  of  the  load  distribution  efficiency  of different  strategies  for  system  with  30  nodes  and  N=3  with equal  amount  of  metadata  maintained  at  each  node.  The values of the system size and number of replicas are based on the typical configuration deployed for majority of our services.", "text": "Figure  8:  Comparison  of  the  load  distribution  efficiency  of different  strategies  for  system  with  30  nodes  and  N=3  with equal  amount  of  metadata  maintained  at  each  node.  The values of the system size and number of replicas are based on the typical configuration deployed for majority of our services."}, {"self_ref": "#/texts/265", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 66.36, "t": 603.455, "r": 73.161, "b": 599.046, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.4", "text": "0.4"}, {"self_ref": "#/texts/266", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 66.36, "t": 621.455, "r": 73.161, "b": 617.047, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.5", "text": "0.5"}, {"self_ref": "#/texts/267", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 66.36, "t": 639.815, "r": 73.161, "b": 635.407, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.6", "text": "0.6"}, {"self_ref": "#/texts/268", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 66.36, "t": 657.875, "r": 73.161, "b": 653.467, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.7", "text": "0.7"}, {"self_ref": "#/texts/269", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 66.36, "t": 675.876, "r": 73.161, "b": 671.467, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.8", "text": "0.8"}, {"self_ref": "#/texts/270", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 66.36, "t": 694.236, "r": 73.161, "b": 689.827, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 3]}], "orig": "0.9", "text": "0.9"}, {"self_ref": "#/texts/271", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 70.56, "t": 712.236, "r": 73.161, "b": 707.827, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "1", "text": "1"}, {"self_ref": "#/texts/272", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 75.96, "t": 596.195, "r": 78.561, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1]}], "orig": "0", "text": "0"}, {"self_ref": "#/texts/273", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 99.779, "t": 596.195, "r": 110.418, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 4]}], "orig": "5000", "text": "5000"}, {"self_ref": "#/texts/274", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 127.016, "t": 596.195, "r": 140.355, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "10000", "text": "10000"}, {"self_ref": "#/texts/275", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 155.033, "t": 596.195, "r": 168.372, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "15000", "text": "15000"}, {"self_ref": "#/texts/276", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 183.469, "t": 596.195, "r": 196.808, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "20000", "text": "20000"}, {"self_ref": "#/texts/277", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 211.486, "t": 596.195, "r": 224.825, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "25000", "text": "25000"}, {"self_ref": "#/texts/278", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 239.862, "t": 596.195, "r": 253.261, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "30000", "text": "30000"}, {"self_ref": "#/texts/279", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 267.878, "t": 596.195, "r": 281.277, "b": 591.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "35000", "text": "35000"}, {"self_ref": "#/texts/280", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 102.48, "t": 587.195, "r": 249.538, "b": 582.787, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 60]}], "orig": "Size of metadata maintained at each node (in abstract units)", "text": "Size of metadata maintained at each node (in abstract units)"}, {"self_ref": "#/texts/281", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 59.365, "t": 688.981, "r": 63.774, "b": 615.42, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 30]}], "orig": "Efficieny (mean load/max load)", "text": "Efficieny (mean load/max load)"}, {"self_ref": "#/texts/282", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 237.96, "t": 624.515, "r": 260.902, "b": 620.106, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Strategy 1", "text": "Strategy 1"}, {"self_ref": "#/texts/283", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 237.96, "t": 616.835, "r": 260.902, "b": 612.426, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Strategy 2", "text": "Strategy 2"}, {"self_ref": "#/texts/284", "parent": {"$ref": "#/pictures/7"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 237.96, "t": 608.795, "r": 260.902, "b": 604.386, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "Strategy 3", "text": "Strategy 3"}, {"self_ref": "#/texts/285", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 54.0, "t": 488.724, "r": 296.093, "b": 429.668, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 372]}], "orig": "evaluate the skew in their load distribution while all strategies use the same amount of space to maintain their membership information.  For  instance,  in  strategy  1  each  node  needs  to maintain  the  token  positions  of  all  the  nodes  in  the  ring  and  in strategy 3 each node needs to maintain the information regarding the partitions assigned to each node.", "text": "evaluate the skew in their load distribution while all strategies use the same amount of space to maintain their membership information.  For  instance,  in  strategy  1  each  node  needs  to maintain  the  token  positions  of  all  the  nodes  in  the  ring  and  in strategy 3 each node needs to maintain the information regarding the partitions assigned to each node."}, {"self_ref": "#/texts/286", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 54.0, "t": 420.624, "r": 296.111, "b": 351.248, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 418]}], "orig": "In our next experiment, these strategies were evaluated by varying the relevant parameters (T and Q). The load balancing efficiency of each strategy was measured for different sizes of membership information that needs to be maintained at each node, where Load balancing efficiency is  defined as the ratio of average number of requests served by each node to the maximum number of requests served by the hottest node.", "text": "In our next experiment, these strategies were evaluated by varying the relevant parameters (T and Q). The load balancing efficiency of each strategy was measured for different sizes of membership information that needs to be maintained at each node, where Load balancing efficiency is  defined as the ratio of average number of requests served by each node to the maximum number of requests served by the hottest node."}, {"self_ref": "#/texts/287", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 54.0, "t": 342.203, "r": 298.276, "b": 76.14800000000002, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1758]}], "orig": "The results are given in Figure 8. As seen in the figure, strategy 3 achieves the best load balancing efficiency and strategy 2 has the worst  load  balancing  efficiency.  For  a  brief  time,  Strategy  2 served  as  an  interim  setup  during  the  process  of  migrating Dynamo instances from using Strategy 1 to Strategy 3. Compared to Strategy 1, Strategy 3 achieves better efficiency and reduces the size of membership information maintained at each node by three orders of magnitude. While storage is not a major issue the nodes gossip the membership information periodically and as such it is desirable  to  keep  this  information  as  compact  as  possible.    In addition to this, strategy 3 is advantageous and simpler to deploy for the  following  reasons:  (i) Faster  bootstrapping/recovery: Since  partition  ranges  are  fixed,  they  can  be  stored  in  separate files,  meaning  a  partition  can  be  relocated  as  a  unit  by  simply transferring  the  file  (avoiding  random  accesses  needed  to  locate specific  items).  This  simplifies  the  process  of  bootstrapping  and recovery. (ii) Ease of archival : Periodical archiving of the dataset is a mandatory requirement for most of Amazon storage services. Archiving  the  entire  dataset  stored  by  Dynamo  is  simpler  in strategy 3  because  the  partition  files  can  be  archived  separately. By contrast, in Strategy 1, the  tokens  are  chosen  randomly  and, archiving the data stored in Dynamo requires retrieving the keys from  individual  nodes  separately  and  is  usually  inefficient  and slow.  The  disadvantage  of  strategy  3  is  that  changing  the  node membership requires coordination in order to preserve the properties required of the assignment.", "text": "The results are given in Figure 8. As seen in the figure, strategy 3 achieves the best load balancing efficiency and strategy 2 has the worst  load  balancing  efficiency.  For  a  brief  time,  Strategy  2 served  as  an  interim  setup  during  the  process  of  migrating Dynamo instances from using Strategy 1 to Strategy 3. Compared to Strategy 1, Strategy 3 achieves better efficiency and reduces the size of membership information maintained at each node by three orders of magnitude. While storage is not a major issue the nodes gossip the membership information periodically and as such it is desirable  to  keep  this  information  as  compact  as  possible.    In addition to this, strategy 3 is advantageous and simpler to deploy for the  following  reasons:  (i) Faster  bootstrapping/recovery: Since  partition  ranges  are  fixed,  they  can  be  stored  in  separate files,  meaning  a  partition  can  be  relocated  as  a  unit  by  simply transferring  the  file  (avoiding  random  accesses  needed  to  locate specific  items).  This  simplifies  the  process  of  bootstrapping  and recovery. (ii) Ease of archival : Periodical archiving of the dataset is a mandatory requirement for most of Amazon storage services. Archiving  the  entire  dataset  stored  by  Dynamo  is  simpler  in strategy 3  because  the  partition  files  can  be  archived  separately. By contrast, in Strategy 1, the  tokens  are  chosen  randomly  and, archiving the data stored in Dynamo requires retrieving the keys from  individual  nodes  separately  and  is  usually  inefficient  and slow.  The  disadvantage  of  strategy  3  is  that  changing  the  node membership requires coordination in order to preserve the properties required of the assignment."}, {"self_ref": "#/texts/288", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 716.712, "r": 511.399, "b": 691.092, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 42]}], "orig": "6.3 Divergent Versions: When and How Many?", "text": "6.3 Divergent Versions: When and How Many?", "level": 1}, {"self_ref": "#/texts/289", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 688.164, "r": 562.477, "b": 608.349, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 501]}], "orig": "As noted earlier, Dynamo is designed to tradeoff consistency for availability. To understand the precise impact of different failures on  consistency,  detailed  data  is  required  on  multiple  factors: outage length, type of failure, component reliability, workload etc. Presenting these numbers in detail is outside of the scope of this paper.  However,  this  section  discusses  a  good  summary  metric: the number of divergent versions seen by the application in a live production environment.", "text": "As noted earlier, Dynamo is designed to tradeoff consistency for availability. To understand the precise impact of different failures on  consistency,  detailed  data  is  required  on  multiple  factors: outage length, type of failure, component reliability, workload etc. Presenting these numbers in detail is outside of the scope of this paper.  However,  this  section  discusses  a  good  summary  metric: the number of divergent versions seen by the application in a live production environment."}, {"self_ref": "#/texts/290", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 599.365, "r": 560.347, "b": 478.15, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 797]}], "orig": "Divergent versions of a data item arise in two scenarios. The first is  when  the  system  is  facing  failure  scenarios  such  as  node failures, data center failures, and network partitions. The second is when the system is handling a large number of concurrent writers to a single data item and multiple nodes end up coordinating the updates concurrently. From  both a usability and efficiency perspective,  it  is  preferred  to  keep  the  number  of  divergent versions  at  any  given  time  as  low  as  possible.  If  the  versions cannot be syntactically reconciled based on vector clocks alone, they have  to  be passed  to the business  logic for  semantic reconciliation. Semantic reconciliation introduces additional load on services, so it is desirable to minimize the need for it.", "text": "Divergent versions of a data item arise in two scenarios. The first is  when  the  system  is  facing  failure  scenarios  such  as  node failures, data center failures, and network partitions. The second is when the system is handling a large number of concurrent writers to a single data item and multiple nodes end up coordinating the updates concurrently. From  both a usability and efficiency perspective,  it  is  preferred  to  keep  the  number  of  divergent versions  at  any  given  time  as  low  as  possible.  If  the  versions cannot be syntactically reconciled based on vector clocks alone, they have  to  be passed  to the business  logic for  semantic reconciliation. Semantic reconciliation introduces additional load on services, so it is desirable to minimize the need for it."}, {"self_ref": "#/texts/291", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 469.165, "r": 562.445, "b": 410.11, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 385]}], "orig": "In  our  next  experiment,  the  number  of  versions  returned  to  the shopping  cart  service  was  profiled  for  a  period  of  24  hours. During this period, 99.94% of requests saw exactly one version; 0.00057% of requests saw 2 versions; 0.00047% of requests saw 3 versions  and  0.00009%  of  requests  saw  4  versions.  This  shows that divergent versions are created rarely.", "text": "In  our  next  experiment,  the  number  of  versions  returned  to  the shopping  cart  service  was  profiled  for  a  period  of  24  hours. During this period, 99.94% of requests saw exactly one version; 0.00057% of requests saw 2 versions; 0.00047% of requests saw 3 versions  and  0.00009%  of  requests  saw  4  versions.  This  shows that divergent versions are created rarely."}, {"self_ref": "#/texts/292", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 401.065, "r": 560.324, "b": 342.009, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 390]}], "orig": "Experience  shows  that  the  increase  in  the  number  of  divergent versions is contributed not by failures but due to the increase in number  of  concurrent  writers.  The  increase  in  the  number  of concurrent writes is usually triggered by busy robots (automated client programs) and rarely by humans. This issue is not discussed in detail due to the sensitive nature of the story.", "text": "Experience  shows  that  the  increase  in  the  number  of  divergent versions is contributed not by failures but due to the increase in number  of  concurrent  writers.  The  increase  in  the  number  of concurrent writes is usually triggered by busy robots (automated client programs) and rarely by humans. This issue is not discussed in detail due to the sensitive nature of the story."}, {"self_ref": "#/texts/293", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 331.932, "r": 507.348, "b": 308.352, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 47]}], "orig": "6.4 Client-driven or Server-driven Coordination", "text": "6.4 Client-driven or Server-driven Coordination", "level": 1}, {"self_ref": "#/texts/294", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 305.364, "r": 560.32, "b": 194.529, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 716]}], "orig": "As  mentioned  in  Section  5,  Dynamo  has  a  request  coordination component that uses a state machine to handle incoming requests. Client requests are uniformly assigned to nodes in the ring by a load balancer. Any Dynamo node can act as a coordinator for a read request. Write requests on the other hand will be coordinated by a node in the key's current preference list. This restriction is due  to the fact that these preferred  nodes  have  the  added responsibility  of  creating  a  new  version  stamp  that  causally subsumes the version that has been updated by the write request. Note  that  if  Dynamo's  versioning  scheme  is  based  on  physical timestamps, any node can coordinate a write request.", "text": "As  mentioned  in  Section  5,  Dynamo  has  a  request  coordination component that uses a state machine to handle incoming requests. Client requests are uniformly assigned to nodes in the ring by a load balancer. Any Dynamo node can act as a coordinator for a read request. Write requests on the other hand will be coordinated by a node in the key's current preference list. This restriction is due  to the fact that these preferred  nodes  have  the  added responsibility  of  creating  a  new  version  stamp  that  causally subsumes the version that has been updated by the write request. Note  that  if  Dynamo's  versioning  scheme  is  based  on  physical timestamps, any node can coordinate a write request."}, {"self_ref": "#/texts/295", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 13, "bbox": {"l": 318.24, "t": 185.54399999999998, "r": 560.342, "b": 74.70900000000006, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 715]}], "orig": "An  alternative  approach  to  request  coordination  is  to  move  the state machine to the client nodes. In this scheme client applications use a library to perform request coordination locally. A client periodically picks a random Dynamo node and downloads its current view of Dynamo membership state. Using this information the client can determine which set of nodes form the  preference  list  for  any  given  key.  Read  requests  can  be coordinated at the client node thereby avoiding the extra network hop  that  is  incurred  if  the  request  were  assigned  to  a  random Dynamo  node  by  the  load balancer. Writes will either be forwarded  to  a  node  in  the  key's  preference  list  or  can  be", "text": "An  alternative  approach  to  request  coordination  is  to  move  the state machine to the client nodes. In this scheme client applications use a library to perform request coordination locally. A client periodically picks a random Dynamo node and downloads its current view of Dynamo membership state. Using this information the client can determine which set of nodes form the  preference  list  for  any  given  key.  Read  requests  can  be coordinated at the client node thereby avoiding the extra network hop  that  is  incurred  if  the  request  were  assigned  to  a  random Dynamo  node  by  the  load balancer. Writes will either be forwarded  to  a  node  in  the  key's  preference  list  or  can  be"}, {"self_ref": "#/texts/296", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 13, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "207 1", "text": "207 1"}, {"self_ref": "#/texts/297", "parent": {"$ref": "#/tables/1"}, "children": [], "content_layer": "body", "label": "caption", "prov": [{"page_no": 14, "bbox": {"l": 54.24, "t": 717.564, "r": 296.546, "b": 699.908, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 84]}], "orig": "Table  2: Performance  of  client-driven and  server-driven coordination approaches.", "text": "Table  2: Performance  of  client-driven and  server-driven coordination approaches."}, {"self_ref": "#/texts/298", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 54.0, "t": 578.964, "r": 295.982, "b": 561.309, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 67]}], "orig": "coordinated locally if Dynamo is using timestamps based versioning.", "text": "coordinated locally if Dynamo is using timestamps based versioning."}, {"self_ref": "#/texts/299", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 54.0, "t": 552.324, "r": 296.102, "b": 400.089, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 960]}], "orig": "An important advantage of the client-driven coordination approach is that a load balancer is no longer required to uniformly distribute client load. Fair load distribution is implicitly guaranteed by the near uniform assignment of keys to the storage nodes. Obviously, the efficiency of this scheme is dependent on how fresh the membership information is at the client. Currently clients  poll  a  random  Dynamo  node  every  10  seconds  for membership updates.  A  pull  based  approach  was  chosen  over  a push based one as the former scales better with large number of clients  and  requires  very  little  state  to  be  maintained  at  servers regarding  clients.  However,  in  the  worst  case  the  client  can  be exposed to stale membership for duration of 10 seconds. In case, if  the  client  detects  its  membership  table  is  stale  (for  instance, when some members are unreachable), it will immediately refresh its membership information.", "text": "An important advantage of the client-driven coordination approach is that a load balancer is no longer required to uniformly distribute client load. Fair load distribution is implicitly guaranteed by the near uniform assignment of keys to the storage nodes. Obviously, the efficiency of this scheme is dependent on how fresh the membership information is at the client. Currently clients  poll  a  random  Dynamo  node  every  10  seconds  for membership updates.  A  pull  based  approach  was  chosen  over  a push based one as the former scales better with large number of clients  and  requires  very  little  state  to  be  maintained  at  servers regarding  clients.  However,  in  the  worst  case  the  client  can  be exposed to stale membership for duration of 10 seconds. In case, if  the  client  detects  its  membership  table  is  stale  (for  instance, when some members are unreachable), it will immediately refresh its membership information."}, {"self_ref": "#/texts/300", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 54.0, "t": 393.096, "r": 296.088, "b": 228.48900000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1043]}], "orig": "Table 2 shows the latency improvements at the 99.9 th percentile and averages that were observed for a period of 24 hours using client-driven coordination compared to the server-driven approach.  As  seen  in  the  table,  the  client-driven  coordination approach  reduces  the  latencies  by  at  least  30  milliseconds  for 99.9 th percentile  latencies  and  decreases  the  average  by  3  to  4 milliseconds.  The  latency  improvement  is  because  the  clientdriven approach eliminates the overhead of the load balancer and the  extra  network  hop  that  may  be  incurred  when  a  request  is assigned to a random node. As seen in the table, average latencies tend to be significantly lower than latencies at the 99.9th percentile.  This  is  because  Dynamo's  storage engine caches and write  buffer  have  good  hit  ratios.  Moreover,  since  the  load balancers  and  network  introduce  additional  variability  to  the response time,  the  gain  in  response  time  is  higher  for  the  99.9 th percentile than the average.", "text": "Table 2 shows the latency improvements at the 99.9 th percentile and averages that were observed for a period of 24 hours using client-driven coordination compared to the server-driven approach.  As  seen  in  the  table,  the  client-driven  coordination approach  reduces  the  latencies  by  at  least  30  milliseconds  for 99.9 th percentile  latencies  and  decreases  the  average  by  3  to  4 milliseconds.  The  latency  improvement  is  because  the  clientdriven approach eliminates the overhead of the load balancer and the  extra  network  hop  that  may  be  incurred  when  a  request  is assigned to a random node. As seen in the table, average latencies tend to be significantly lower than latencies at the 99.9th percentile.  This  is  because  Dynamo's  storage engine caches and write  buffer  have  good  hit  ratios.  Moreover,  since  the  load balancers  and  network  introduce  additional  variability  to  the response time,  the  gain  in  response  time  is  higher  for  the  99.9 th percentile than the average."}, {"self_ref": "#/texts/301", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 14, "bbox": {"l": 54.0, "t": 218.41200000000003, "r": 279.732, "b": 194.89200000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 45]}], "orig": "6.5 Balancing background vs. foreground tasks", "text": "6.5 Balancing background vs. foreground tasks", "level": 1}, {"self_ref": "#/texts/302", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 54.0, "t": 191.904, "r": 296.125, "b": 81.06899999999996, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 744]}, {"page_no": 14, "bbox": {"l": 318.24, "t": 717.744, "r": 560.351, "b": 679.389, "coord_origin": "BOTTOMLEFT"}, "charspan": [745, 947]}], "orig": "Each  node  performs  different  kinds  of  background  tasks  for replica synchronization and data handoff (either due to hinting or adding/removing  nodes)  in  addition  to  its  normal  foreground put/get operations. In early production settings, these background tasks  triggered  the  problem  of  resource  contention  and  affected the  performance of the regular put and get operations. Hence, it became necessary to ensure that background tasks ran only when the  regular  critical  operations  are  not  affected  significantly.  To this end, the background tasks were integrated with an admission control  mechanism.  Each  of  the  background  tasks  uses  this controller to reserve runtime slices of the resource (e.g. database), shared across all background tasks. A feedback mechanism based on the monitored performance of the foreground tasks is employed to change the number of slices that are available to the background tasks.", "text": "Each  node  performs  different  kinds  of  background  tasks  for replica synchronization and data handoff (either due to hinting or adding/removing  nodes)  in  addition  to  its  normal  foreground put/get operations. In early production settings, these background tasks  triggered  the  problem  of  resource  contention  and  affected the  performance of the regular put and get operations. Hence, it became necessary to ensure that background tasks ran only when the  regular  critical  operations  are  not  affected  significantly.  To this end, the background tasks were integrated with an admission control  mechanism.  Each  of  the  background  tasks  uses  this controller to reserve runtime slices of the resource (e.g. database), shared across all background tasks. A feedback mechanism based on the monitored performance of the foreground tasks is employed to change the number of slices that are available to the background tasks."}, {"self_ref": "#/texts/303", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 318.24, "t": 670.344, "r": 560.364, "b": 507.788, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1011]}], "orig": "The  admission  controller  constantly  monitors  the  behavior  of resource accesses while executing a \"foreground\" put/get operation. Monitored aspects include latencies for disk operations, failed  database  accesses  due  to  lock-contention  and  transaction timeouts, and request queue wait times. This information is used to  check  whether  the  percentiles  of  latencies  (or  failures)  in  a given trailing  time  window are close to a desired threshold. For example, the background controller checks to see how close the 99 th percentile database read latency (over the last 60 seconds) is to  a  preset  threshold  (say  50ms).    The  controller  uses  such comparisons to assess the resource availability for the foreground operations. Subsequently, it decides on how many time slices will be available to background tasks, thereby using the feedback loop to limit the intrusiveness of the background activities.  Note that a similar problem of managing background tasks has been studied in [4].", "text": "The  admission  controller  constantly  monitors  the  behavior  of resource accesses while executing a \"foreground\" put/get operation. Monitored aspects include latencies for disk operations, failed  database  accesses  due  to  lock-contention  and  transaction timeouts, and request queue wait times. This information is used to  check  whether  the  percentiles  of  latencies  (or  failures)  in  a given trailing  time  window are close to a desired threshold. For example, the background controller checks to see how close the 99 th percentile database read latency (over the last 60 seconds) is to  a  preset  threshold  (say  50ms).    The  controller  uses  such comparisons to assess the resource availability for the foreground operations. Subsequently, it decides on how many time slices will be available to background tasks, thereby using the feedback loop to limit the intrusiveness of the background activities.  Note that a similar problem of managing background tasks has been studied in [4]."}, {"self_ref": "#/texts/304", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 14, "bbox": {"l": 318.24, "t": 497.712, "r": 404.088, "b": 487.932, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 14]}], "orig": "6.6 Discussion", "text": "6.6 Discussion", "level": 1}, {"self_ref": "#/texts/305", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 318.24, "t": 484.944, "r": 560.296, "b": 415.509, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 445]}], "orig": "This  section  summarizes  some  of  the  experiences  gained  during the  process  of  implementation  and  maintenance  of  Dynamo. Many Amazon internal services have used Dynamo for the past two years and it has provided significant levels of availability to its applications. In particular, applications have received successful  responses  (without  timing  out)  for  99.9995%  of  its requests and no data loss event has occurred to date.", "text": "This  section  summarizes  some  of  the  experiences  gained  during the  process  of  implementation  and  maintenance  of  Dynamo. Many Amazon internal services have used Dynamo for the past two years and it has provided significant levels of availability to its applications. In particular, applications have received successful  responses  (without  timing  out)  for  99.9995%  of  its requests and no data loss event has occurred to date."}, {"self_ref": "#/texts/306", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 318.24, "t": 406.524, "r": 560.336, "b": 171.48900000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 1522]}], "orig": "Moreover, the  primary  advantage  of  Dynamo  is  that  it  provides the necessary knobs using the three parameters of (N,R,W) to tune their  instance  based  on  their  needs..  Unlike  popular  commercial data stores, Dynamo exposes data consistency and reconciliation logic issues to the developers. At the outset, one may expect the application logic to become more complex. However, historically, Amazon's  platform  is built for high availability  and  many applications  are  designed  to  handle  different  failure  modes  and inconsistencies that may arise. Hence, porting such applications to use  Dynamo  was  a  relatively  simple  task.  For  new  applications that  want  to  use  Dynamo,  some  analysis  is  required  during  the initial stages of the development  to  pick  the right conflict resolution mechanisms that meet the business case appropriately. Finally,  Dynamo  adopts  a  full  membership  model  where  each node  is  aware  of  the  data  hosted  by  its  peers.  To  do  this,  each node actively gossips the full routing table with other nodes in the system. This model works well for a system that contains couple of hundreds of nodes. However, scaling such a design to run with tens of thousands of nodes is not trivial because the overhead in maintaining the routing table increases with the system size. This limitation might be overcome by introducing hierarchical extensions  to  Dynamo.  Also,  note  that  this  problem  is  actively addressed by O(1) DHT systems(e.g., [14]).", "text": "Moreover, the  primary  advantage  of  Dynamo  is  that  it  provides the necessary knobs using the three parameters of (N,R,W) to tune their  instance  based  on  their  needs..  Unlike  popular  commercial data stores, Dynamo exposes data consistency and reconciliation logic issues to the developers. At the outset, one may expect the application logic to become more complex. However, historically, Amazon's  platform  is built for high availability  and  many applications  are  designed  to  handle  different  failure  modes  and inconsistencies that may arise. Hence, porting such applications to use  Dynamo  was  a  relatively  simple  task.  For  new  applications that  want  to  use  Dynamo,  some  analysis  is  required  during  the initial stages of the development  to  pick  the right conflict resolution mechanisms that meet the business case appropriately. Finally,  Dynamo  adopts  a  full  membership  model  where  each node  is  aware  of  the  data  hosted  by  its  peers.  To  do  this,  each node actively gossips the full routing table with other nodes in the system. This model works well for a system that contains couple of hundreds of nodes. However, scaling such a design to run with tens of thousands of nodes is not trivial because the overhead in maintaining the routing table increases with the system size. This limitation might be overcome by introducing hierarchical extensions  to  Dynamo.  Also,  note  that  this  problem  is  actively addressed by O(1) DHT systems(e.g., [14])."}, {"self_ref": "#/texts/307", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 14, "bbox": {"l": 318.24, "t": 161.47199999999998, "r": 427.222, "b": 151.692, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 14]}], "orig": "7. CONCLUSIONS", "text": "7. CONCLUSIONS", "level": 1}, {"self_ref": "#/texts/308", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 14, "bbox": {"l": 318.24, "t": 148.70399999999995, "r": 560.346, "b": 79.269, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 467]}], "orig": "This  paper  described  Dynamo,  a  highly  available  and  scalable data store, used for storing state of a number of core services of Amazon.com's e-commerce platform. Dynamo has provided the desired  levels  of  availability  and  performance  and  has  been successful  in  handling  server  failures,  data  center  failures  and network partitions. Dynamo is incrementally scalable and allows service  owners  to  scale  up  and  down  based  on  their  current", "text": "This  paper  described  Dynamo,  a  highly  available  and  scalable data store, used for storing state of a number of core services of Amazon.com's e-commerce platform. Dynamo has provided the desired  levels  of  availability  and  performance  and  has  been successful  in  handling  server  failures,  data  center  failures  and network partitions. Dynamo is incrementally scalable and allows service  owners  to  scale  up  and  down  based  on  their  current"}, {"self_ref": "#/texts/309", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 14, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "208 1", "text": "208 1"}, {"self_ref": "#/texts/310", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 717.744, "r": 296.087, "b": 679.388, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 204]}], "orig": "request  load.  Dynamo  allows  service  owners  to  customize  their storage system to meet their desired performance, durability and consistency SLAs by allowing them to tune the parameters N, R, and W.", "text": "request  load.  Dynamo  allows  service  owners  to  customize  their storage system to meet their desired performance, durability and consistency SLAs by allowing them to tune the parameters N, R, and W."}, {"self_ref": "#/texts/311", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 670.344, "r": 296.095, "b": 611.288, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 347]}], "orig": "The production use of Dynamo for the past year demonstrates that decentralized  techniques  can  be  combined  to  provide  a  single highly-available system. Its success in one of the most challenging  application  environments  shows  that  an  eventualconsistent  storage  system  can  be  a  building  block  for  highlyavailable applications.", "text": "The production use of Dynamo for the past year demonstrates that decentralized  techniques  can  be  combined  to  provide  a  single highly-available system. Its success in one of the most challenging  application  environments  shows  that  an  eventualconsistent  storage  system  can  be  a  building  block  for  highlyavailable applications."}, {"self_ref": "#/texts/312", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 601.212, "r": 198.258, "b": 591.432, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 16]}], "orig": "ACKNOWLEDGEMENTS", "text": "ACKNOWLEDGEMENTS", "level": 1}, {"self_ref": "#/texts/313", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "text", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 588.444, "r": 296.097, "b": 529.388, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 386]}], "orig": "The authors would like to thank Pat Helland for his contribution to  the  initial  design  of  Dynamo.  We  would  also  like  to  thank Marvin  Theimer  and  Robert  van  Renesse  for  their  comments. Finally, we would like to thank our shepherd, Jeff Mogul, for his detailed  comments and inputs while preparing the camera ready version that vastly improved the quality of the paper.", "text": "The authors would like to thank Pat Helland for his contribution to  the  initial  design  of  Dynamo.  We  would  also  like  to  thank Marvin  Theimer  and  Robert  van  Renesse  for  their  comments. Finally, we would like to thank our shepherd, Jeff Mogul, for his detailed  comments and inputs while preparing the camera ready version that vastly improved the quality of the paper."}, {"self_ref": "#/texts/314", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "body", "label": "section_header", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 519.312, "r": 137.623, "b": 509.532, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 10]}], "orig": "REFERENCES", "text": "REFERENCES", "level": 1}, {"self_ref": "#/texts/315", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 506.544, "r": 294.057, "b": 457.809, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 293]}], "orig": "[1] Adya, A., Bolosky, W. J., Castro, M., Cermak, G., Chaiken, R., Douceur, J. R., Howell, J., Lorch, J. R., Theimer, M., and Wattenhofer, R. P. 2002. Farsite: federated, available, and reliable storage for an incompletely trusted environment. SIGOPS Oper. Syst. Rev. 36, SI (Dec. 2002), 1-14.", "text": "Adya, A., Bolosky, W. J., Castro, M., Cermak, G., Chaiken, R., Douceur, J. R., Howell, J., Lorch, J. R., Theimer, M., and Wattenhofer, R. P. 2002. Farsite: federated, available, and reliable storage for an incompletely trusted environment. SIGOPS Oper. Syst. Rev. 36, SI (Dec. 2002), 1-14.", "enumerated": true, "marker": "[1]"}, {"self_ref": "#/texts/316", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 450.804, "r": 291.835, "b": 412.449, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 183]}], "orig": "[2] Bernstein, P.A., and Goodman, N. An algorithm for concurrency control and recovery in replicated distributed databases. ACM Trans. on Database Systems, 9(4):596-615, December 1984", "text": "Bernstein, P.A., and Goodman, N. An algorithm for concurrency control and recovery in replicated distributed databases. ACM Trans. on Database Systems, 9(4):596-615, December 1984", "enumerated": true, "marker": "[2]"}, {"self_ref": "#/texts/317", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 405.444, "r": 294.337, "b": 336.009, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 384]}], "orig": "[3] Chang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M., Chandra, T., Fikes, A., and Gruber, R. E. 2006. Bigtable: a distributed storage system for structured data. In Proceedings of the 7th Conference on USENIX Symposium on Operating Systems Design and Implementation - Volume 7 (Seattle, WA, November 06 - 08, 2006). USENIX Association, Berkeley, CA, 15-15.", "text": "Chang, F., Dean, J., Ghemawat, S., Hsieh, W. C., Wallach, D. A., Burrows, M., Chandra, T., Fikes, A., and Gruber, R. E. 2006. Bigtable: a distributed storage system for structured data. In Proceedings of the 7th Conference on USENIX Symposium on Operating Systems Design and Implementation - Volume 7 (Seattle, WA, November 06 - 08, 2006). USENIX Association, Berkeley, CA, 15-15.", "enumerated": true, "marker": "[3]"}, {"self_ref": "#/texts/318", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 329.005, "r": 293.423, "b": 300.969, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 147]}], "orig": "[4] Douceur, J. R. and Bolosky, W. J. 2000. Process-based regulation of low-importance processes. SIGOPS Oper. Syst. Rev. 34, 2 (Apr. 2000), 26-27.", "text": "Douceur, J. R. and Bolosky, W. J. 2000. Process-based regulation of low-importance processes. SIGOPS Oper. Syst. Rev. 34, 2 (Apr. 2000), 26-27.", "enumerated": true, "marker": "[4]"}, {"self_ref": "#/texts/319", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.0, "t": 293.905, "r": 288.797, "b": 234.84899999999993, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 303]}], "orig": "[5] Fox, A., Gribble, S. D., Chawathe, Y., Brewer, E. A., and Gauthier, P. 1997. Cluster-based scalable network services. In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles (Saint Malo, France, October 05 - 08, 1997). W. M. Waite, Ed. SOSP '97. ACM Press, New York, NY, 78-91.", "text": "Fox, A., Gribble, S. D., Chawathe, Y., Brewer, E. A., and Gauthier, P. 1997. Cluster-based scalable network services. In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles (Saint Malo, France, October 05 - 08, 1997). W. M. Waite, Ed. SOSP '97. ACM Press, New York, NY, 78-91.", "enumerated": true, "marker": "[5]"}, {"self_ref": "#/texts/320", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.001, "t": 227.84500000000003, "r": 295.042, "b": 179.10900000000004, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 245]}], "orig": "[6] Ghemawat, S., Gobioff, H., and Leung, S. 2003. The Google file system. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles (Bolton Landing, NY, USA, October 19 - 22, 2003). SOSP '03. ACM Press, New York, NY, 29-43.", "text": "Ghemawat, S., Gobioff, H., and Leung, S. 2003. The Google file system. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles (Bolton Landing, NY, USA, October 19 - 22, 2003). SOSP '03. ACM Press, New York, NY, 29-43.", "enumerated": true, "marker": "[6]"}, {"self_ref": "#/texts/321", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.001, "t": 172.10500000000002, "r": 290.012, "b": 112.99000000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 293]}], "orig": "[7] Gray, J., Helland, P., O'Neil, P., and Shasha, D. 1996. The dangers of replication and a solution. In Proceedings of the 1996 ACM SIGMOD international Conference on Management of Data (Montreal, Quebec, Canada, June 04 06, 1996). J. Widom, Ed. SIGMOD '96. ACM Press, New York, NY, 173-182.", "text": "Gray, J., Helland, P., O'Neil, P., and Shasha, D. 1996. The dangers of replication and a solution. In Proceedings of the 1996 ACM SIGMOD international Conference on Management of Data (Montreal, Quebec, Canada, June 04 06, 1996). J. Widom, Ed. SIGMOD '96. ACM Press, New York, NY, 173-182.", "enumerated": true, "marker": "[7]"}, {"self_ref": "#/texts/322", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 54.001, "t": 105.98599999999999, "r": 284.576, "b": 77.95000000000005, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 171]}], "orig": "[8] Gupta, I., Chandra, T. D., and Goldszmidt, G. S. 2001. On scalable and efficient distributed failure detectors. In Proceedings of the Twentieth Annual ACM Symposium on", "text": "Gupta, I., Chandra, T. D., and Goldszmidt, G. S. 2001. On scalable and efficient distributed failure detectors. In Proceedings of the Twentieth Annual ACM Symposium on", "enumerated": true, "marker": "[8]"}, {"self_ref": "#/texts/323", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 336.241, "t": 717.745, "r": 548.275, "b": 689.71, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 119]}], "orig": "Principles of Distributed Computing (Newport, Rhode Island, United States). PODC '01. ACM Press, New York, NY, 170-179.", "text": "Principles of Distributed Computing (Newport, Rhode Island, United States). PODC '01. ACM Press, New York, NY, 170-179.", "enumerated": false, "marker": ""}, {"self_ref": "#/texts/324", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.241, "t": 682.706, "r": 558.185, "b": 633.97, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 273]}], "orig": "[9] Kubiatowicz, J., Bindel, D., Chen, Y., Czerwinski, S., Eaton, P., Geels, D., Gummadi, R., Rhea, S., Weatherspoon, H., Wells, C., and Zhao, B. 2000. OceanStore: an architecture for global-scale persistent storage. SIGARCH Comput. Archit. News 28, 5 (Dec. 2000), 190-201.", "text": "Kubiatowicz, J., Bindel, D., Chen, Y., Czerwinski, S., Eaton, P., Geels, D., Gummadi, R., Rhea, S., Weatherspoon, H., Wells, C., and Zhao, B. 2000. OceanStore: an architecture for global-scale persistent storage. SIGARCH Comput. Archit. News 28, 5 (Dec. 2000), 190-201.", "enumerated": true, "marker": "[9]"}, {"self_ref": "#/texts/325", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.241, "t": 626.966, "r": 557.681, "b": 557.531, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 378]}], "orig": "[10] Karger, D., Lehman, E., Leighton, T., Panigrahy, R., Levine, M., and Lewin, D. 1997. Consistent hashing and random trees: distributed caching protocols for relieving hot spots on the World Wide Web. In Proceedings of the Twenty-Ninth Annual ACM Symposium on theory of Computing (El Paso, Texas, United States, May 04 - 06, 1997). STOC '97. ACM Press, New York, NY, 654-663.", "text": "Karger, D., Lehman, E., Leighton, T., Panigrahy, R., Levine, M., and Lewin, D. 1997. Consistent hashing and random trees: distributed caching protocols for relieving hot spots on the World Wide Web. In Proceedings of the Twenty-Ninth Annual ACM Symposium on theory of Computing (El Paso, Texas, United States, May 04 - 06, 1997). STOC '97. ACM Press, New York, NY, 654-663.", "enumerated": true, "marker": "[10]"}, {"self_ref": "#/texts/326", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.241, "t": 550.526, "r": 552.96, "b": 532.809, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 118]}], "orig": "[11] Lindsay, B.G.,  et. al., 'Notes on Distributed Databases', Research Report RJ2571(33471), IBM Research, July 1979", "text": "Lindsay, B.G.,  et. al., 'Notes on Distributed Databases', Research Report RJ2571(33471), IBM Research, July 1979", "enumerated": true, "marker": "[11]"}, {"self_ref": "#/texts/327", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 525.805, "r": 549.362, "b": 497.769, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 127]}], "orig": "[12] Lamport, L. Time, clocks, and the ordering of events in a distributed system. ACM Communications, 21(7), pp. 558565, 1978.", "text": "Lamport, L. Time, clocks, and the ordering of events in a distributed system. ACM Communications, 21(7), pp. 558565, 1978.", "enumerated": true, "marker": "[12]"}, {"self_ref": "#/texts/328", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 490.765, "r": 548.348, "b": 462.73, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 140]}], "orig": "[13] Merkle, R. A digital signature based on a conventional encryption function. Proceedings of CRYPTO, pages 369378. Springer-Verlag, 1988.", "text": "Merkle, R. A digital signature based on a conventional encryption function. Proceedings of CRYPTO, pages 369378. Springer-Verlag, 1988.", "enumerated": true, "marker": "[13]"}, {"self_ref": "#/texts/329", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 455.725, "r": 560.09, "b": 406.99, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 272]}], "orig": "[14] Ramasubramanian, V., and Sirer, E. G.  Beehive: O(1)lookup performance for power-law query distributions in peer-topeer overlays. In Proceedings of the 1st Conference on Symposium on Networked Systems Design and Implementation, San Francisco, CA, March 29 - 31, 2004.", "text": "Ramasubramanian, V., and Sirer, E. G.  Beehive: O(1)lookup performance for power-law query distributions in peer-topeer overlays. In Proceedings of the 1st Conference on Symposium on Networked Systems Design and Implementation, San Francisco, CA, March 29 - 31, 2004.", "enumerated": true, "marker": "[14]"}, {"self_ref": "#/texts/330", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 399.986, "r": 560.015, "b": 340.871, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 330]}], "orig": "[15] Reiher, P., Heidemann, J., Ratner, D., Skinner, G., and Popek, G. 1994. Resolving file conflicts in the Ficus file system. In Proceedings of the USENIX Summer 1994 Technical Conference on USENIX Summer 1994 Technical Conference - Volume 1 (Boston, Massachusetts, June 06 - 10, 1994). USENIX Association, Berkeley, CA, 12-12..", "text": "Reiher, P., Heidemann, J., Ratner, D., Skinner, G., and Popek, G. 1994. Resolving file conflicts in the Ficus file system. In Proceedings of the USENIX Summer 1994 Technical Conference on USENIX Summer 1994 Technical Conference - Volume 1 (Boston, Massachusetts, June 06 - 10, 1994). USENIX Association, Berkeley, CA, 12-12..", "enumerated": true, "marker": "[15]"}, {"self_ref": "#/texts/331", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 333.866, "r": 558.168, "b": 295.511, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 190]}], "orig": "[16] Rowstron, A., and Druschel, P. Pastry: Scalable, decentralized object location and routing for large-scale peerto-peer systems. Proceedings of Middleware, pages 329-350, November, 2001.", "text": "Rowstron, A., and Druschel, P. Pastry: Scalable, decentralized object location and routing for large-scale peerto-peer systems. Proceedings of Middleware, pages 329-350, November, 2001.", "enumerated": true, "marker": "[16]"}, {"self_ref": "#/texts/332", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 288.506, "r": 548.494, "b": 250.091, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 204]}], "orig": "[17] Rowstron, A.,  and Druschel, P. Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility. Proceedings of Symposium on Operating Systems Principles, October 2001.", "text": "Rowstron, A.,  and Druschel, P. Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility. Proceedings of Symposium on Operating Systems Principles, October 2001.", "enumerated": true, "marker": "[17]"}, {"self_ref": "#/texts/333", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 243.087, "r": 560.237, "b": 204.67200000000003, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 201]}], "orig": "[18] Saito, Y., Fr\u00f8lund, S., Veitch, A., Merchant, A., and Spence, S. 2004. FAB: building distributed enterprise disk arrays from commodity components. SIGOPS Oper. Syst. Rev. 38, 5 (Dec. 2004), 48-58.", "text": "Saito, Y., Fr\u00f8lund, S., Veitch, A., Merchant, A., and Spence, S. 2004. FAB: building distributed enterprise disk arrays from commodity components. SIGOPS Oper. Syst. Rev. 38, 5 (Dec. 2004), 48-58.", "enumerated": true, "marker": "[18]"}, {"self_ref": "#/texts/334", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 197.66700000000003, "r": 535.786, "b": 169.63200000000006, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 154]}], "orig": "[19] Satyanarayanan, M., Kistler, J.J., Siegel, E.H. Coda: A Resilient Distributed File System. IEEE Workshop on Workstation Operating Systems, Nov. 1987.", "text": "Satyanarayanan, M., Kistler, J.J., Siegel, E.H. Coda: A Resilient Distributed File System. IEEE Workshop on Workstation Operating Systems, Nov. 1987.", "enumerated": true, "marker": "[19]"}, {"self_ref": "#/texts/335", "parent": {"$ref": "#/groups/3"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 15, "bbox": {"l": 318.24, "t": 162.62799999999993, "r": 557.323, "b": 93.19200000000001, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 370]}], "orig": "[20] Stoica, I., Morris, R., Karger, D., Kaashoek, M. F., and Balakrishnan, H. 2001. Chord: A scalable peer-to-peer lookup service for internet applications. In Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols For Computer Communications (San Diego, California, United States). SIGCOMM '01. ACM Press, New York, NY, 149-160.", "text": "Stoica, I., Morris, R., Karger, D., Kaashoek, M. F., and Balakrishnan, H. 2001. Chord: A scalable peer-to-peer lookup service for internet applications. In Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols For Computer Communications (San Diego, California, United States). SIGCOMM '01. ACM Press, New York, NY, 149-160.", "enumerated": true, "marker": "[20]"}, {"self_ref": "#/texts/336", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 15, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "209 1", "text": "209 1"}, {"self_ref": "#/texts/337", "parent": {"$ref": "#/groups/4"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 16, "bbox": {"l": 54.0, "t": 717.744, "r": 291.462, "b": 648.309, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 394]}], "orig": "[21] Terry, D. B., Theimer, M. M., Petersen, K., Demers, A. J., Spreitzer, M. J., and Hauser, C. H. 1995. Managing update conflicts in Bayou, a weakly connected replicated storage system. In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (Copper Mountain, Colorado, United States, December 03 - 06, 1995). M. B. Jones, Ed. SOSP '95. ACM Press, New York, NY, 172-182.", "text": "Terry, D. B., Theimer, M. M., Petersen, K., Demers, A. J., Spreitzer, M. J., and Hauser, C. H. 1995. Managing update conflicts in Bayou, a weakly connected replicated storage system. In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles (Copper Mountain, Colorado, United States, December 03 - 06, 1995). M. B. Jones, Ed. SOSP '95. ACM Press, New York, NY, 172-182.", "enumerated": true, "marker": "[21]"}, {"self_ref": "#/texts/338", "parent": {"$ref": "#/groups/4"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 16, "bbox": {"l": 54.001, "t": 641.304, "r": 278.023, "b": 613.269, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 160]}], "orig": "[22] Thomas, R. H.  A majority consensus approach to concurrency control for multiple copy databases. ACM Transactions on Database Systems 4 (2): 180-209, 1979.", "text": "Thomas, R. H.  A majority consensus approach to concurrency control for multiple copy databases. ACM Transactions on Database Systems 4 (2): 180-209, 1979.", "enumerated": true, "marker": "[22]"}, {"self_ref": "#/texts/339", "parent": {"$ref": "#/groups/4"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 16, "bbox": {"l": 318.241, "t": 717.744, "r": 556.078, "b": 679.388, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 191]}], "orig": "[23] Weatherspoon, H., Eaton, P., Chun, B., and Kubiatowicz, J. 2007. Antiquity: exploiting a secure log for wide-area distributed storage. SIGOPS Oper. Syst. Rev. 41, 3 (Jun. 2007), 371-384.", "text": "Weatherspoon, H., Eaton, P., Chun, B., and Kubiatowicz, J. 2007. Antiquity: exploiting a secure log for wide-area distributed storage. SIGOPS Oper. Syst. Rev. 41, 3 (Jun. 2007), 371-384.", "enumerated": true, "marker": "[23]"}, {"self_ref": "#/texts/340", "parent": {"$ref": "#/groups/4"}, "children": [], "content_layer": "body", "label": "list_item", "prov": [{"page_no": 16, "bbox": {"l": 318.242, "t": 672.324, "r": 553.743, "b": 613.268, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 292]}], "orig": "[24] Welsh, M., Culler, D., and Brewer, E. 2001. SEDA: an architecture for well-conditioned, scalable internet services. In Proceedings of the Eighteenth ACM Symposium on Operating Systems Principles (Banff, Alberta, Canada, October 21 - 24, 2001). SOSP '01. ACM Press, New York, NY, 230-243.", "text": "Welsh, M., Culler, D., and Brewer, E. 2001. SEDA: an architecture for well-conditioned, scalable internet services. In Proceedings of the Eighteenth ACM Symposium on Operating Systems Principles (Banff, Alberta, Canada, October 21 - 24, 2001). SOSP '01. ACM Press, New York, NY, 230-243.", "enumerated": true, "marker": "[24]"}, {"self_ref": "#/texts/341", "parent": {"$ref": "#/body"}, "children": [], "content_layer": "furniture", "label": "page_footer", "prov": [{"page_no": 16, "bbox": {"l": 299.0, "t": 40.14700000000005, "r": 312.5, "b": 32.047000000000025, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 5]}], "orig": "210 2", "text": "210 2"}], "pictures": [{"self_ref": "#/pictures/0", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/42"}, {"$ref": "#/texts/43"}, {"$ref": "#/texts/44"}, {"$ref": "#/texts/45"}, {"$ref": "#/texts/46"}, {"$ref": "#/texts/47"}, {"$ref": "#/texts/48"}, {"$ref": "#/texts/49"}, {"$ref": "#/texts/50"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 3, "bbox": {"l": 58.832889556884766, "t": 711.8781127929688, "r": 256.04449462890625, "b": 489.7120361328125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/42"}], "references": [], "footnotes": [], "annotations": []}, {"self_ref": "#/pictures/1", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/77"}, {"$ref": "#/texts/78"}, {"$ref": "#/texts/79"}, {"$ref": "#/texts/80"}, {"$ref": "#/texts/81"}, {"$ref": "#/texts/82"}, {"$ref": "#/texts/83"}, {"$ref": "#/texts/84"}, {"$ref": "#/texts/85"}, {"$ref": "#/texts/86"}, {"$ref": "#/texts/87"}, {"$ref": "#/texts/88"}, {"$ref": "#/texts/89"}, {"$ref": "#/texts/90"}, {"$ref": "#/texts/91"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 5, "bbox": {"l": 74.55238342285156, "t": 717.4221038818359, "r": 256.890380859375, "b": 582.9283599853516, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/77"}], "references": [], "footnotes": [], "annotations": []}, {"self_ref": "#/pictures/2", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/125"}, {"$ref": "#/texts/126"}, {"$ref": "#/texts/127"}, {"$ref": "#/texts/128"}, {"$ref": "#/texts/129"}, {"$ref": "#/texts/130"}, {"$ref": "#/texts/131"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 7, "bbox": {"l": 63.35589599609375, "t": 716.6913833618164, "r": 242.99293518066406, "b": 522.1643371582031, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [], "references": [], "footnotes": [], "annotations": []}, {"self_ref": "#/pictures/3", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/194"}, {"$ref": "#/texts/195"}, {"$ref": "#/texts/196"}, {"$ref": "#/texts/197"}, {"$ref": "#/texts/198"}, {"$ref": "#/texts/199"}, {"$ref": "#/texts/200"}, {"$ref": "#/texts/201"}, {"$ref": "#/texts/202"}, {"$ref": "#/texts/203"}, {"$ref": "#/texts/204"}, {"$ref": "#/texts/205"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 10, "bbox": {"l": 52.50687026977539, "t": 721.9166412353516, "r": 301.1681213378906, "b": 544.8081970214844, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/194"}], "references": [], "footnotes": [], "annotations": []}, {"self_ref": "#/pictures/4", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/212"}, {"$ref": "#/texts/213"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 10, "bbox": {"l": 316.5185241699219, "t": 720.9463424682617, "r": 558.3970947265625, "b": 556.2559814453125, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/212"}], "references": [], "footnotes": [], "annotations": []}, {"self_ref": "#/pictures/5", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/230"}, {"$ref": "#/texts/231"}, {"$ref": "#/texts/232"}, {"$ref": "#/texts/233"}, {"$ref": "#/texts/234"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 11, "bbox": {"l": 53.31742858886719, "t": 717.3514633178711, "r": 291.9895935058594, "b": 584.9536743164062, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [], "references": [], "footnotes": [], "annotations": []}, {"self_ref": "#/pictures/6", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/252"}, {"$ref": "#/texts/253"}, {"$ref": "#/texts/254"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 12, "bbox": {"l": 91.94941711425781, "t": 716.9537200927734, "r": 517.15185546875, "b": 561.4158935546875, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/252"}], "references": [], "footnotes": [], "annotations": []}, {"self_ref": "#/pictures/7", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/264"}, {"$ref": "#/texts/265"}, {"$ref": "#/texts/266"}, {"$ref": "#/texts/267"}, {"$ref": "#/texts/268"}, {"$ref": "#/texts/269"}, {"$ref": "#/texts/270"}, {"$ref": "#/texts/271"}, {"$ref": "#/texts/272"}, {"$ref": "#/texts/273"}, {"$ref": "#/texts/274"}, {"$ref": "#/texts/275"}, {"$ref": "#/texts/276"}, {"$ref": "#/texts/277"}, {"$ref": "#/texts/278"}, {"$ref": "#/texts/279"}, {"$ref": "#/texts/280"}, {"$ref": "#/texts/281"}, {"$ref": "#/texts/282"}, {"$ref": "#/texts/283"}, {"$ref": "#/texts/284"}], "content_layer": "body", "label": "picture", "prov": [{"page_no": 13, "bbox": {"l": 51.74768829345703, "t": 719.5362930297852, "r": 286.99713134765625, "b": 569.9481353759766, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/264"}], "references": [], "footnotes": [], "annotations": []}], "tables": [{"self_ref": "#/tables/0", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/97"}], "content_layer": "body", "label": "table", "prov": [{"page_no": 5, "bbox": {"l": 318.63519287109375, "t": 696.2334442138672, "r": 555.1702880859375, "b": 453.10272216796875, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/97"}], "references": [], "footnotes": [], "data": {"table_cells": [{"bbox": {"l": 343.26, "t": 99.70500000000004, "r": 372.543, "b": 106.20899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Problem", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 417.856, "t": 99.70500000000004, "r": 453.864, "b": 106.20899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Technique", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 496.715, "t": 99.70500000000004, "r": 533.571, "b": 106.20899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Advantage", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 339.0, "t": 115.245, "r": 376.786, "b": 121.74900000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Partitioning", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 404.635, "t": 115.245, "r": 467.052, "b": 121.74900000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Consistent Hashing", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 496.041, "t": 115.245, "r": 534.255, "b": 130.92899999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Incremental Scalability", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 329.82, "t": 140.14499999999998, "r": 386.008, "b": 155.82899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "High Availability for writes", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 402.42, "t": 140.14499999999998, "r": 469.361, "b": 165.06899999999996, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Vector clocks with reconciliation during reads", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 489.96, "t": 140.14499999999998, "r": 540.479, "b": 165.06899999999996, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Version size is decoupled from update rates.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 325.62, "t": 174.22500000000002, "r": 390.292, "b": 189.96900000000005, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Handling temporary failures", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 403.38, "t": 174.22500000000002, "r": 468.294, "b": 189.96900000000005, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Sloppy Quorum and hinted handoff", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 483.061, "t": 174.22500000000002, "r": 547.259, "b": 226.75, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Provides high availability and durability guarantee when some of the replicas are not available.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 328.02, "t": 235.96500000000003, "r": 387.732, "b": 251.649, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Recovering from permanent failures", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 405.54, "t": 235.96500000000003, "r": 466.19, "b": 251.649, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Anti-entropy using Merkle trees", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 482.46, "t": 235.96500000000003, "r": 547.793, "b": 260.82899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Synchronizes divergent replicas in the background.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 330.9, "t": 270.04499999999996, "r": 384.869, "b": 285.729, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Membership and failure detection", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 401.76, "t": 270.04499999999996, "r": 470.002, "b": 294.969, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Gossip-based membership protocol and failure detection.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 482.82, "t": 270.04499999999996, "r": 547.464, "b": 331.75, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Preserves symmetry and avoids having a centralized registry for storing membership and node liveness information.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}], "num_rows": 6, "num_cols": 3, "grid": [[{"bbox": {"l": 343.26, "t": 99.70500000000004, "r": 372.543, "b": 106.20899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Problem", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 417.856, "t": 99.70500000000004, "r": 453.864, "b": 106.20899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Technique", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 496.715, "t": 99.70500000000004, "r": 533.571, "b": 106.20899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Advantage", "column_header": true, "row_header": false, "row_section": false, "fillable": false}], [{"bbox": {"l": 339.0, "t": 115.245, "r": 376.786, "b": 121.74900000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Partitioning", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 404.635, "t": 115.245, "r": 467.052, "b": 121.74900000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Consistent Hashing", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 496.041, "t": 115.245, "r": 534.255, "b": 130.92899999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Incremental Scalability", "column_header": false, "row_header": false, "row_section": false, "fillable": false}], [{"bbox": {"l": 329.82, "t": 140.14499999999998, "r": 386.008, "b": 155.82899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "High Availability for writes", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 402.42, "t": 140.14499999999998, "r": 469.361, "b": 165.06899999999996, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Vector clocks with reconciliation during reads", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 489.96, "t": 140.14499999999998, "r": 540.479, "b": 165.06899999999996, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Version size is decoupled from update rates.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}], [{"bbox": {"l": 325.62, "t": 174.22500000000002, "r": 390.292, "b": 189.96900000000005, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Handling temporary failures", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 403.38, "t": 174.22500000000002, "r": 468.294, "b": 189.96900000000005, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Sloppy Quorum and hinted handoff", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 483.061, "t": 174.22500000000002, "r": 547.259, "b": 226.75, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 3, "end_row_offset_idx": 4, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Provides high availability and durability guarantee when some of the replicas are not available.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}], [{"bbox": {"l": 328.02, "t": 235.96500000000003, "r": 387.732, "b": 251.649, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Recovering from permanent failures", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 405.54, "t": 235.96500000000003, "r": 466.19, "b": 251.649, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Anti-entropy using Merkle trees", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 482.46, "t": 235.96500000000003, "r": 547.793, "b": 260.82899999999995, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 4, "end_row_offset_idx": 5, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Synchronizes divergent replicas in the background.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}], [{"bbox": {"l": 330.9, "t": 270.04499999999996, "r": 384.869, "b": 285.729, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Membership and failure detection", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 401.76, "t": 270.04499999999996, "r": 470.002, "b": 294.969, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "Gossip-based membership protocol and failure detection.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 482.82, "t": 270.04499999999996, "r": 547.464, "b": 331.75, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 5, "end_row_offset_idx": 6, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "Preserves symmetry and avoids having a centralized registry for storing membership and node liveness information.", "column_header": false, "row_header": false, "row_section": false, "fillable": false}]]}, "annotations": []}, {"self_ref": "#/tables/1", "parent": {"$ref": "#/body"}, "children": [{"$ref": "#/texts/297"}], "content_layer": "body", "label": "table", "prov": [{"page_no": 14, "bbox": {"l": 48.00107192993164, "t": 694.0668334960938, "r": 297.8468017578125, "b": 598.3177947998047, "coord_origin": "BOTTOMLEFT"}, "charspan": [0, 0]}], "captions": [{"$ref": "#/texts/297"}], "references": [], "footnotes": [], "data": {"table_cells": [{"bbox": {"l": 100.081, "t": 101.05799999999999, "r": 139.028, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "99.9th percentile read latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 154.261, "t": 101.05799999999999, "r": 193.208, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "99.9th percentile write latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 208.081, "t": 111.43799999999999, "r": 241.55, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "Average read latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 256.141, "t": 111.43799999999999, "r": 289.61, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "Average write latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 55.86, "t": 153.31799999999998, "r": 85.313, "b": 171.60699999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Server- driven", "column_header": false, "row_header": true, "row_section": false, "fillable": false}, {"bbox": {"l": 110.82, "t": 163.69899999999996, "r": 128.288, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "68.9", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 164.995, "t": 163.69899999999996, "r": 182.463, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "68.5", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 218.569, "t": 163.69899999999996, "r": 231.049, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "3.9", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 264.155, "t": 163.69899999999996, "r": 281.623, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "4.02", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 57.66, "t": 174.49800000000005, "r": 83.623, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Client- driven", "column_header": false, "row_header": true, "row_section": false, "fillable": false}, {"bbox": {"l": 110.8, "t": 184.87800000000004, "r": 128.27, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "30.4", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 164.978, "t": 184.87800000000004, "r": 182.449, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "30.4", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 216.096, "t": 184.87800000000004, "r": 233.567, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "1.55", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 266.614, "t": 184.87800000000004, "r": 279.095, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "1.9", "column_header": false, "row_header": false, "row_section": false, "fillable": false}], "num_rows": 3, "num_cols": 5, "grid": [[{"row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 100.081, "t": 101.05799999999999, "r": 139.028, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "99.9th percentile read latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 154.261, "t": 101.05799999999999, "r": 193.208, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "99.9th percentile write latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 208.081, "t": 111.43799999999999, "r": 241.55, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "Average read latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 256.141, "t": 111.43799999999999, "r": 289.61, "b": 150.42700000000002, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 0, "end_row_offset_idx": 1, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "Average write latency (ms)", "column_header": true, "row_header": false, "row_section": false, "fillable": false}], [{"bbox": {"l": 55.86, "t": 153.31799999999998, "r": 85.313, "b": 171.60699999999997, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Server- driven", "column_header": false, "row_header": true, "row_section": false, "fillable": false}, {"bbox": {"l": 110.82, "t": 163.69899999999996, "r": 128.288, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "68.9", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 164.995, "t": 163.69899999999996, "r": 182.463, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "68.5", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 218.569, "t": 163.69899999999996, "r": 231.049, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "3.9", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 264.155, "t": 163.69899999999996, "r": 281.623, "b": 171.66700000000003, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 1, "end_row_offset_idx": 2, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "4.02", "column_header": false, "row_header": false, "row_section": false, "fillable": false}], [{"bbox": {"l": 57.66, "t": 174.49800000000005, "r": 83.623, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 0, "end_col_offset_idx": 1, "text": "Client- driven", "column_header": false, "row_header": true, "row_section": false, "fillable": false}, {"bbox": {"l": 110.8, "t": 184.87800000000004, "r": 128.27, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 1, "end_col_offset_idx": 2, "text": "30.4", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 164.978, "t": 184.87800000000004, "r": 182.449, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 2, "end_col_offset_idx": 3, "text": "30.4", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 216.096, "t": 184.87800000000004, "r": 233.567, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 3, "end_col_offset_idx": 4, "text": "1.55", "column_header": false, "row_header": false, "row_section": false, "fillable": false}, {"bbox": {"l": 266.614, "t": 184.87800000000004, "r": 279.095, "b": 192.846, "coord_origin": "TOPLEFT"}, "row_span": 1, "col_span": 1, "start_row_offset_idx": 2, "end_row_offset_idx": 3, "start_col_offset_idx": 4, "end_col_offset_idx": 5, "text": "1.9", "column_header": false, "row_header": false, "row_section": false, "fillable": false}]]}, "annotations": []}], "key_value_items": [], "form_items": [], "pages": {"1": {"size": {"width": 612.0, "height": 792.0}, "page_no": 1}, "2": {"size": {"width": 612.0, "height": 792.0}, "page_no": 2}, "3": {"size": {"width": 612.0, "height": 792.0}, "page_no": 3}, "4": {"size": {"width": 612.0, "height": 792.0}, "page_no": 4}, "5": {"size": {"width": 612.0, "height": 792.0}, "page_no": 5}, "6": {"size": {"width": 612.0, "height": 792.0}, "page_no": 6}, "7": {"size": {"width": 612.0, "height": 792.0}, "page_no": 7}, "8": {"size": {"width": 612.0, "height": 792.0}, "page_no": 8}, "9": {"size": {"width": 612.0, "height": 792.0}, "page_no": 9}, "10": {"size": {"width": 612.0, "height": 792.0}, "page_no": 10}, "11": {"size": {"width": 612.0, "height": 792.0}, "page_no": 11}, "12": {"size": {"width": 612.0, "height": 792.0}, "page_no": 12}, "13": {"size": {"width": 612.0, "height": 792.0}, "page_no": 13}, "14": {"size": {"width": 612.0, "height": 792.0}, "page_no": 14}, "15": {"size": {"width": 612.0, "height": 792.0}, "page_no": 15}, "16": {"size": {"width": 612.0, "height": 792.0}, "page_no": 16}}}