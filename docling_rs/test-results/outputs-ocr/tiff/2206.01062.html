<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8"/>
<title>2206.01062</title>
<meta name="generator" content="Docling HTML Serializer"/>
<style>
    html {
        background-color: #f5f5f5;
        font-family: Arial, sans-serif;
        line-height: 1.6;
    }
    body {
        max-width: 800px;
        margin: 0 auto;
        padding: 2rem;
        background-color: white;
        box-shadow: 0 0 10px rgba(0,0,0,0.1);
    }
    h1, h2, h3, h4, h5, h6 {
        color: #333;
        margin-top: 1.5em;
        margin-bottom: 0.5em;
    }
    h1 {
        font-size: 2em;
        border-bottom: 1px solid #eee;
        padding-bottom: 0.3em;
    }
    table {
        border-collapse: collapse;
        margin: 1em 0;
        width: 100%;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;
    }
    th {
        background-color: #f2f2f2;
        font-weight: bold;
    }
    figure {
        margin: 1.5em 0;
        text-align: center;
    }
    figcaption {
        color: #666;
        font-style: italic;
        margin-top: 0.5em;
    }
    img {
        max-width: 100%;
        height: auto;
    }
    pre {
        background-color: #f6f8fa;
        border-radius: 3px;
        padding: 1em;
        overflow: auto;
    }
    code {
        font-family: monospace;
        background-color: #f6f8fa;
        padding: 0.2em 0.4em;
        border-radius: 3px;
    }
    pre code {
        background-color: transparent;
        padding: 0;
    }
    .formula {
        text-align: center;
        padding: 0.5em;
        margin: 1em 0;
        background-color: #f9f9f9;
    }
    .formula-not-decoded {
        text-align: center;
        padding: 0.5em;
        margin: 1em 0;
        background: repeating-linear-gradient(
            45deg,
            #f0f0f0,
            #f0f0f0 10px,
            #f9f9f9 10px,
            #f9f9f9 20px
        );
    }
    .page-break {
        page-break-after: always;
        border-top: 1px dashed #ccc;
        margin: 2em 0;
    }
    .key-value-region {
        background-color: #f9f9f9;
        padding: 1em;
        border-radius: 4px;
        margin: 1em 0;
    }
    .key-value-region dt {
        font-weight: bold;
    }
    .key-value-region dd {
        margin-left: 1em;
        margin-bottom: 0.5em;
    }
    .form-container {
        border: 1px solid #ddd;
        padding: 1em;
        border-radius: 4px;
        margin: 1em 0;
    }
    .form-item {
        margin-bottom: 0.5em;
    }
    .image-classification {
        font-size: 0.9em;
        color: #666;
        margin-top: 0.5em;
    }
</style>
</head>
<body>
<div class='page'>
<h2>DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis</h2>
<p>Birgit Pfitzmann IBM Research Rueschlikon, Switzerland bpf@zurich.ibm.com</p>
<p>Christoph Auer IBM Research Rueschlikon, Switzerland caugozurich.ibm.com</p>
<p>Ahmed S. Nassar IRM Research Rueschlikon, Switzerland ahn@zurich.ibm.com</p>
<h2>ABSTRACT</h2>
<p>Accurate document layout analysis is a ley requirement for highquality PDF document conversion. With the recent availability of public, large ground-truth datasets such as PubLayNet and DocHank, deep-learning models have proven to be very effective at layout detection and segmentation. While these datasets are of adequate size to train such models, they severely lack in layout variability since they are sourced from scientific article repositories such as PubMed and arXir only. Consequently, the accuracy of the layout segmentation drops significantly when these models are applied on more challenging and diverse layouts. In this paper, we present Doc.a Net, a new, publicly available, document-layout annotation dataset in COCO format. It contains 80863 manually annotated pages from diverse data sources to represent a wide varizbility in Layouts. For each PDF page, the layout annotations provide labelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also provides a subset of double- and triple-annotated pages to determine the inter-annotator agreement. In multiple experiments. we provide baseline accuracy scores (in mAP) for a set of popular object detection models. We also demonstrate that these models fall approximately 10% behind the inter-annotator agreement. Furthermore, we provide evidence that DocLayNet is of sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and DoclayNet, showing that layout predictions of the DocLayNettrained models are more robust and thus the preferred choice for general-purpose document-layout analysis</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li style="list-style-type: '• ';">Information systems → Document structure; •Applied com-</li>
</ul>
<p>puting + Document analysis; · Computing methodologies → Machine learning: Computer vision; Object detection;</p>
<p>Permersion to make digital or hoed copies of part or all of this work dor personal or classedom use is granded without lee poovided that copies are nor made or distributed for pestit or commercial advantage and that caples bear this socice and the full citation on the first page. Copyrights doc third-party components of this work must he honored. For all ocher uses, contact the owner/authenti</p>
<p>KDD 22 Asgoat 14-04, 202, Wanhingion, DC, USA</p>
<p>© 2022 Capricht hold by the auner/nuther|.</p>
<p>ACM BEN 518-1-4503-5088-0/22/05.</p>
<p>httpe:/dai arg/101145/3534678.353943</p>
<p>Michele Dolfi IBM Research Rueschlikon, Switzerland dol@zurich.ibm.com</p>
<p>Peter Staar IBM Research Rueschlikon, Switzerland taa@zurich.ibm.com</p>
<figure><figcaption><div class="caption">Figure 1: Four examples of complex page layouts across different document categories</div></figcaption></figure>
<h2>KEYWORDS</h2>
<p>PDF document conversion, layout segmentation, object-detection, data set, Machine Learning</p>
<p>Birgit Pritamann, Christoph Aner, Michele Dolfi, Ahmed S. Nasar, and Peter Staar. 2022. DocLayNet A Large Human Annotated Dataset for DocumentLayout Analyst. In Proceedings of the 28th ACM SIGREND Conference ou Knomledge Discovery and Data Mining (KDD 22l, Augast 14-14, 2022, Washington, DC, USA. ACM, New York, NY, USA, 9 pages https://dol.org/16.1145/</p>
<figure><figcaption><div class="caption">Figure 2: Distribution of DocLayNet pages across document categories.</div></figcaption></figure>
<p>to a minimum, since they introduce difficulties in annotation (see Section 4). As a second condition, we focussed on medium to large documents (&gt; 10 pages) with technical content, dense in complex tables, figures, plots and captions. Such documents carry a lot of information value, but are often hard to analyse with high accuracy due to their challenging layouts. Counterexamples of documents not included in the dataset are receipts, invoices, hand-written documents or photographs showing "tect in the wild".</p>
<p>The pages in DocLayNet can be grouped into six distinct categories, namely Financial Reports, Manuals, Scientific Articles, Laws &amp; Regulations, Patents and Gourament Tenders. Each document categary was sourced from various repositories. For example, Financial Reports contain both free-style format annual reports which expose company-specific, artistic Layouts as well as the more formal SEC flings. The two largest categories (Financial Reports and Manunt) contain a large amount of free-style layouts in order to obtain maximum variability. In the other four categories, we boosted the variabality by mixing documents from independent providers, such as different government websites or publishers. In Figure 2, we show the document categories contained in DocLayNet with their respective sizes.</p>
<p>We did not control the document selection with regard to language. The vast majority of documents contained in DocLayNet (close to 95) are published in English language. However, DocLayNet also contains a number of documents in other languages such as German (2.5%), French (1.0%) and Japanese (1.05). While the document language has negligible impact on the pertormance of computer vision methods such as object detection and segmentation models, it might prove challenging for layout analysis methods which exploit textual features.</p>
<p>To ensure that future benchmarks in the document-layout analyis community can be easily compared, we have split up DocLayNet into pre-defined train-, test- and validation-sets. In this way, we can avnad spurious variations in the evaluation scores due to random splitting in train-, test- and validation-sets. We also ensured that less frequent labels are represented in train and test sets in equal</p>
<p>Er. AAPL. from hitpel wwwarmunirepart.com/</p>
<p>Table i shows the overall frequency and distribution of the labels among the different sets. Importantly, we ensure that subsets are only split on full-document boundaries. This aroads that pages of the same document are spread over train, test and validation set, which can give an undesired evaluation advantage to models and Lead to overestimation of their prediction accuracy. We will show the impact of this decision in Section 5.</p>
<p>In order to accommodate the different types of models currently in use by the community, we provide DocLayNet in an augmented COCO format [16]. This entails the standard COCO ground-truth file (in JSON format) with the associated page images in PNG format, 1025x1025 pixels). Furthermore, custom fields have been added to each COCO record to specify document category, original document filename and page number. In addition, we also provide the original PDF pages, as well as sidecar files containing parsed PDF text and text-cell coordinates in JSON). All additional files are linked to the primary page images by their matching filenames.</p>
<p>Despite being, cost-intense and far less scalable than automation, human annotation has several benefits over automated groundtruth generation. The first and most obvirus reason to leverage human annotations is the freedom to annotate any type of document without requiring a programmatic source. For most PDF documents, the original source document is not available. The latter is not a hard constraint with human annotation, but it is for automated methods. A second reason to use buman annotations is that the latter usually provide a more natural interpretation of the page layout. The human-interpreted layout can significantly deviate from the programmatic layout used in typesetting. For example, "invisible" tables might be used solely for aligning text paragraphs on columns. Such typesetting tricks might be interpreted by automated methods incorrectly as an actual table, while the human annotation will interpret it correctly as Text or other styles. The same applies to multi-line text elements, when authors decided to space them as "invisible" list elements without bullet sumbol. A third reason to gather ground-truth through human annotation is to estimate a "natural" upper bound on the sermentation accuracy. As we will show in Section 4, certain documents featuring complex layouts can have diferent but equally acceptable layout interpretations. This natural upper bound for segmentation accuracy can be found by annotating the same pages multiple times by different people and evaluating the inter-annotator agreement. Such a baseline consistency evaluation is very useful to define expectations for a good target accuracy in trained deep neural network models and avoid overfitting (see Table 1). On the flip side, achieving high annotation consistency proved to be a key challenge in human annotation, as we outline in Section 4.</p>
<h2>4 ANNOTATION CAMPAIGN</h2>
<p>The annotation campaign was carried out in four phases. In phase one, we identified and prepared the data sources for annotation. In phase two, we determined the class labels and how annotations should be done on the documents in order to obtain maximum consistency. The latter was guided by a detailed requirement analysis and exhaustive experiments. In phase three, we trained the annotation staff and performed exams for quality assurance. In phase four,</p>
</div>
</body>
</html>